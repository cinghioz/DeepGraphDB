{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbcae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import warnings\n",
    "from DeepGraphDB import DeepGraphDB\n",
    "import random\n",
    "\n",
    "class HeteroNBFLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Bellman-Ford Layer for DGL HeteroGraph\n",
    "    Implements MESSAGE, AGGREGATE and boundary conditions for heterogeneous graphs\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, \n",
    "                 message_func: str = \"distmult\", aggregate_func: str = \"pna\", \n",
    "                 layer_norm: bool = True, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.message_func = message_func\n",
    "        self.aggregate_func = aggregate_func\n",
    "        \n",
    "        # Layer normalization and dropout\n",
    "        self.layer_norm = nn.LayerNorm(input_dim) if layer_norm else None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Aggregation components\n",
    "        if aggregate_func == \"pna\":\n",
    "            # Principal Neighborhood Aggregation\n",
    "            self.pna_linear = nn.Linear(input_dim * 4, input_dim)  # mean, max, min, std\n",
    "        elif aggregate_func in [\"sum\", \"mean\", \"max\"]:\n",
    "            pass  # No additional parameters needed\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown aggregate function: {aggregate_func}\")\n",
    "    \n",
    "    def forward(self, hg: dgl.DGLGraph, node_features: Dict[str, torch.Tensor], \n",
    "                relation_embeddings: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass for heterogeneous NBF layer\n",
    "        \n",
    "        Args:\n",
    "            hg: DGL HeteroGraph\n",
    "            node_features: Dict mapping node types to feature tensors\n",
    "            relation_embeddings: Dict mapping relation types to embedding tensors\n",
    "            \n",
    "        Returns:\n",
    "            Updated node features by type\n",
    "        \"\"\"\n",
    "        # Store features in graph\n",
    "        for ntype, features in node_features.items():\n",
    "            hg.nodes[ntype].data['h'] = features\n",
    "        \n",
    "        # Apply message passing for each edge type (relation)\n",
    "        funcs = {}\n",
    "        for etype in hg.canonical_etypes:\n",
    "            src_type, rel_type, dst_type = etype\n",
    "            funcs[etype] = (self.message_func_hetero(rel_type, relation_embeddings), \n",
    "                           self.aggregate_func_hetero)\n",
    "        \n",
    "        # Execute heterogeneous message passing\n",
    "        hg.multi_update_all(funcs, 'sum')\n",
    "        \n",
    "        # Extract updated features\n",
    "        updated_features = {}\n",
    "        for ntype in hg.ntypes:\n",
    "            if 'h_new' in hg.nodes[ntype].data:\n",
    "                h_new = hg.nodes[ntype].data['h_new']\n",
    "                \n",
    "                # Add residual connection (boundary condition)\n",
    "                if 'h0' in hg.nodes[ntype].data:\n",
    "                    h_new = h_new + hg.nodes[ntype].data['h0']\n",
    "                \n",
    "                # Apply layer normalization and dropout\n",
    "                if self.layer_norm:\n",
    "                    h_new = self.layer_norm(h_new)\n",
    "                h_new = self.dropout(h_new)\n",
    "                \n",
    "                updated_features[ntype] = h_new\n",
    "            else:\n",
    "                # If no messages received, keep original features\n",
    "                updated_features[ntype] = node_features[ntype]\n",
    "        \n",
    "        return updated_features\n",
    "    \n",
    "    def message_func_hetero(self, rel_type: str, relation_embeddings: Dict[str, torch.Tensor]):\n",
    "        \"\"\"Create message function for specific relation type\"\"\"\n",
    "        def message_func(edges):\n",
    "            h_src = edges.src['h']\n",
    "            rel_emb = relation_embeddings[rel_type]\n",
    "            \n",
    "            # Broadcast relation embedding to match batch size\n",
    "            if rel_emb.dim() == 1:\n",
    "                rel_emb = rel_emb.unsqueeze(0).expand(h_src.size(0), -1)\n",
    "            elif rel_emb.size(0) == 1 and h_src.size(0) > 1:\n",
    "                rel_emb = rel_emb.expand(h_src.size(0), -1)\n",
    "            \n",
    "            if self.message_func == \"transe\":\n",
    "                # Translation: h + r\n",
    "                message = h_src + rel_emb\n",
    "            elif self.message_func == \"distmult\":\n",
    "                # Element-wise multiplication: h âŠ™ r\n",
    "                message = h_src * rel_emb\n",
    "            elif self.message_func == \"rotate\":\n",
    "                # Complex rotation (for complex embeddings)\n",
    "                if self.input_dim % 2 != 0:\n",
    "                    raise ValueError(\"RotatE requires even embedding dimension\")\n",
    "                h_re, h_im = h_src.chunk(2, dim=-1)\n",
    "                r_re, r_im = rel_emb.chunk(2, dim=-1)\n",
    "                message_re = h_re * r_re - h_im * r_im\n",
    "                message_im = h_re * r_im + h_im * r_re\n",
    "                message = torch.cat([message_re, message_im], dim=-1)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown message function: {self.message_func}\")\n",
    "            \n",
    "            return {'msg': message}\n",
    "        \n",
    "        return message_func\n",
    "    \n",
    "    def aggregate_func_hetero(self, nodes):\n",
    "        \"\"\"Aggregate function for heterogeneous message passing\"\"\"\n",
    "        if 'msg' not in nodes.mailbox:\n",
    "            # No messages received\n",
    "            return {}\n",
    "        \n",
    "        messages = nodes.mailbox['msg']  # Shape: (num_nodes, num_neighbors, hidden_dim)\n",
    "        \n",
    "        if self.aggregate_func == \"sum\":\n",
    "            aggregated = torch.sum(messages, dim=1)\n",
    "        elif self.aggregate_func == \"mean\":\n",
    "            aggregated = torch.mean(messages, dim=1)\n",
    "        elif self.aggregate_func == \"max\":\n",
    "            aggregated = torch.max(messages, dim=1)[0]\n",
    "        elif self.aggregate_func == \"pna\":\n",
    "            # Principal Neighborhood Aggregation\n",
    "            mean_msg = torch.mean(messages, dim=1)\n",
    "            max_msg = torch.max(messages, dim=1)[0]\n",
    "            min_msg = torch.min(messages, dim=1)[0]\n",
    "            std_msg = torch.std(messages, dim=1)\n",
    "            \n",
    "            pna_features = torch.cat([mean_msg, max_msg, min_msg, std_msg], dim=-1)\n",
    "            aggregated = self.pna_linear(pna_features)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown aggregate function: {self.aggregate_func}\")\n",
    "        \n",
    "        return {'h_new': aggregated}\n",
    "\n",
    "\n",
    "class HeteroNBFNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Bellman-Ford Network for DGL HeteroGraph\n",
    "    \"\"\"\n",
    "    def __init__(self, node_types: List[str], edge_types: List[Tuple[str, str, str]], \n",
    "                 embedding_dim: int = 32, num_layers: int = 6, \n",
    "                 message_func: str = \"distmult\", aggregate_func: str = \"pna\", \n",
    "                 layer_norm: bool = True, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.node_types = node_types\n",
    "        self.edge_types = edge_types  # List of (src_type, rel_type, dst_type)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Extract relation types\n",
    "        self.relation_types = list(set(etype[1] for etype in edge_types))\n",
    "        \n",
    "        # Relation embeddings for each relation type\n",
    "        self.relation_embeddings = nn.ModuleDict({\n",
    "            rel_type: nn.Embedding(1, embedding_dim)\n",
    "            for rel_type in self.relation_types\n",
    "        })\n",
    "        \n",
    "        # Query embeddings for INDICATOR function (one per relation type)\n",
    "        self.query_embeddings = nn.ModuleDict({\n",
    "            rel_type: nn.Embedding(1, embedding_dim)\n",
    "            for rel_type in self.relation_types\n",
    "        })\n",
    "        \n",
    "        # NBF layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            HeteroNBFLayer(embedding_dim, embedding_dim, message_func, \n",
    "                          aggregate_func, layer_norm, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection for link prediction scores\n",
    "        self.output_linear = nn.Linear(embedding_dim, 1)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Initialize model parameters\"\"\"\n",
    "        for rel_emb in self.relation_embeddings.values():\n",
    "            nn.init.xavier_uniform_(rel_emb.weight)\n",
    "        for query_emb in self.query_embeddings.values():\n",
    "            nn.init.xavier_uniform_(query_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.output_linear.weight)\n",
    "        nn.init.zeros_(self.output_linear.bias)\n",
    "    \n",
    "    def indicator_function(self, hg: dgl.DGLGraph, source_nodes: Dict[int, torch.Tensor], \n",
    "                          query_relation: str) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        INDICATOR function: Initialize boundary conditions for heterogeneous graph\n",
    "        \n",
    "        Args:\n",
    "            hg: DGL HeteroGraph\n",
    "            source_nodes: Dict mapping node types to source node indices\n",
    "            query_relation: Query relation type\n",
    "            \n",
    "        Returns:\n",
    "            Initial node features by type\n",
    "        \"\"\"\n",
    "        h0 = {}\n",
    "        query_emb = self.query_embeddings[query_relation](torch.tensor([0], device=next(iter(source_nodes.values())).device))\n",
    "        query_emb = query_emb.squeeze(0)  # Remove batch dimension\n",
    "        \n",
    "        for ntype in hg.ntypes:\n",
    "            num_nodes = hg.number_of_nodes(ntype)\n",
    "            device = next(iter(source_nodes.values())).device\n",
    "            \n",
    "            # Initialize all nodes with zeros\n",
    "            h0_type = torch.zeros(num_nodes, self.embedding_dim, device=device)\n",
    "            \n",
    "            # Set source nodes with query embeddings\n",
    "            if ntype in source_nodes and len(source_nodes[ntype]) > 0:\n",
    "                h0_type[source_nodes[ntype]] = query_emb.unsqueeze(0).expand(len(source_nodes[ntype]), -1)\n",
    "            \n",
    "            h0[ntype] = h0_type\n",
    "        \n",
    "        return h0\n",
    "    \n",
    "    def forward(self, hg: dgl.DGLGraph, source_nodes: Dict[int, torch.Tensor], \n",
    "                target_nodes: Dict[str, torch.Tensor], query_relation: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of HeteroNBFNet\n",
    "        \n",
    "        Args:\n",
    "            hg: DGL HeteroGraph\n",
    "            source_nodes: Dict mapping node types to source node indices\n",
    "            target_nodes: Dict mapping node types to target node indices\n",
    "            query_relation: Query relation type\n",
    "        \n",
    "        Returns:\n",
    "            Link prediction scores\n",
    "        \"\"\"\n",
    "        # Initialize boundary conditions (INDICATOR function)\n",
    "        h0 = self.indicator_function(hg, source_nodes, query_relation)\n",
    "        \n",
    "        # Store boundary conditions in graph\n",
    "        for ntype, features in h0.items():\n",
    "            hg.nodes[ntype].data['h0'] = features\n",
    "        \n",
    "        # Initialize current features\n",
    "        current_features = {ntype: h0[ntype].clone() for ntype in h0.keys()}\n",
    "        \n",
    "        # Get relation embeddings\n",
    "        rel_embeddings = {}\n",
    "        device = next(iter(source_nodes.values())).device\n",
    "        for rel_type in self.relation_types:\n",
    "            rel_embeddings[rel_type] = self.relation_embeddings[rel_type](\n",
    "                torch.tensor([0], device=device)\n",
    "            ).squeeze(0)\n",
    "        \n",
    "        # Apply NBF layers iteratively\n",
    "        for layer in self.layers:\n",
    "            current_features = layer(hg, current_features, rel_embeddings)\n",
    "        \n",
    "        # Extract final representations for target nodes and compute scores\n",
    "        scores = []\n",
    "        for ntype, target_indices in target_nodes.items():\n",
    "            if len(target_indices) > 0 and ntype in current_features:\n",
    "                target_features = current_features[ntype][target_indices]\n",
    "                target_scores = self.output_linear(target_features).squeeze(-1)\n",
    "                scores.append(target_scores)\n",
    "        \n",
    "        if scores:\n",
    "            return torch.cat(scores, dim=0)\n",
    "        else:\n",
    "            # Return empty tensor if no valid targets\n",
    "            device = next(iter(source_nodes.values())).device\n",
    "            return torch.tensor([], device=device)\n",
    "    \n",
    "    def get_embeddings_by_type(self, hg: dgl.DGLGraph) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Return final node embeddings organized by type\n",
    "        \n",
    "        Args:\n",
    "            hg: DGL HeteroGraph with computed node features\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping node types to embedding tensors\n",
    "        \"\"\"\n",
    "        embeddings_by_type = {}\n",
    "        \n",
    "        for ntype in hg.ntypes:\n",
    "            if 'h' in hg.nodes[ntype].data:\n",
    "                embeddings_by_type[ntype] = hg.nodes[ntype].data['h']\n",
    "            elif f'h_{self.num_layers-1}' in hg.nodes[ntype].data:\n",
    "                # Try to get last layer features\n",
    "                embeddings_by_type[ntype] = hg.nodes[ntype].data[f'h_{self.num_layers-1}']\n",
    "        \n",
    "        return embeddings_by_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4999e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroKGDataset(Dataset):\n",
    "    \"\"\"Dataset for heterogeneous knowledge graph triplets\"\"\"\n",
    "    def __init__(self, triplets: List[Tuple[str, str, str]], \n",
    "                 entity_types: Dict[str, str],\n",
    "                 entity_to_id: Dict[str, int],\n",
    "                 num_negatives: int = 32):\n",
    "        self.triplets = triplets\n",
    "        self.entity_types = entity_types\n",
    "        self.entity_to_id = entity_to_id\n",
    "        self.num_negatives = num_negatives\n",
    "        \n",
    "        # Group entities by type for negative sampling\n",
    "        self.entities_by_type = defaultdict(list)\n",
    "        for entity_name, entity_type in entity_types.items():\n",
    "            if entity_name in entity_to_id:\n",
    "                self.entities_by_type[entity_type].append(entity_to_id[entity_name])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        head_name, relation, tail_name = self.triplets[idx]\n",
    "        \n",
    "        if head_name not in self.entity_to_id or tail_name not in self.entity_to_id:\n",
    "            # Return empty sample for invalid triplets\n",
    "            return {\n",
    "                'positive': None,\n",
    "                'negatives': []\n",
    "            }\n",
    "        \n",
    "        head_id = self.entity_to_id[head_name]\n",
    "        tail_id = self.entity_to_id[tail_name]\n",
    "        head_type = self.entity_types[head_name]\n",
    "        tail_type = self.entity_types[tail_name]\n",
    "        \n",
    "        # Generate negative samples\n",
    "        negatives = []\n",
    "        tail_candidates = self.entities_by_type[tail_type]\n",
    "        \n",
    "        if len(tail_candidates) > 1:\n",
    "            for _ in range(min(self.num_negatives, len(tail_candidates) - 1)):\n",
    "                neg_tail = np.random.choice(tail_candidates)\n",
    "                while neg_tail == tail_id:\n",
    "                    neg_tail = np.random.choice(tail_candidates)\n",
    "                negatives.append((head_name, relation, self._id_to_entity_name(neg_tail)))\n",
    "        \n",
    "        return {\n",
    "            'positive': (head_name, relation, tail_name),\n",
    "            'negatives': negatives\n",
    "        }\n",
    "    \n",
    "    def _id_to_entity_name(self, entity_id: int) -> str:\n",
    "        \"\"\"Convert entity ID back to name\"\"\"\n",
    "        for name, eid in self.entity_to_id.items():\n",
    "            if eid == entity_id:\n",
    "                return name\n",
    "        return f\"unknown_{entity_id}\"\n",
    "\n",
    "\n",
    "class HeteroNBFNetTrainer:\n",
    "    \"\"\"Trainer for HeteroNBFNet with batch training support\"\"\"\n",
    "    \n",
    "    def __init__(self, model: HeteroNBFNet, device: str = 'cuda'):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.optimizer = None\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def setup_optimizer(self, lr: float = 5e-3):\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "    \n",
    "    def train_batch(self, hg: dgl.DGLGraph, batch_data: Dict, \n",
    "                   entity_types: Dict[str, str], entity_to_id: Dict[str, int]):\n",
    "        \"\"\"Train on a batch of heterogeneous triplets\"\"\"\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Prepare positive and negative samples\n",
    "        positive_triplets = [item for item in batch_data['positives'] if item is not None]\n",
    "        negative_triplets = []\n",
    "        for negs in batch_data['negatives']:\n",
    "            negative_triplets.extend(negs)\n",
    "        \n",
    "        if not positive_triplets:\n",
    "            return 0.0\n",
    "        \n",
    "        all_triplets = positive_triplets + negative_triplets\n",
    "        \n",
    "        # Group by relation type\n",
    "        triplets_by_relation = defaultdict(list)\n",
    "        for triplet in all_triplets:\n",
    "            head_name, relation, tail_name = triplet\n",
    "            triplets_by_relation[relation].append(triplet)\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        num_relations = 0\n",
    "        \n",
    "        # Process each relation separately\n",
    "        for relation, rel_triplets in triplets_by_relation.items():\n",
    "            # Separate positive and negative for this relation\n",
    "            num_pos_rel = sum(1 for t in positive_triplets if t[1] == relation)\n",
    "            \n",
    "            # Group source and target nodes by type\n",
    "            source_nodes = defaultdict(list)\n",
    "            target_nodes = defaultdict(list)\n",
    "            labels = []\n",
    "            \n",
    "            for i, (head_name, rel, tail_name) in enumerate(rel_triplets):\n",
    "                if head_name in entity_to_id and tail_name in entity_to_id:\n",
    "                    head_id = entity_to_id[head_name]\n",
    "                    tail_id = entity_to_id[tail_name]\n",
    "                    head_type = entity_types[head_name]\n",
    "                    tail_type = entity_types[tail_name]\n",
    "                    \n",
    "                    source_nodes[head_type].append(head_id)\n",
    "                    target_nodes[tail_type].append(tail_id)\n",
    "                    \n",
    "                    # Label: 1 for positive, 0 for negative\n",
    "                    labels.append(1.0 if i < num_pos_rel else 0.0)\n",
    "            \n",
    "            if not labels:\n",
    "                continue\n",
    "            \n",
    "            # Convert to tensors\n",
    "            source_nodes_tensor = {}\n",
    "            target_nodes_tensor = {}\n",
    "            \n",
    "            for ntype, nodes in source_nodes.items():\n",
    "                if nodes:\n",
    "                    source_nodes_tensor[ntype] = torch.tensor(nodes, device=self.device)\n",
    "            \n",
    "            for ntype, nodes in target_nodes.items():\n",
    "                if nodes:\n",
    "                    target_nodes_tensor[ntype] = torch.tensor(nodes, device=self.device)\n",
    "            \n",
    "            labels_tensor = torch.tensor(labels, device=self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            if source_nodes_tensor and target_nodes_tensor:\n",
    "                scores = self.model(hg.to(self.device), source_nodes_tensor, \n",
    "                                  target_nodes_tensor, relation)\n",
    "                \n",
    "                if len(scores) > 0:\n",
    "                    # Compute loss\n",
    "                    loss = self.criterion(scores, labels_tensor)\n",
    "                    total_loss += loss\n",
    "                    num_relations += 1\n",
    "        \n",
    "        if num_relations > 0:\n",
    "            # Average loss across relations\n",
    "            avg_loss = total_loss / num_relations\n",
    "            \n",
    "            # Backward pass\n",
    "            avg_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            return avg_loss.item()\n",
    "        \n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def collate_hetero_batch(batch):\n",
    "    \"\"\"Collate function for HeteroKGDataset\"\"\"\n",
    "    positives = []\n",
    "    negatives = []\n",
    "    \n",
    "    for item in batch:\n",
    "        if item['positive'] is not None:\n",
    "            positives.append(item['positive'])\n",
    "        negatives.append(item['negatives'])\n",
    "    \n",
    "    return {\n",
    "        'positives': positives,\n",
    "        'negatives': negatives\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage and training script\n",
    "def train_hetero_nbfnet(hg: dgl.DGLGraph,\n",
    "                       train_triplets: List[Tuple[str, str, str]], \n",
    "                       entity_types: Dict[str, str],\n",
    "                       val_triplets: Optional[List[Tuple[str, str, str]]] = None,\n",
    "                       batch_size: int = 256, num_epochs: int = 20,\n",
    "                       embedding_dim: int = 32, num_layers: int = 6,\n",
    "                       lr: float = 5e-3, device: str = 'cuda'):\n",
    "    \"\"\"\n",
    "    Complete training pipeline for HeteroNBFNet\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create entity mappings\n",
    "    entity_to_id = {}\n",
    "    entity_id = 0\n",
    "    \n",
    "    for triplet in train_triplets + (val_triplets or []):\n",
    "        for entity in [triplet[0], triplet[2]]:  # head and tail\n",
    "            if entity not in entity_to_id:\n",
    "                entity_to_id[entity] = entity_id\n",
    "                entity_id += 1\n",
    "    \n",
    "    print(f\"Total entities: {len(entity_to_id)}\")\n",
    "    print(f\"Entity types: {set(entity_types.values())}\")\n",
    "    \n",
    "    print(f\"Graph created with:\")\n",
    "    print(f\"  Node types: {hg.ntypes}\")\n",
    "    print(f\"  Edge types: {hg.canonical_etypes}\")\n",
    "    for ntype in hg.ntypes:\n",
    "        print(f\"  {ntype}: {hg.number_of_nodes(ntype)} nodes\")\n",
    "    \n",
    "    # Create model\n",
    "    print(\"Initializing HeteroNBFNet...\")\n",
    "    model = HeteroNBFNet(\n",
    "        node_types=hg.ntypes,\n",
    "        edge_types=hg.canonical_etypes,\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_layers=num_layers\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = HeteroNBFNetTrainer(model, device)\n",
    "    trainer.setup_optimizer(lr)\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = HeteroKGDataset(train_triplets, entity_types, entity_to_id)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=collate_hetero_batch\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            loss = trainer.train_batch(hg, batch, entity_types, entity_to_id)\n",
    "            total_loss += loss\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / max(num_batches, 1)\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {avg_loss:.4f}\")\n",
    "    \n",
    "    return model, trainer, hg, entity_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a650bb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplets_vectorized(g, mapping):\n",
    "    triplets = []\n",
    "    \n",
    "    for canonical_etype in g.canonical_etypes:\n",
    "        src_ntype, etype, dst_ntype = canonical_etype\n",
    "        src, dst = g.edges(etype=canonical_etype)\n",
    "        \n",
    "        # Create triplets as tuples directly\n",
    "        edge_triplets = list(zip(src.tolist(), [mapping[etype]] * len(src), dst.tolist()))\n",
    "        triplets.extend(edge_triplets)\n",
    "    \n",
    "    return triplets\n",
    "\n",
    "def create_dict_from_tuples(tuples_array):\n",
    "    # Get unique keys in one pass\n",
    "    keys = {t[0] for t in tuples_array}\n",
    "    \n",
    "    # Pre-allocate dictionary with empty lists\n",
    "    result = {key: [] for key in keys}\n",
    "    \n",
    "    # Single pass to populate\n",
    "    for key_string, first_int  in tuples_array:\n",
    "        result[key_string].append(first_int)\n",
    "    \n",
    "    return result\n",
    "\n",
    "gdb = DeepGraphDB()\n",
    "gdb.load_graph(\"/home/cc/PHD/dglframework/DeepKG/DeepGraphDB/graphs/primekg.bin\")\n",
    "\n",
    "entity_types = {  }\n",
    "\n",
    "triplets = list(set(get_triplets_vectorized(gdb.graph, gdb.edge_types_mapping)))\n",
    "\n",
    "# Split data\n",
    "random.shuffle(triplets)\n",
    "split_idx = int(0.7 * len(triplets))\n",
    "train_triplets = triplets[:split_idx]\n",
    "val_triplets = triplets[split_idx:]\n",
    "\n",
    "# Train model\n",
    "model, trainer, hg, entity_to_id = train_hetero_nbfnet(\n",
    "    hg=gdb.graph,\n",
    "    train_triplets=train_triplets,\n",
    "    entity_types=create_dict_from_tuples(gdb.reverse_node_mapping.keys()),\n",
    "    val_triplets=val_triplets,\n",
    "    batch_size=32000,\n",
    "    num_epochs=50,\n",
    "    device='cuda'  # Use 'cuda' if available\n",
    ")\n",
    "\n",
    "print(\"Training completed successfully!\")\n",
    "print(\"HeteroNBFNet with DGL HeteroGraph is working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a79501",
   "metadata": {},
   "outputs": [],
   "source": [
    "en = create_dict_from_tuples(gdb.reverse_node_mapping.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d28f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "en.values()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
