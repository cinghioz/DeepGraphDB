{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878fd3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "from dgl.nn import GraphConv\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from DeepGraphDB import DeepGraphDB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class HeteroEntityEmbedding(nn.Module):\n",
    "    \"\"\"Handles embeddings for heterogeneous entity types\"\"\"\n",
    "    \n",
    "    def __init__(self, entity_types: List[str], node_counts: Dict[str, int], \n",
    "                 embedding_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.entity_types = entity_types\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.node_counts = node_counts\n",
    "        \n",
    "        # Create separate embedding tables for each entity type\n",
    "        self.embeddings = nn.ModuleDict()\n",
    "        for entity_type in entity_types:\n",
    "            if node_counts[entity_type] > 0:\n",
    "                self.embeddings[entity_type] = nn.Embedding(\n",
    "                    node_counts[entity_type], embedding_dim\n",
    "                )\n",
    "                nn.init.xavier_uniform_(self.embeddings[entity_type].weight)\n",
    "    \n",
    "    def forward(self, entity_ids: torch.Tensor, entity_types: List[str]) -> torch.Tensor:\n",
    "        \"\"\"Get embeddings for entities of different types\"\"\"\n",
    "        batch_size = entity_ids.size(0)\n",
    "        embeddings = torch.zeros(batch_size, self.embedding_dim, \n",
    "                                device=entity_ids.device, dtype=torch.float)\n",
    "        \n",
    "        # Group entities by type for efficient lookup\n",
    "        type_to_indices = {}\n",
    "        for i, etype in enumerate(entity_types):\n",
    "            if etype not in type_to_indices:\n",
    "                type_to_indices[etype] = []\n",
    "            type_to_indices[etype].append(i)\n",
    "        \n",
    "        # Get embeddings for each type\n",
    "        for etype, indices in type_to_indices.items():\n",
    "            if etype in self.embeddings:\n",
    "                indices_tensor = torch.tensor(indices, device=entity_ids.device, dtype=torch.long)\n",
    "                entity_subset = entity_ids[indices_tensor]\n",
    "                embeddings[indices_tensor] = self.embeddings[etype](entity_subset)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "class RelationEmbedding(nn.Module):\n",
    "    \"\"\"Handles relation embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, relation_types: List[str], embedding_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.relation_types = relation_types\n",
    "        self.num_relations = len(relation_types)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.relation_embedding = nn.Embedding(self.num_relations, embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.relation_embedding.weight)\n",
    "    \n",
    "    def forward(self, relation_ids: torch.Tensor) -> torch.Tensor:\n",
    "        return self.relation_embedding(relation_ids)\n",
    "\n",
    "class NBFLayer(nn.Module):\n",
    "    \"\"\"Neural Bellman-Ford Layer for message passing\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_relations: int, \n",
    "                 message_func: str = 'distmult'):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_relations = num_relations\n",
    "        self.message_func = message_func\n",
    "        \n",
    "        # Message function layers\n",
    "        if message_func == 'distmult':\n",
    "            self.message_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        elif message_func == 'rotate':\n",
    "            self.message_layer = nn.Linear(input_dim * 2, hidden_dim)\n",
    "        else:\n",
    "            self.message_layer = nn.Linear(input_dim + input_dim, hidden_dim)\n",
    "        \n",
    "        # Relation-specific transformation\n",
    "        self.relation_linear = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Update function\n",
    "        self.update_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Layer norm\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "    def message_function(self, head_emb: torch.Tensor, rel_emb: torch.Tensor, \n",
    "                        tail_emb: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute messages based on the specified function\"\"\"\n",
    "        if self.message_func == 'distmult':\n",
    "            # Element-wise product\n",
    "            message = head_emb * rel_emb\n",
    "        elif self.message_func == 'rotate':\n",
    "            # Rotation in complex space (simplified)\n",
    "            real_head = head_emb[:, :self.input_dim//2]\n",
    "            imag_head = head_emb[:, self.input_dim//2:]\n",
    "            real_rel = rel_emb[:, :self.input_dim//2]\n",
    "            imag_rel = rel_emb[:, self.input_dim//2:]\n",
    "            \n",
    "            real_msg = real_head * real_rel - imag_head * imag_rel\n",
    "            imag_msg = real_head * imag_rel + imag_head * real_rel\n",
    "            message = torch.cat([real_msg, imag_msg], dim=-1)\n",
    "        else:\n",
    "            # Concatenation\n",
    "            message = torch.cat([head_emb, rel_emb], dim=-1)\n",
    "        \n",
    "        return self.message_layer(message)\n",
    "    \n",
    "    def forward(self, head_emb: torch.Tensor, rel_emb: torch.Tensor, \n",
    "                prev_emb: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of NBF layer\"\"\"\n",
    "        # Compute message\n",
    "        message = self.message_function(head_emb, rel_emb, prev_emb)\n",
    "        \n",
    "        # Update step\n",
    "        updated = self.update_layer(torch.cat([message, prev_emb], dim=-1))\n",
    "        \n",
    "        # Add residual connection and layer norm\n",
    "        output = self.layer_norm(updated + prev_emb)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class SimpleNBFNet(nn.Module):\n",
    "    \"\"\"Simplified NBFNet for heterogeneous knowledge graphs\"\"\"\n",
    "    \n",
    "    def __init__(self, entity_types: List[str], relation_types: List[str],\n",
    "                 node_counts: Dict[str, int], embedding_dim: int = 256, \n",
    "                 hidden_dim: int = 256, num_layers: int = 3,\n",
    "                 message_func: str = 'distmult', dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.entity_types = entity_types\n",
    "        self.relation_types = relation_types\n",
    "        self.node_counts = node_counts\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Embeddings\n",
    "        self.entity_embedding = HeteroEntityEmbedding(\n",
    "            entity_types, node_counts, embedding_dim\n",
    "        )\n",
    "        self.relation_embedding = RelationEmbedding(\n",
    "            relation_types, embedding_dim\n",
    "        )\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = nn.Linear(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # NBF layers\n",
    "        self.nbf_layers = nn.ModuleList([\n",
    "            NBFLayer(hidden_dim, hidden_dim, len(relation_types), message_func)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output layers\n",
    "        self.output_projection = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, triplets: torch.Tensor, head_types: List[str], \n",
    "                tail_types: List[str]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for batch of triplets\n",
    "        \n",
    "        Args:\n",
    "            triplets: (batch_size, 3) tensor of [head, relation, tail]\n",
    "            head_types: List of head entity types for each triplet\n",
    "            tail_types: List of tail entity types for each triplet\n",
    "        \n",
    "        Returns:\n",
    "            scores: (batch_size,) tensor of scores\n",
    "        \"\"\"\n",
    "        batch_size = triplets.size(0)\n",
    "        \n",
    "        # Extract components\n",
    "        heads = triplets[:, 0]\n",
    "        relations = triplets[:, 1]\n",
    "        tails = triplets[:, 2]\n",
    "        \n",
    "        # Get embeddings\n",
    "        head_emb = self.entity_embedding(heads, head_types)\n",
    "        rel_emb = self.relation_embedding(relations)\n",
    "        tail_emb = self.entity_embedding(tails, tail_types)\n",
    "        \n",
    "        # Project to hidden dimension\n",
    "        head_emb = self.input_projection(head_emb)\n",
    "        rel_emb = self.input_projection(rel_emb)\n",
    "        tail_emb = self.input_projection(tail_emb)\n",
    "        \n",
    "        # Apply dropout\n",
    "        head_emb = self.dropout_layer(head_emb)\n",
    "        rel_emb = self.dropout_layer(rel_emb)\n",
    "        tail_emb = self.dropout_layer(tail_emb)\n",
    "        \n",
    "        # Initialize with tail embeddings (for link prediction h,r,?)\n",
    "        current_emb = tail_emb\n",
    "        \n",
    "        # Apply NBF layers\n",
    "        for layer in self.nbf_layers:\n",
    "            current_emb = layer(head_emb, rel_emb, current_emb)\n",
    "            current_emb = self.dropout_layer(current_emb)\n",
    "        \n",
    "        # Compute final scores\n",
    "        scores = self.output_projection(current_emb).squeeze(-1)\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def get_embeddings(self, entity_ids: torch.Tensor, \n",
    "                      entity_types: List[str]) -> torch.Tensor:\n",
    "        \"\"\"Get entity embeddings for given entities\"\"\"\n",
    "        return self.entity_embedding(entity_ids, entity_types)\n",
    "\n",
    "class NBFNetTrainer:\n",
    "    \"\"\"Training utilities for NBFNet\"\"\"\n",
    "    \n",
    "    def __init__(self, model: SimpleNBFNet, learning_rate: float = 1e-3,\n",
    "                 weight_decay: float = 1e-5):\n",
    "        self.model = model\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    "        )\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def train_step(self, pos_triplets: torch.Tensor, neg_triplets: torch.Tensor,\n",
    "                   pos_head_types: List[str], pos_tail_types: List[str],\n",
    "                   neg_head_types: List[str], neg_tail_types: List[str]) -> float:\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        pos_scores = self.model(pos_triplets, pos_head_types, pos_tail_types)\n",
    "        neg_scores = self.model(neg_triplets, neg_head_types, neg_tail_types)\n",
    "        \n",
    "        # Create labels\n",
    "        pos_labels = torch.ones(pos_scores.size(0), device=pos_scores.device)\n",
    "        neg_labels = torch.zeros(neg_scores.size(0), device=neg_scores.device)\n",
    "        \n",
    "        # Compute loss\n",
    "        all_scores = torch.cat([pos_scores, neg_scores], dim=0)\n",
    "        all_labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
    "        \n",
    "        loss = self.criterion(all_scores, all_labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def evaluate(self, pos_triplets: torch.Tensor, neg_triplets: torch.Tensor,\n",
    "                pos_head_types: List[str], pos_tail_types: List[str],\n",
    "                neg_head_types: List[str], neg_tail_types: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            pos_scores = self.model(pos_triplets, pos_head_types, pos_tail_types)\n",
    "            neg_scores = self.model(neg_triplets, neg_head_types, neg_tail_types)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            pos_preds = (torch.sigmoid(pos_scores) > 0.5).float()\n",
    "            neg_preds = (torch.sigmoid(neg_scores) > 0.5).float()\n",
    "            pos_acc = pos_preds.mean().item()\n",
    "            neg_acc = (1 - neg_preds).mean().item()\n",
    "            total_acc = (pos_acc + neg_acc) / 2\n",
    "            \n",
    "            # Compute AUC\n",
    "            # Combine positive and negative scores\n",
    "            all_scores = torch.cat([pos_scores, neg_scores])\n",
    "            all_labels = torch.cat([torch.ones_like(pos_scores), torch.zeros_like(neg_scores)])\n",
    "            \n",
    "            # Convert to numpy for sklearn\n",
    "            scores_np = torch.sigmoid(all_scores).cpu().numpy()\n",
    "            labels_np = all_labels.cpu().numpy()\n",
    "            \n",
    "            # Calculate AUC\n",
    "            auc = roc_auc_score(labels_np, scores_np)\n",
    "            \n",
    "            return {\n",
    "                'pos_accuracy': pos_acc,\n",
    "                'neg_accuracy': neg_acc,\n",
    "                'total_accuracy': total_acc,\n",
    "                'avg_pos_score': torch.sigmoid(pos_scores).mean().item(),\n",
    "                'avg_neg_score': torch.sigmoid(neg_scores).mean().item(),\n",
    "                'auc': auc\n",
    "            }   \n",
    "\n",
    "def prepare_batch_data(triplets: List[Tuple], triplets_ntypes: List[Tuple], \n",
    "                      batch_size: int = 1024) -> List[Tuple]:\n",
    "    \"\"\"Prepare batched data for training\"\"\"\n",
    "    batches = []\n",
    "    \n",
    "    for i in range(0, len(triplets), batch_size):\n",
    "        batch_triplets = triplets[i:i+batch_size]\n",
    "        batch_ntypes = triplets_ntypes[i:i+batch_size] if triplets_ntypes else None\n",
    "        \n",
    "        # Convert to tensors\n",
    "        triplet_tensor = torch.tensor(batch_triplets, dtype=torch.long)\n",
    "        \n",
    "        if batch_ntypes:\n",
    "            head_types = [nt[0] for nt in batch_ntypes]\n",
    "            tail_types = [nt[1] for nt in batch_ntypes]\n",
    "        else:\n",
    "            head_types = ['entity'] * len(batch_triplets)\n",
    "            tail_types = ['entity'] * len(batch_triplets)\n",
    "        \n",
    "        batches.append((triplet_tensor, head_types, tail_types))\n",
    "    \n",
    "    return batches\n",
    "\n",
    "# Example usage and training loop\n",
    "def train_nbfnet(pos_triplets, pos_triplets_ntypes, neg_triplets, neg_triplets_ntypes,\n",
    "                 entity_types, relation_types, node_counts, \n",
    "                 num_epochs: int = 100, batch_size: int = 1024):\n",
    "    \"\"\"Complete training function\"\"\"\n",
    "    \n",
    "    # Initialize model\n",
    "    model = SimpleNBFNet(\n",
    "        entity_types=entity_types,\n",
    "        relation_types=relation_types,\n",
    "        node_counts=node_counts,\n",
    "        embedding_dim=256,\n",
    "        hidden_dim=256,\n",
    "        num_layers=3,\n",
    "        message_func='distmult'\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = NBFNetTrainer(model, learning_rate=1e-3)\n",
    "    \n",
    "    # Prepare batched data\n",
    "    pos_batches = prepare_batch_data(pos_triplets, pos_triplets_ntypes, batch_size)\n",
    "    neg_batches = prepare_batch_data(neg_triplets, neg_triplets_ntypes, batch_size)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = min(len(pos_batches), len(neg_batches))\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            pos_batch = pos_batches[i % len(pos_batches)]\n",
    "            neg_batch = neg_batches[i % len(neg_batches)]\n",
    "            \n",
    "            pos_triplets_batch, pos_head_types, pos_tail_types = pos_batch\n",
    "            neg_triplets_batch, neg_head_types, neg_tail_types = neg_batch\n",
    "            \n",
    "            # Ensure same batch size\n",
    "            min_batch_size = min(pos_triplets_batch.size(0), neg_triplets_batch.size(0))\n",
    "            pos_triplets_batch = pos_triplets_batch[:min_batch_size]\n",
    "            neg_triplets_batch = neg_triplets_batch[:min_batch_size]\n",
    "            pos_head_types = pos_head_types[:min_batch_size]\n",
    "            pos_tail_types = pos_tail_types[:min_batch_size]\n",
    "            neg_head_types = neg_head_types[:min_batch_size]\n",
    "            neg_tail_types = neg_tail_types[:min_batch_size]\n",
    "            \n",
    "            loss = trainer.train_step(\n",
    "                pos_triplets_batch, neg_triplets_batch,\n",
    "                pos_head_types, pos_tail_types,\n",
    "                neg_head_types, neg_tail_types\n",
    "            )\n",
    "            total_loss += loss\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            # Evaluate on a sample\n",
    "            eval_metrics = trainer.evaluate(\n",
    "                pos_triplets_batch, neg_triplets_batch,\n",
    "                pos_head_types, pos_tail_types,\n",
    "                neg_head_types, neg_tail_types\n",
    "            )\n",
    "            print(f\"Epoch {epoch}: Loss={total_loss/num_batches:.4f}, \"\n",
    "                  f\"Acc={eval_metrics['total_accuracy']:.4f}\")\n",
    "    \n",
    "    return model, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed5d4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplets_vectorized(g, edge_mapping):\n",
    "    \"\"\"Extract triplets from heterogeneous DGL graph with efficient negative sampling.\"\"\"\n",
    "    triplets = []\n",
    "    triplets_ntypes = []\n",
    "    neg_triplets = []\n",
    "    neg_triplets_ntypes = []\n",
    "    \n",
    "    if hasattr(g, 'canonical_etypes'):\n",
    "        # Pre-compute invalid entity types for each relation\n",
    "        relation_invalid_heads = {}\n",
    "        relation_invalid_tails = {}\n",
    "        node_type_ranges = {}\n",
    "        \n",
    "        # Cache node counts for each type\n",
    "        for ntype in g.ntypes:\n",
    "            node_type_ranges[ntype] = g.num_nodes(ntype)\n",
    "        \n",
    "        # Pre-compute invalid types for each relation\n",
    "        for canonical_etype in g.canonical_etypes:\n",
    "            src_ntype, etype, dst_ntype = canonical_etype\n",
    "            \n",
    "            if etype not in relation_invalid_heads:\n",
    "                # Find valid head types for this relation\n",
    "                valid_head_types = set()\n",
    "                valid_tail_types = set()\n",
    "                for canon_et in g.canonical_etypes:\n",
    "                    if canon_et[1] == etype:\n",
    "                        valid_head_types.add(canon_et[0])\n",
    "                        valid_tail_types.add(canon_et[2])\n",
    "                \n",
    "                # Get invalid types\n",
    "                all_node_types = set(g.ntypes)\n",
    "                relation_invalid_heads[etype] = list(all_node_types - valid_head_types)\n",
    "                relation_invalid_tails[etype] = list(all_node_types - valid_tail_types)\n",
    "        \n",
    "        # Process each relation type\n",
    "        for canonical_etype in g.canonical_etypes:\n",
    "            src_ntype, etype, dst_ntype = canonical_etype\n",
    "            src, dst = g.edges(etype=canonical_etype)\n",
    "            \n",
    "            if len(src) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Map relation name to index\n",
    "            relation_idx = edge_mapping[etype]\n",
    "            edge_triplets = list(zip(src.tolist(), [relation_idx] * len(src), dst.tolist()))\n",
    "            triplets.extend(edge_triplets)\n",
    "            triplets_ntypes.extend(list(zip([src_ntype] * len(src), [dst_ntype] * len(dst))))\n",
    "            \n",
    "            # Vectorized negative triplet generation\n",
    "            num_edges = len(src)\n",
    "            src_numpy = src.numpy()\n",
    "            dst_numpy = dst.numpy()\n",
    "            \n",
    "            # Random choice for head vs tail corruption (vectorized)\n",
    "            corrupt_head_mask = np.random.random(num_edges) < 0.5\n",
    "            \n",
    "            # Generate corrupted heads\n",
    "            invalid_head_types = relation_invalid_heads[etype]\n",
    "            if invalid_head_types and np.any(corrupt_head_mask):\n",
    "                head_corruption_indices = np.where(corrupt_head_mask)[0]\n",
    "                \n",
    "                # Vectorized selection of corrupted head types\n",
    "                corrupted_head_types = np.random.choice(\n",
    "                    invalid_head_types, \n",
    "                    size=len(head_corruption_indices)\n",
    "                )\n",
    "                \n",
    "                # Vectorized generation of corrupted head IDs\n",
    "                corrupted_head_ids = np.array([\n",
    "                    np.random.randint(0, node_type_ranges[head_type]) \n",
    "                    if node_type_ranges[head_type] > 0 else 0\n",
    "                    for head_type in corrupted_head_types\n",
    "                ])\n",
    "                \n",
    "                # Create negative triplets for head corruption\n",
    "                valid_mask = np.array([node_type_ranges[ht] > 0 for ht in corrupted_head_types])\n",
    "                if np.any(valid_mask):\n",
    "                    valid_indices = head_corruption_indices[valid_mask]\n",
    "                    valid_corrupted_heads = corrupted_head_ids[valid_mask]\n",
    "                    valid_corrupted_head_types = corrupted_head_types[valid_mask]\n",
    "                    \n",
    "                    head_neg_triplets = list(zip(\n",
    "                        valid_corrupted_heads.tolist(),\n",
    "                        [relation_idx] * len(valid_indices),\n",
    "                        dst_numpy[valid_indices].tolist()\n",
    "                    ))\n",
    "                    neg_triplets.extend(head_neg_triplets)\n",
    "                    neg_triplets_ntypes.extend(list(zip(\n",
    "                        valid_corrupted_head_types.tolist(),\n",
    "                        [dst_ntype] * len(valid_indices)\n",
    "                    )))\n",
    "            \n",
    "            # Generate corrupted tails\n",
    "            invalid_tail_types = relation_invalid_tails[etype]\n",
    "            if invalid_tail_types and np.any(~corrupt_head_mask):\n",
    "                tail_corruption_indices = np.where(~corrupt_head_mask)[0]\n",
    "                \n",
    "                # Vectorized selection of corrupted tail types\n",
    "                corrupted_tail_types = np.random.choice(\n",
    "                    invalid_tail_types, \n",
    "                    size=len(tail_corruption_indices)\n",
    "                )\n",
    "                \n",
    "                # Vectorized generation of corrupted tail IDs\n",
    "                corrupted_tail_ids = np.array([\n",
    "                    np.random.randint(0, node_type_ranges[tail_type]) \n",
    "                    if node_type_ranges[tail_type] > 0 else 0\n",
    "                    for tail_type in corrupted_tail_types\n",
    "                ])\n",
    "                \n",
    "                # Create negative triplets for tail corruption\n",
    "                valid_mask = np.array([node_type_ranges[tt] > 0 for tt in corrupted_tail_types])\n",
    "                if np.any(valid_mask):\n",
    "                    valid_indices = tail_corruption_indices[valid_mask]\n",
    "                    valid_corrupted_tails = corrupted_tail_ids[valid_mask]\n",
    "                    valid_corrupted_tail_types = corrupted_tail_types[valid_mask]\n",
    "                    \n",
    "                    tail_neg_triplets = list(zip(\n",
    "                        src_numpy[valid_indices].tolist(),\n",
    "                        [relation_idx] * len(valid_indices),\n",
    "                        valid_corrupted_tails.tolist()\n",
    "                    ))\n",
    "                    neg_triplets.extend(tail_neg_triplets)\n",
    "                    neg_triplets_ntypes.extend(list(zip(\n",
    "                        [src_ntype] * len(valid_indices),\n",
    "                        valid_corrupted_tail_types.tolist()\n",
    "                    )))\n",
    "    \n",
    "    else:\n",
    "        # Homogeneous graph - vectorized version\n",
    "        src, dst = g.edges()\n",
    "        edge_triplets = list(zip(src.tolist(), [0] * len(src), dst.tolist()))\n",
    "        triplets.extend(edge_triplets)\n",
    "        \n",
    "        # Vectorized negative sampling for homogeneous graphs\n",
    "        num_edges = len(src)\n",
    "        total_nodes = g.num_nodes()\n",
    "        \n",
    "        if total_nodes > 0 and num_edges > 0:\n",
    "            src_numpy = src.numpy()\n",
    "            dst_numpy = dst.numpy()\n",
    "            \n",
    "            # Random choice for head vs tail corruption\n",
    "            corrupt_head_mask = np.random.random(num_edges) < 0.5\n",
    "            \n",
    "            # Generate all corrupted nodes at once\n",
    "            corrupted_heads = np.random.randint(0, total_nodes, size=num_edges)\n",
    "            corrupted_tails = np.random.randint(0, total_nodes, size=num_edges)\n",
    "            \n",
    "            # Apply corruption based on mask\n",
    "            neg_src = np.where(corrupt_head_mask, corrupted_heads, src_numpy)\n",
    "            neg_dst = np.where(corrupt_head_mask, dst_numpy, corrupted_tails)\n",
    "            \n",
    "            neg_triplets = list(zip(\n",
    "                neg_src.tolist(),\n",
    "                [0] * num_edges,\n",
    "                neg_dst.tolist()\n",
    "            ))\n",
    "    \n",
    "    return triplets, triplets_ntypes, neg_triplets, neg_triplets_ntypes\n",
    "\n",
    "def get_entity_embeddings(model, entity_ids, entity_types):\n",
    "    \"\"\"Extract learned entity embeddings\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    if isinstance(entity_ids, list):\n",
    "        entity_ids = torch.tensor(entity_ids, dtype=torch.long)\n",
    "    \n",
    "    entity_ids = entity_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings = model.get_embeddings(entity_ids, entity_types)\n",
    "    \n",
    "    return embeddings.cpu().numpy()\n",
    "\n",
    "def predict_triplet_scores(model, triplets, head_types, tail_types):\n",
    "    \"\"\"Predict scores for given triplets\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    if isinstance(triplets, list):\n",
    "        triplets = torch.tensor(triplets, dtype=torch.long)\n",
    "    \n",
    "    triplets = triplets.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores = model(triplets, head_types, tail_types)\n",
    "        probabilities = torch.sigmoid(scores)\n",
    "    \n",
    "    return scores.cpu().numpy(), probabilities.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e810b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "gdb = DeepGraphDB()\n",
    "gdb.load_graph(\"/home/cc/PHD/dglframework/DeepKG/DeepGraphDB/graphs/primekg.bin\")\n",
    "\n",
    "# Extract graph information\n",
    "num_entities = gdb.graph.number_of_nodes()\n",
    "\n",
    "# relation_types = list(set([etype[1] for etype in gdb.graph.canonical_etypes]))\n",
    "# relation_to_idx = {rel: idx for idx, rel in enumerate(relation_types)}\n",
    "\n",
    "# Extract triplets\n",
    "pos_triplets, pos_triplets_ntypes, neg_triplets, neg_triplets_ntypes = get_triplets_vectorized(gdb.graph, gdb.edge_types_mapping)\n",
    "\n",
    "print(f\"Graph stats: {num_entities} entities, {len(gdb.edge_types_mapping)} relations, {len(pos_triplets)} triplets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6f2f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(gdb.graph, 'ntypes'):\n",
    "    # Heterogeneous graph\n",
    "    entity_types = list(gdb.graph.ntypes)\n",
    "    node_counts = {ntype: gdb.graph.num_nodes(ntype) for ntype in entity_types}\n",
    "    \n",
    "    # Get relation types from canonical edge types\n",
    "    if hasattr(gdb.graph, 'canonical_etypes'):\n",
    "        relation_types = list(set([etype[1] for etype in gdb.graph.canonical_etypes]))\n",
    "    else:\n",
    "        relation_types = list(gdb.edge_types_mapping.keys()) if hasattr(gdb, 'edge_types_mapping') else ['default_relation']\n",
    "else:\n",
    "    # Homogeneous graph\n",
    "    entity_types = ['entity']\n",
    "    node_counts = {'entity': gdb.graph.num_nodes()}\n",
    "    relation_types = ['relation']\n",
    "\n",
    "print(f\"Entity types: {entity_types}\")\n",
    "print(f\"Node counts: {node_counts}\")\n",
    "print(f\"Relation types: {relation_types}\")\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleNBFNet(\n",
    "    entity_types=entity_types,\n",
    "    relation_types=relation_types,\n",
    "    node_counts=node_counts,\n",
    "    embedding_dim=256,  # Reduced for faster training\n",
    "    hidden_dim=256,\n",
    "    num_layers=6,       # Reduced for faster training\n",
    "    message_func='distmult',\n",
    "    dropout=0.25\n",
    ").to(device)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = NBFNetTrainer(model, learning_rate=1e-3, weight_decay=1e-5)\n",
    "\n",
    "# Prepare data\n",
    "batch_size = 250000\n",
    "pos_batches = prepare_batch_data(pos_triplets, pos_triplets_ntypes, batch_size)\n",
    "neg_batches = prepare_batch_data(neg_triplets, neg_triplets_ntypes, batch_size)\n",
    "\n",
    "print(f\"Prepared {len(pos_batches)} positive batches and {len(neg_batches)} negative batches\")\n",
    "\n",
    "# Training loop with progress tracking\n",
    "num_epochs = 100\n",
    "best_acc = 0.0\n",
    "\n",
    "#TODO: Split triples based on entity types or relation types\n",
    "\n",
    "random.shuffle(pos_batches)\n",
    "random.shuffle(neg_batches)\n",
    "\n",
    "eval_pos_batches = pos_batches[:5]\n",
    "eval_neg_batches = neg_batches[:5]\n",
    "\n",
    "train_pos_batches = pos_batches[5:]\n",
    "train_neg_batches = neg_batches[5:]\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = min(len(train_pos_batches), len(train_neg_batches))\n",
    "    \n",
    "    for i in range(num_batches):  # Limit batches per epoch for speed\n",
    "        pos_batch = train_pos_batches[i % len(train_pos_batches)]\n",
    "        neg_batch = train_neg_batches[i % len(train_neg_batches)]\n",
    "        \n",
    "        pos_triplets_batch, pos_head_types, pos_tail_types = pos_batch\n",
    "        neg_triplets_batch, neg_head_types, neg_tail_types = neg_batch\n",
    "        \n",
    "        # Move to device\n",
    "        pos_triplets_batch = pos_triplets_batch.to(device)\n",
    "        neg_triplets_batch = neg_triplets_batch.to(device)\n",
    "        \n",
    "        # Ensure same batch size\n",
    "        min_batch_size = min(pos_triplets_batch.size(0), neg_triplets_batch.size(0))\n",
    "        pos_triplets_batch = pos_triplets_batch[:min_batch_size]\n",
    "        neg_triplets_batch = neg_triplets_batch[:min_batch_size]\n",
    "        pos_head_types = pos_head_types[:min_batch_size]\n",
    "        pos_tail_types = pos_tail_types[:min_batch_size]\n",
    "        neg_head_types = neg_head_types[:min_batch_size]\n",
    "        neg_tail_types = neg_tail_types[:min_batch_size]\n",
    "        \n",
    "        loss = trainer.train_step(\n",
    "            pos_triplets_batch, neg_triplets_batch,\n",
    "            pos_head_types, pos_tail_types,\n",
    "            neg_head_types, neg_tail_types\n",
    "        )\n",
    "        total_loss += loss\n",
    "\n",
    "    evals = {\n",
    "        'total_accuracy': np.array([0.0]),\n",
    "        'auc': np.array([0.0]),\n",
    "        'pos_accuracy': np.array([0.0]),\n",
    "        'neg_accuracy': np.array([0.0]),\n",
    "        'avg_pos_score': np.array([0.0]),\n",
    "        'avg_neg_score': np.array([0.0])\n",
    "    }\n",
    "\n",
    "    # Evaluate every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.eval()\n",
    "        # Evaluate on a sample batch\n",
    "        for eval_pos_batch, eval_neg_batch in zip(eval_pos_batches, eval_neg_batches):\n",
    "            eval_pos_triplets, eval_pos_head_types, eval_pos_tail_types = eval_pos_batch\n",
    "            eval_neg_triplets, eval_neg_head_types, eval_neg_tail_types = eval_neg_batch\n",
    "            \n",
    "            eval_pos_triplets = eval_pos_triplets.to(device)\n",
    "            eval_neg_triplets = eval_neg_triplets.to(device)\n",
    "            \n",
    "            min_eval_size = min(eval_pos_triplets.size(0), eval_neg_triplets.size(0))\n",
    "            eval_pos_triplets = eval_pos_triplets[:min_eval_size]\n",
    "            eval_neg_triplets = eval_neg_triplets[:min_eval_size]\n",
    "            eval_pos_head_types = eval_pos_head_types[:min_eval_size]\n",
    "            eval_pos_tail_types = eval_pos_tail_types[:min_eval_size]\n",
    "            eval_neg_head_types = eval_neg_head_types[:min_eval_size]\n",
    "            eval_neg_tail_types = eval_neg_tail_types[:min_eval_size]\n",
    "            \n",
    "            eval_metrics = trainer.evaluate(\n",
    "                eval_pos_triplets, eval_neg_triplets,\n",
    "                eval_pos_head_types, eval_pos_tail_types,\n",
    "                eval_neg_head_types, eval_neg_tail_types\n",
    "            )\n",
    "            \n",
    "            # Store mean accuracy for this epoch and calculate AUC\n",
    "            evals['total_accuracy'] = np.append(evals['total_accuracy'], eval_metrics['total_accuracy'])\n",
    "            evals['pos_accuracy'] = np.append(evals['pos_accuracy'], eval_metrics['pos_accuracy'])\n",
    "            evals['neg_accuracy'] = np.append(evals['neg_accuracy'], eval_metrics['neg_accuracy'])\n",
    "            evals['avg_pos_score'] = np.append(evals['avg_pos_score'], eval_metrics['avg_pos_score'])\n",
    "            evals['avg_neg_score'] = np.append(evals['avg_neg_score'], eval_metrics['avg_neg_score'])     \n",
    "            evals['auc'] = np.append(evals['auc'], eval_metrics['auc'])\n",
    "\n",
    "        current_acc = evals['total_accuracy'].mean()\n",
    "        if current_acc > best_acc:\n",
    "            best_acc = current_acc\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), 'best_nbfnet_model.pth')\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Loss={total_loss/num_batches:.4f}, \"\n",
    "            f\"Acc={current_acc:.4f} (Best: {best_acc:.4f}) - AUC={evals['auc'].mean():.4f}\")\n",
    "        print(f\"  Pos Acc: {evals['pos_accuracy'].mean():.4f}, \"\n",
    "            f\"Neg Acc: {evals['neg_accuracy'].mean():.4f}\")\n",
    "        print(f\"  Avg Pos Score: {evals['avg_pos_score'].mean():.4f}, \"\n",
    "            f\"Avg Neg Score: {evals['avg_neg_score'].mean():.4f}\")\n",
    "    \n",
    "# Example: Get embeddings for specific entities\n",
    "sample_entity_ids = [0, 1, 2, 3, 4]\n",
    "sample_entity_types = ['gene', 'gene', 'disease', 'drug', 'gene']  # Example types\n",
    "\n",
    "embeddings = get_entity_embeddings(model, sample_entity_ids, sample_entity_types)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Example: Predict scores for new triplets\n",
    "test_triplets = [[0, 0, 1], [2, 1, 3]]  # [head, relation, tail]\n",
    "test_head_types = ['gene', 'disease']\n",
    "test_tail_types = ['gene', 'drug']\n",
    "\n",
    "scores, probs = predict_triplet_scores(model, test_triplets, test_head_types, test_tail_types)\n",
    "print(f\"Triplet scores: {scores}\")\n",
    "print(f\"Triplet probabilities: {probs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293c3899",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
