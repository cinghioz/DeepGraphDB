{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878fd3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "from dgl.nn import GraphConv\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from DeepGraphDB import DeepGraphDB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class HeteroEntityEmbedding(nn.Module):\n",
    "    \"\"\"Handles embeddings for heterogeneous entity types\"\"\"\n",
    "    \n",
    "    def __init__(self, entity_types: List[str], node_counts: Dict[str, int], \n",
    "                 embedding_dim: int = 256, precomputed_features: Dict[str, torch.Tensor] = None,\n",
    "                 freeze_pretrained: bool = False):\n",
    "        super().__init__()\n",
    "        self.entity_types = entity_types\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.node_counts = node_counts\n",
    "        \n",
    "        # Create separate embedding tables for each entity type\n",
    "        self.embeddings = nn.ModuleDict()\n",
    "        for entity_type in entity_types:\n",
    "            if node_counts[entity_type] > 0:\n",
    "                self.embeddings[entity_type] = nn.Embedding(\n",
    "                    node_counts[entity_type], embedding_dim\n",
    "                )\n",
    "                nn.init.xavier_uniform_(self.embeddings[entity_type].weight)\n",
    "    \n",
    "    def forward(self, entity_ids: torch.Tensor, entity_types: List[str]) -> torch.Tensor:\n",
    "        \"\"\"Get embeddings for entities of different types\"\"\"\n",
    "        batch_size = entity_ids.size(0)\n",
    "        embeddings = torch.zeros(batch_size, self.embedding_dim, \n",
    "                                device=entity_ids.device, dtype=torch.float)\n",
    "        \n",
    "        # Group entities by type for efficient lookup\n",
    "        type_to_indices = {}\n",
    "        for i, etype in enumerate(entity_types):\n",
    "            if etype not in type_to_indices:\n",
    "                type_to_indices[etype] = []\n",
    "            type_to_indices[etype].append(i)\n",
    "        \n",
    "        # Get embeddings for each type\n",
    "        for etype, indices in type_to_indices.items():\n",
    "            if etype in self.embeddings:\n",
    "                indices_tensor = torch.tensor(indices, device=entity_ids.device, dtype=torch.long)\n",
    "                entity_subset = entity_ids[indices_tensor]\n",
    "                embeddings[indices_tensor] = self.embeddings[etype](entity_subset)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "# class HeteroEntityEmbedding(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Handles embeddings for different entity types.\n",
    "#     This version is MODIFIED to accept pre-computed features.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, entity_types: List[str], node_counts: Dict[str, int], \n",
    "#                  embedding_dim: int, precomputed_features: Dict[str, torch.Tensor] = None,\n",
    "#                  freeze_pretrained: bool = False):\n",
    "#         super().__init__()\n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.embeddings = nn.ModuleDict()\n",
    "\n",
    "#         print(\"Initializing entity embeddings...\")\n",
    "#         for ent_type in entity_types:\n",
    "#             num_nodes = node_counts[ent_type]\n",
    "            \n",
    "#             # Check if pre-computed features are available for this entity type\n",
    "#             if precomputed_features and ent_type in precomputed_features:\n",
    "#                 features = precomputed_features[ent_type]\n",
    "                \n",
    "#                 # --- Validation Checks ---\n",
    "#                 if features.shape[0] != num_nodes:\n",
    "#                     raise ValueError(\n",
    "#                         f\"Mismatch for entity type '{ent_type}': node_counts is {num_nodes} but \"\n",
    "#                         f\"precomputed_features tensor has {features.shape[0]} rows.\"\n",
    "#                     )\n",
    "#                 if features.shape[1] != self.embedding_dim:\n",
    "#                     raise ValueError(\n",
    "#                         f\"Mismatch for entity type '{ent_type}': embedding_dim is {self.embedding_dim} but \"\n",
    "#                         f\"precomputed_features tensor has {features.shape[1]} columns.\"\n",
    "#                     )\n",
    "                \n",
    "#                 print(f\"-> Loading pre-computed features for entity type: '{ent_type}'\")\n",
    "#                 # Use from_pretrained to load your data\n",
    "#                 self.embeddings[ent_type] = nn.Embedding.from_pretrained(\n",
    "#                     features, freeze=freeze_pretrained\n",
    "#                 )\n",
    "#             else:\n",
    "#                 # If no pre-computed features, initialize randomly\n",
    "#                 print(f\"-> Randomly initializing features for entity type: '{ent_type}'\")\n",
    "#                 self.embeddings[ent_type] = nn.Embedding(num_nodes, self.embedding_dim)\n",
    "                \n",
    "#     def forward(self, entity_ids: torch.Tensor, entity_types: List[str]) -> torch.Tensor:\n",
    "#         \"\"\"\n",
    "#         Retrieves embeddings for a batch of heterogeneous entities.\n",
    "        \n",
    "#         Args:\n",
    "#             entity_ids: (batch_size,) tensor of entity IDs.\n",
    "#             entity_types: List of strings (length batch_size) specifying the type of each entity.\n",
    "        \n",
    "#         Returns:\n",
    "#             (batch_size, embedding_dim) tensor of embeddings.\n",
    "#         \"\"\"\n",
    "#         # Group indices by entity type to perform batched lookups, which is efficient.\n",
    "#         type_to_indices = defaultdict(list)\n",
    "#         type_to_original_pos = defaultdict(list)\n",
    "        \n",
    "#         for i, ent_type in enumerate(entity_types):\n",
    "#             type_to_indices[ent_type].append(entity_ids[i].item())\n",
    "#             type_to_original_pos[ent_type].append(i)\n",
    "\n",
    "#         # Create a placeholder for the output\n",
    "#         output_embeddings = torch.zeros(len(entity_ids), self.embedding_dim, device=entity_ids.device)\n",
    "        \n",
    "#         for ent_type, indices in type_to_indices.items():\n",
    "#             # Get embeddings for all entities of this type in the batch\n",
    "#             indices_tensor = torch.tensor(indices, dtype=torch.long, device=entity_ids.device)\n",
    "#             embs = self.embeddings[ent_type](indices_tensor)\n",
    "            \n",
    "#             # Scatter them back to their original positions in the batch\n",
    "#             original_pos = type_to_original_pos[ent_type]\n",
    "#             output_embeddings[original_pos] = embs\n",
    "            \n",
    "#         return output_embeddings\n",
    "\n",
    "class RelationEmbedding(nn.Module):\n",
    "    \"\"\"Handles relation embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, relation_types: List[str], embedding_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.relation_types = relation_types\n",
    "        self.num_relations = len(relation_types)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.relation_embedding = nn.Embedding(self.num_relations, embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.relation_embedding.weight)\n",
    "    \n",
    "    def forward(self, relation_ids: torch.Tensor) -> torch.Tensor:\n",
    "        return self.relation_embedding(relation_ids)\n",
    "\n",
    "class NBFLayer(nn.Module):\n",
    "    \"\"\"Neural Bellman-Ford Layer for message passing\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_relations: int, \n",
    "                 message_func: str = 'distmult'):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_relations = num_relations\n",
    "        self.message_func = message_func\n",
    "        \n",
    "        # Message function layers\n",
    "        if message_func == 'distmult':\n",
    "            self.message_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        elif message_func == 'rotate':\n",
    "            # self.message_layer = nn.Linear(input_dim * 2, hidden_dim)\n",
    "            self.message_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        elif message_func == 'pna':\n",
    "            # PNA combines multiple aggregations. Here, we simulate this by combining\n",
    "            # head and relation embeddings in multiple ways.\n",
    "            # We will use addition, subtraction, and element-wise multiplication.\n",
    "            self.aggregators = ['add', 'sub', 'mul']\n",
    "            \n",
    "            # The input to the message layer will be the concatenation of these results.\n",
    "            # Each result has size `hidden_dim`, so the total size is num_aggregators * hidden_dim.\n",
    "            pna_input_dim = len(self.aggregators) * hidden_dim\n",
    "            self.message_layer = nn.Linear(pna_input_dim, hidden_dim)\n",
    "        else:\n",
    "            self.message_layer = nn.Linear(input_dim + input_dim, hidden_dim)\n",
    "        \n",
    "        # Relation-specific transformation\n",
    "        self.relation_linear = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Update function\n",
    "        self.update_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Layer norm\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "    def message_function(self, head_emb: torch.Tensor, rel_emb: torch.Tensor, \n",
    "                        tail_emb: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute messages based on the specified function\"\"\"\n",
    "        if self.message_func == 'distmult':\n",
    "            # Element-wise product\n",
    "            message = head_emb * rel_emb\n",
    "        elif self.message_func == 'rotate':\n",
    "            # hidden_dim must be even for this to work\n",
    "            real_head, imag_head = torch.chunk(head_emb, 2, dim=-1)\n",
    "            real_rel, imag_rel = torch.chunk(rel_emb, 2, dim=-1)\n",
    "            \n",
    "            real_msg = real_head * real_rel - imag_head * imag_rel\n",
    "            imag_msg = real_head * imag_rel + imag_head * real_rel\n",
    "\n",
    "            message = torch.cat([real_msg, imag_msg], dim=-1)\n",
    "        elif self.message_func == 'pna':\n",
    "            # Apply multiple \"aggregation\" functions element-wise\n",
    "            msg_add = head_emb + rel_emb\n",
    "            msg_sub = head_emb - rel_emb\n",
    "            msg_mul = head_emb * rel_emb\n",
    "            \n",
    "            # Concatenate the results of the aggregators\n",
    "            message = torch.cat([msg_add, msg_sub, msg_mul], dim=-1)\n",
    "        else:\n",
    "            # Concatenation\n",
    "            message = torch.cat([head_emb, rel_emb], dim=-1)\n",
    "        \n",
    "        return self.message_layer(message)\n",
    "    \n",
    "    def forward(self, head_emb: torch.Tensor, rel_emb: torch.Tensor, \n",
    "                prev_emb: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of NBF layer\"\"\"\n",
    "        # Compute message\n",
    "        message = self.message_function(head_emb, rel_emb, prev_emb)\n",
    "        \n",
    "        # Update step\n",
    "        updated = self.update_layer(torch.cat([message, prev_emb], dim=-1))\n",
    "        \n",
    "        # Add residual connection and layer norm\n",
    "        output = self.layer_norm(updated + prev_emb)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class SimpleNBFNet(nn.Module):\n",
    "    \"\"\"Simplified NBFNet for heterogeneous knowledge graphs\"\"\"\n",
    "    \n",
    "    def __init__(self, entity_types: List[str], relation_types: List[str],\n",
    "                 node_counts: Dict[str, int], precomputed_features: Dict[str, torch.Tensor] = None,\n",
    "                 embedding_dim: int = 256, \n",
    "                 hidden_dim: int = 256, num_layers: int = 3,\n",
    "                 message_func: str = 'distmult', dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.entity_types = entity_types\n",
    "        self.relation_types = relation_types\n",
    "        self.node_counts = node_counts\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Embeddings\n",
    "        self.entity_embedding = HeteroEntityEmbedding(\n",
    "            entity_types, node_counts, embedding_dim, precomputed_features, freeze_pretrained=False\n",
    "        )\n",
    "        self.relation_embedding = RelationEmbedding(\n",
    "            relation_types, embedding_dim\n",
    "        )\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = nn.Linear(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # NBF layers\n",
    "        self.nbf_layers = nn.ModuleList([\n",
    "            NBFLayer(hidden_dim, hidden_dim, len(relation_types), message_func)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output layers\n",
    "        self.output_projection = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, triplets: torch.Tensor, head_types: List[str], \n",
    "                tail_types: List[str]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for batch of triplets\n",
    "        \n",
    "        Args:\n",
    "            triplets: (batch_size, 3) tensor of [head, relation, tail]\n",
    "            head_types: List of head entity types for each triplet\n",
    "            tail_types: List of tail entity types for each triplet\n",
    "        \n",
    "        Returns:\n",
    "            scores: (batch_size,) tensor of scores\n",
    "        \"\"\"\n",
    "        batch_size = triplets.size(0)\n",
    "        \n",
    "        # Extract components\n",
    "        heads = triplets[:, 0]\n",
    "        relations = triplets[:, 1]\n",
    "        tails = triplets[:, 2]\n",
    "        \n",
    "        # Get embeddings\n",
    "        head_emb = self.entity_embedding(heads, head_types)\n",
    "        rel_emb = self.relation_embedding(relations)\n",
    "        tail_emb = self.entity_embedding(tails, tail_types)\n",
    "        \n",
    "        # Project to hidden dimension\n",
    "        head_emb = self.input_projection(head_emb)\n",
    "        rel_emb = self.input_projection(rel_emb)\n",
    "        tail_emb = self.input_projection(tail_emb)\n",
    "        \n",
    "        # Apply dropout\n",
    "        head_emb = self.dropout_layer(head_emb)\n",
    "        rel_emb = self.dropout_layer(rel_emb)\n",
    "        tail_emb = self.dropout_layer(tail_emb)\n",
    "        \n",
    "        # Initialize with tail embeddings (for link prediction h,r,?) #TODO: fare anche su head?\n",
    "        current_emb = tail_emb\n",
    "        \n",
    "        # Apply NBF layers\n",
    "        for layer in self.nbf_layers:\n",
    "            current_emb = layer(head_emb, rel_emb, current_emb)\n",
    "            current_emb = self.dropout_layer(current_emb)\n",
    "        \n",
    "        # Compute final scores\n",
    "        scores = self.output_projection(current_emb).squeeze(-1)\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def get_embeddings(self, entity_ids: torch.Tensor, \n",
    "                      entity_types: List[str]) -> torch.Tensor:\n",
    "        \"\"\"Get entity embeddings for given entities\"\"\"\n",
    "        return self.entity_embedding(entity_ids, entity_types)\n",
    "\n",
    "class NBFNetTrainer:\n",
    "    \"\"\"Training utilities for NBFNet\"\"\"\n",
    "    \n",
    "    def __init__(self, model: SimpleNBFNet, learning_rate: float = 1e-3,\n",
    "                 weight_decay: float = 1e-5):\n",
    "        self.model = model\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    "        )\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def train_step(self, pos_triplets: torch.Tensor, neg_triplets: torch.Tensor,\n",
    "                   pos_head_types: List[str], pos_tail_types: List[str],\n",
    "                   neg_head_types: List[str], neg_tail_types: List[str]) -> float:\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        pos_scores = self.model(pos_triplets, pos_head_types, pos_tail_types)\n",
    "        neg_scores = self.model(neg_triplets, neg_head_types, neg_tail_types)\n",
    "        \n",
    "        # Create labels\n",
    "        pos_labels = torch.ones(pos_scores.size(0), device=pos_scores.device)\n",
    "        neg_labels = torch.zeros(neg_scores.size(0), device=neg_scores.device)\n",
    "        \n",
    "        # Compute loss\n",
    "        all_scores = torch.cat([pos_scores, neg_scores], dim=0)\n",
    "        all_labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
    "        \n",
    "        loss = self.criterion(all_scores, all_labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def evaluate(self, pos_triplets: torch.Tensor, neg_triplets: torch.Tensor,\n",
    "                pos_head_types: List[str], pos_tail_types: List[str],\n",
    "                neg_head_types: List[str], neg_tail_types: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            pos_scores = self.model(pos_triplets, pos_head_types, pos_tail_types)\n",
    "            neg_scores = self.model(neg_triplets, neg_head_types, neg_tail_types)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            pos_preds = (torch.sigmoid(pos_scores) > 0.5).float()\n",
    "            neg_preds = (torch.sigmoid(neg_scores) > 0.5).float()\n",
    "            pos_acc = pos_preds.mean().item()\n",
    "            neg_acc = (1 - neg_preds).mean().item()\n",
    "            total_acc = (pos_acc + neg_acc) / 2\n",
    "            \n",
    "            # Compute AUC\n",
    "            # Combine positive and negative scores\n",
    "            all_scores = torch.cat([pos_scores, neg_scores])\n",
    "            all_labels = torch.cat([torch.ones_like(pos_scores), torch.zeros_like(neg_scores)])\n",
    "            \n",
    "            # Convert to numpy for sklearn\n",
    "            scores_np = torch.sigmoid(all_scores).cpu().numpy()\n",
    "            labels_np = all_labels.cpu().numpy()\n",
    "            \n",
    "            # Calculate AUC\n",
    "            auc = roc_auc_score(labels_np, scores_np)\n",
    "            \n",
    "            return {\n",
    "                'pos_accuracy': pos_acc,\n",
    "                'neg_accuracy': neg_acc,\n",
    "                'total_accuracy': total_acc,\n",
    "                'avg_pos_score': torch.sigmoid(pos_scores).mean().item(),\n",
    "                'avg_neg_score': torch.sigmoid(neg_scores).mean().item(),\n",
    "                'auc': auc\n",
    "            }   \n",
    "\n",
    "def prepare_batch_data(triplets: List[Tuple], triplets_ntypes: List[Tuple], \n",
    "                      batch_size: int = 1024) -> List[Tuple]:\n",
    "    \"\"\"Prepare batched data for training\"\"\"\n",
    "    batches = []\n",
    "    \n",
    "    for i in range(0, len(triplets), batch_size):\n",
    "        batch_triplets = triplets[i:i+batch_size]\n",
    "        batch_ntypes = triplets_ntypes[i:i+batch_size] if triplets_ntypes else None\n",
    "        \n",
    "        # Convert to tensors\n",
    "        triplet_tensor = torch.tensor(batch_triplets, dtype=torch.long)\n",
    "        \n",
    "        if batch_ntypes:\n",
    "            head_types = [nt[0] for nt in batch_ntypes]\n",
    "            tail_types = [nt[1] for nt in batch_ntypes]\n",
    "        else:\n",
    "            head_types = ['entity'] * len(batch_triplets)\n",
    "            tail_types = ['entity'] * len(batch_triplets)\n",
    "        \n",
    "        batches.append((triplet_tensor, head_types, tail_types))\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed5d4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplets_vectorized(g, edge_mapping):\n",
    "    \"\"\"Extract triplets from heterogeneous DGL graph with efficient negative sampling.\"\"\"\n",
    "    triplets = []\n",
    "    triplets_ntypes = []\n",
    "    neg_triplets = []\n",
    "    neg_triplets_ntypes = []\n",
    "    \n",
    "    if hasattr(g, 'canonical_etypes'):\n",
    "        # Pre-compute invalid entity types for each relation\n",
    "        relation_invalid_heads = {}\n",
    "        relation_invalid_tails = {}\n",
    "        node_type_ranges = {}\n",
    "        \n",
    "        # Cache node counts for each type\n",
    "        for ntype in g.ntypes:\n",
    "            node_type_ranges[ntype] = g.num_nodes(ntype)\n",
    "        \n",
    "        # Pre-compute invalid types for each relation\n",
    "        for canonical_etype in g.canonical_etypes:\n",
    "            src_ntype, etype, dst_ntype = canonical_etype\n",
    "            \n",
    "            if etype not in relation_invalid_heads:\n",
    "                # Find valid head types for this relation\n",
    "                valid_head_types = set()\n",
    "                valid_tail_types = set()\n",
    "                for canon_et in g.canonical_etypes:\n",
    "                    if canon_et[1] == etype:\n",
    "                        valid_head_types.add(canon_et[0])\n",
    "                        valid_tail_types.add(canon_et[2])\n",
    "                \n",
    "                # Get invalid types\n",
    "                all_node_types = set(g.ntypes)\n",
    "                relation_invalid_heads[etype] = list(all_node_types - valid_head_types)\n",
    "                relation_invalid_tails[etype] = list(all_node_types - valid_tail_types)\n",
    "        \n",
    "        # Process each relation type\n",
    "        for canonical_etype in g.canonical_etypes:\n",
    "            src_ntype, etype, dst_ntype = canonical_etype\n",
    "            src, dst = g.edges(etype=canonical_etype)\n",
    "            \n",
    "            if len(src) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Map relation name to index\n",
    "            relation_idx = edge_mapping[etype]\n",
    "            # relation_idx = etype\n",
    "\n",
    "            edge_triplets = list(zip(src.tolist(), [relation_idx] * len(src), dst.tolist()))\n",
    "            triplets.extend(edge_triplets)\n",
    "            triplets_ntypes.extend(list(zip([src_ntype] * len(src), [dst_ntype] * len(dst))))\n",
    "            \n",
    "            # Vectorized negative triplet generation\n",
    "            num_edges = len(src)\n",
    "            src_numpy = src.numpy()\n",
    "            dst_numpy = dst.numpy()\n",
    "            \n",
    "            # Random choice for head vs tail corruption (vectorized)\n",
    "            corrupt_head_mask = np.random.random(num_edges) < 0.5\n",
    "            \n",
    "            # Generate corrupted heads\n",
    "            invalid_head_types = relation_invalid_heads[etype]\n",
    "            if invalid_head_types and np.any(corrupt_head_mask):\n",
    "                head_corruption_indices = np.where(corrupt_head_mask)[0]\n",
    "                \n",
    "                # Vectorized selection of corrupted head types\n",
    "                corrupted_head_types = np.random.choice(\n",
    "                    invalid_head_types, \n",
    "                    size=len(head_corruption_indices)\n",
    "                )\n",
    "                \n",
    "                # Vectorized generation of corrupted head IDs\n",
    "                corrupted_head_ids = np.array([\n",
    "                    np.random.randint(0, node_type_ranges[head_type]) \n",
    "                    if node_type_ranges[head_type] > 0 else 0\n",
    "                    for head_type in corrupted_head_types\n",
    "                ])\n",
    "                \n",
    "                # Create negative triplets for head corruption\n",
    "                valid_mask = np.array([node_type_ranges[ht] > 0 for ht in corrupted_head_types])\n",
    "                if np.any(valid_mask):\n",
    "                    valid_indices = head_corruption_indices[valid_mask]\n",
    "                    valid_corrupted_heads = corrupted_head_ids[valid_mask]\n",
    "                    valid_corrupted_head_types = corrupted_head_types[valid_mask]\n",
    "                    \n",
    "                    head_neg_triplets = list(zip(\n",
    "                        valid_corrupted_heads.tolist(),\n",
    "                        [relation_idx] * len(valid_indices),\n",
    "                        dst_numpy[valid_indices].tolist()\n",
    "                    ))\n",
    "                    neg_triplets.extend(head_neg_triplets)\n",
    "                    neg_triplets_ntypes.extend(list(zip(\n",
    "                        valid_corrupted_head_types.tolist(),\n",
    "                        [dst_ntype] * len(valid_indices)\n",
    "                    )))\n",
    "            \n",
    "            # Generate corrupted tails\n",
    "            invalid_tail_types = relation_invalid_tails[etype]\n",
    "            if invalid_tail_types and np.any(~corrupt_head_mask):\n",
    "                tail_corruption_indices = np.where(~corrupt_head_mask)[0]\n",
    "                \n",
    "                # Vectorized selection of corrupted tail types\n",
    "                corrupted_tail_types = np.random.choice(\n",
    "                    invalid_tail_types, \n",
    "                    size=len(tail_corruption_indices)\n",
    "                )\n",
    "                \n",
    "                # Vectorized generation of corrupted tail IDs\n",
    "                corrupted_tail_ids = np.array([\n",
    "                    np.random.randint(0, node_type_ranges[tail_type]) \n",
    "                    if node_type_ranges[tail_type] > 0 else 0\n",
    "                    for tail_type in corrupted_tail_types\n",
    "                ])\n",
    "                \n",
    "                # Create negative triplets for tail corruption\n",
    "                valid_mask = np.array([node_type_ranges[tt] > 0 for tt in corrupted_tail_types])\n",
    "                if np.any(valid_mask):\n",
    "                    valid_indices = tail_corruption_indices[valid_mask]\n",
    "                    valid_corrupted_tails = corrupted_tail_ids[valid_mask]\n",
    "                    valid_corrupted_tail_types = corrupted_tail_types[valid_mask]\n",
    "                    \n",
    "                    tail_neg_triplets = list(zip(\n",
    "                        src_numpy[valid_indices].tolist(),\n",
    "                        [relation_idx] * len(valid_indices),\n",
    "                        valid_corrupted_tails.tolist()\n",
    "                    ))\n",
    "                    neg_triplets.extend(tail_neg_triplets)\n",
    "                    neg_triplets_ntypes.extend(list(zip(\n",
    "                        [src_ntype] * len(valid_indices),\n",
    "                        valid_corrupted_tail_types.tolist()\n",
    "                    )))\n",
    "    \n",
    "    else:\n",
    "        # Homogeneous graph - vectorized version\n",
    "        src, dst = g.edges()\n",
    "        edge_triplets = list(zip(src.tolist(), [0] * len(src), dst.tolist()))\n",
    "        triplets.extend(edge_triplets)\n",
    "        \n",
    "        # Vectorized negative sampling for homogeneous graphs\n",
    "        num_edges = len(src)\n",
    "        total_nodes = g.num_nodes()\n",
    "        \n",
    "        if total_nodes > 0 and num_edges > 0:\n",
    "            src_numpy = src.numpy()\n",
    "            dst_numpy = dst.numpy()\n",
    "            \n",
    "            # Random choice for head vs tail corruption\n",
    "            corrupt_head_mask = np.random.random(num_edges) < 0.5\n",
    "            \n",
    "            # Generate all corrupted nodes at once\n",
    "            corrupted_heads = np.random.randint(0, total_nodes, size=num_edges)\n",
    "            corrupted_tails = np.random.randint(0, total_nodes, size=num_edges)\n",
    "            \n",
    "            # Apply corruption based on mask\n",
    "            neg_src = np.where(corrupt_head_mask, corrupted_heads, src_numpy)\n",
    "            neg_dst = np.where(corrupt_head_mask, dst_numpy, corrupted_tails)\n",
    "            \n",
    "            neg_triplets = list(zip(\n",
    "                neg_src.tolist(),\n",
    "                [0] * num_edges,\n",
    "                neg_dst.tolist()\n",
    "            ))\n",
    "\n",
    "    all = list(zip(triplets, triplets_ntypes, neg_triplets, neg_triplets_ntypes))\n",
    "\n",
    "    random.shuffle(all)\n",
    "\n",
    "    triplets, triplets_ntypes, neg_triplets, neg_triplets_ntypes = zip(*all)\n",
    "    \n",
    "    return triplets, triplets_ntypes, neg_triplets, neg_triplets_ntypes\n",
    "\n",
    "def get_entity_embeddings(model, entity_ids, entity_types):\n",
    "    \"\"\"Extract learned entity embeddings\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    if isinstance(entity_ids, list):\n",
    "        entity_ids = torch.tensor(entity_ids, dtype=torch.long)\n",
    "    \n",
    "    entity_ids = entity_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings = model.get_embeddings(entity_ids, entity_types)\n",
    "    \n",
    "    return embeddings.cpu()\n",
    "\n",
    "def predict_triplet_scores(model, triplets, head_types, tail_types):\n",
    "    \"\"\"Predict scores for given triplets\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    if isinstance(triplets, list):\n",
    "        triplets = torch.tensor(triplets, dtype=torch.long)\n",
    "    \n",
    "    triplets = triplets.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores = model(triplets, head_types, tail_types)\n",
    "        probabilities = torch.sigmoid(scores)\n",
    "    \n",
    "    return scores.cpu().numpy(), probabilities.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e810b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb = DeepGraphDB()\n",
    "gdb.load_graph(\"/home/cc/PHD/dglframework/DeepKG/DeepGraphDB/graphs/primekg.bin\")\n",
    "\n",
    "# Extract graph information\n",
    "num_entities = gdb.graph.number_of_nodes()\n",
    "\n",
    "# relation_types = list(set([etype[1] for etype in gdb.graph.canonical_etypes]))\n",
    "# relation_to_idx = {rel: idx for idx, rel in enumerate(relation_types)}\n",
    "# pos_triplets, pos_triplets_ntypes, neg_triplets, neg_triplets_ntypes = get_triplets_vectorized(gdb.graph, relation_to_idx)\n",
    "\n",
    "# Extract triplets\n",
    "pos_triplets, pos_triplets_ntypes, neg_triplets, neg_triplets_ntypes = get_triplets_vectorized(gdb.graph, gdb.edge_types_mapping)\n",
    "\n",
    "print(f\"Graph stats: {num_entities} entities, {len(gdb.edge_types_mapping)} relations, {len(pos_triplets)} triplets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6f2f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(gdb.graph, 'ntypes'):\n",
    "    # Heterogeneous graph\n",
    "    entity_types = list(gdb.graph.ntypes)\n",
    "    node_counts = {ntype: gdb.graph.num_nodes(ntype) for ntype in entity_types}\n",
    "    \n",
    "    # Get relation types from canonical edge types\n",
    "    if hasattr(gdb.graph, 'canonical_etypes'):\n",
    "        relation_types = list(set([etype[1] for etype in gdb.graph.canonical_etypes]))\n",
    "    else:\n",
    "        relation_types = list(gdb.edge_types_mapping.keys()) if hasattr(gdb, 'edge_types_mapping') else ['default_relation']\n",
    "else:\n",
    "    # Homogeneous graph\n",
    "    entity_types = ['entity']\n",
    "    node_counts = {'entity': gdb.graph.num_nodes()}\n",
    "    relation_types = ['relation']\n",
    "\n",
    "print(f\"Entity types: {entity_types}\")\n",
    "print(f\"Node counts: {node_counts}\")\n",
    "print(f\"Relation types: {relation_types}\")\n",
    "\n",
    "pre_feats_dict = {}\n",
    "\n",
    "# for ent_type in gdb.graph.ntypes:\n",
    "#     if ent_type not in pre_feats_dict:\n",
    "#         pre_feats_dict[ent_type] = gdb.graph.nodes[ent_type].data['x']\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleNBFNet(\n",
    "    entity_types=entity_types,\n",
    "    relation_types=relation_types,\n",
    "    node_counts=node_counts,\n",
    "    precomputed_features=pre_feats_dict if pre_feats_dict else None,\n",
    "    embedding_dim=pre_feats_dict['anatomy'].shape[1] if pre_feats_dict else 256,  # Reduced for faster training\n",
    "    hidden_dim=256,\n",
    "    num_layers=6, # Reduced for faster training\n",
    "    message_func='pna',  # 'distmult', rotate', 'pna' can also be used\n",
    "    dropout=0.20\n",
    ").to(device)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = NBFNetTrainer(model, learning_rate=1e-3, weight_decay=1e-5)\n",
    "\n",
    "# Prepare data\n",
    "batch_size = 200000\n",
    "pos_batches = prepare_batch_data(pos_triplets, pos_triplets_ntypes, batch_size)\n",
    "neg_batches = prepare_batch_data(neg_triplets, neg_triplets_ntypes, batch_size)\n",
    "\n",
    "print(f\"Prepared {len(pos_batches)} positive batches and {len(neg_batches)} negative batches\")\n",
    "\n",
    "# Training loop with progress tracking\n",
    "num_epochs = 25\n",
    "best_acc = 0.0\n",
    "\n",
    "#TODO: Split triples based on entity types or relation types\n",
    "\n",
    "random.shuffle(pos_batches)\n",
    "random.shuffle(neg_batches)\n",
    "\n",
    "val_batches = len(pos_batches) // 8\n",
    "\n",
    "eval_pos_batches = pos_batches[:val_batches]\n",
    "eval_neg_batches = neg_batches[:val_batches]\n",
    "\n",
    "train_pos_batches = pos_batches[val_batches:]\n",
    "train_neg_batches = neg_batches[val_batches:]\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = min(len(train_pos_batches), len(train_neg_batches))\n",
    "\n",
    "    if epoch % 6 == 0:\n",
    "        print(\"Shuffle\")\n",
    "        random.shuffle(pos_batches)\n",
    "        random.shuffle(neg_batches)\n",
    "\n",
    "        eval_pos_batches = pos_batches[:val_batches]\n",
    "        eval_neg_batches = neg_batches[:val_batches]\n",
    "\n",
    "        train_pos_batches = pos_batches[val_batches:]\n",
    "        train_neg_batches = neg_batches[val_batches:]\n",
    "    \n",
    "    for i in range(num_batches):  # Limit batches per epoch for speed\n",
    "        pos_batch = train_pos_batches[i % len(train_pos_batches)]\n",
    "        neg_batch = train_neg_batches[i % len(train_neg_batches)]\n",
    "        \n",
    "        pos_triplets_batch, pos_head_types, pos_tail_types = pos_batch\n",
    "        neg_triplets_batch, neg_head_types, neg_tail_types = neg_batch\n",
    "        \n",
    "        # Move to device\n",
    "        pos_triplets_batch = pos_triplets_batch.to(device)\n",
    "        neg_triplets_batch = neg_triplets_batch.to(device)\n",
    "        \n",
    "        # Ensure same batch size\n",
    "        min_batch_size = min(pos_triplets_batch.size(0), neg_triplets_batch.size(0))\n",
    "        pos_triplets_batch = pos_triplets_batch[:min_batch_size]\n",
    "        neg_triplets_batch = neg_triplets_batch[:min_batch_size]\n",
    "        pos_head_types = pos_head_types[:min_batch_size]\n",
    "        pos_tail_types = pos_tail_types[:min_batch_size]\n",
    "        neg_head_types = neg_head_types[:min_batch_size]\n",
    "        neg_tail_types = neg_tail_types[:min_batch_size]\n",
    "        \n",
    "        loss = trainer.train_step(\n",
    "            pos_triplets_batch, neg_triplets_batch,\n",
    "            pos_head_types, pos_tail_types,\n",
    "            neg_head_types, neg_tail_types\n",
    "        )\n",
    "        total_loss += loss\n",
    "\n",
    "    evals = {\n",
    "        'total_accuracy': np.array([0.0]),\n",
    "        'auc': np.array([0.0]),\n",
    "        'pos_accuracy': np.array([0.0]),\n",
    "        'neg_accuracy': np.array([0.0]),\n",
    "        'avg_pos_score': np.array([0.0]),\n",
    "        'avg_neg_score': np.array([0.0])\n",
    "    }\n",
    "\n",
    "    # Evaluate every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        model.eval()\n",
    "        # Evaluate on a sample batch\n",
    "        for eval_pos_batch, eval_neg_batch in zip(eval_pos_batches, eval_neg_batches):\n",
    "            eval_pos_triplets, eval_pos_head_types, eval_pos_tail_types = eval_pos_batch\n",
    "            eval_neg_triplets, eval_neg_head_types, eval_neg_tail_types = eval_neg_batch\n",
    "            \n",
    "            eval_pos_triplets = eval_pos_triplets.to(device)\n",
    "            eval_neg_triplets = eval_neg_triplets.to(device)\n",
    "            \n",
    "            min_eval_size = min(eval_pos_triplets.size(0), eval_neg_triplets.size(0))\n",
    "            eval_pos_triplets = eval_pos_triplets[:min_eval_size]\n",
    "            eval_neg_triplets = eval_neg_triplets[:min_eval_size]\n",
    "            eval_pos_head_types = eval_pos_head_types[:min_eval_size]\n",
    "            eval_pos_tail_types = eval_pos_tail_types[:min_eval_size]\n",
    "            eval_neg_head_types = eval_neg_head_types[:min_eval_size]\n",
    "            eval_neg_tail_types = eval_neg_tail_types[:min_eval_size]\n",
    "            \n",
    "            eval_metrics = trainer.evaluate(\n",
    "                eval_pos_triplets, eval_neg_triplets,\n",
    "                eval_pos_head_types, eval_pos_tail_types,\n",
    "                eval_neg_head_types, eval_neg_tail_types\n",
    "            )\n",
    "            \n",
    "            # Store mean accuracy for this epoch and calculate AUC\n",
    "            evals['total_accuracy'] = np.append(evals['total_accuracy'], eval_metrics['total_accuracy'])\n",
    "            evals['pos_accuracy'] = np.append(evals['pos_accuracy'], eval_metrics['pos_accuracy'])\n",
    "            evals['neg_accuracy'] = np.append(evals['neg_accuracy'], eval_metrics['neg_accuracy'])\n",
    "            evals['avg_pos_score'] = np.append(evals['avg_pos_score'], eval_metrics['avg_pos_score'])\n",
    "            evals['avg_neg_score'] = np.append(evals['avg_neg_score'], eval_metrics['avg_neg_score'])     \n",
    "            evals['auc'] = np.append(evals['auc'], eval_metrics['auc'])\n",
    "\n",
    "        current_acc = evals['total_accuracy'].mean()\n",
    "        if current_acc > best_acc:\n",
    "            best_acc = current_acc\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), 'best_nbfnet_model.pth')\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Loss={total_loss/num_batches:.4f}, \"\n",
    "            f\"Acc={current_acc:.4f} (Best: {best_acc:.4f}) - AUC={evals['auc'].mean():.4f}\")\n",
    "        print(f\"  Pos Acc: {evals['pos_accuracy'].mean():.4f}, \"\n",
    "            f\"Neg Acc: {evals['neg_accuracy'].mean():.4f}\")\n",
    "        print(f\"  Avg Pos Score: {evals['avg_pos_score'].mean():.4f}, \"\n",
    "            f\"Avg Neg Score: {evals['avg_neg_score'].mean():.4f}\")\n",
    "    \n",
    "# # Example: Get embeddings for specific entities\n",
    "# sample_entity_ids = [0, 1, 2, 3, 4]\n",
    "# sample_entity_types = ['gene', 'gene', 'disease', 'drug', 'gene']  # Example types\n",
    "\n",
    "# embeddings = get_entity_embeddings(model, sample_entity_ids, sample_entity_types)\n",
    "# print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# # Example: Predict scores for new triplets\n",
    "# test_triplets = [[0, 0, 1], [2, 1, 3]]  # [head, relation, tail]\n",
    "# test_head_types = ['gene', 'disease']\n",
    "# test_tail_types = ['gene', 'drug']\n",
    "\n",
    "# scores, probs = predict_triplet_scores(model, test_triplets, test_head_types, test_tail_types)\n",
    "# print(f\"Triplet scores: {scores}\")\n",
    "# print(f\"Triplet probabilities: {probs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdd6e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ChromaVDB.chroma import ChromaFramework\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "vdb = ChromaFramework(persist_directory=\"./ChromaVDB/chroma_db\")\n",
    "\n",
    "tuples = list(gdb.reverse_node_mapping.keys())\n",
    "embs = {}\n",
    "\n",
    "for entity in gdb.graph.ntypes:\n",
    "    ids = [entity_id for entity_name, entity_id in tuples if entity_name == entity]\n",
    "    embs[entity] = get_entity_embeddings(model, ids, [entity] * gdb.graph.num_nodes(entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a05b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 min x 130k nodes\n",
    "BATCH_SIZE = 5000\n",
    "\n",
    "for entity in gdb.graph.ntypes:\n",
    "    embeddings_tensor = embs[entity]\n",
    "\n",
    "    total = embeddings_tensor.shape[0]\n",
    "    names = gdb.node_data[entity]['name'].tolist()\n",
    "    \n",
    "    for i in tqdm(range(0, total, BATCH_SIZE)):\n",
    "        end = i + BATCH_SIZE\n",
    "\n",
    "        batch_ids = [gdb.reverse_node_mapping[(entity, j)] for j in range(i, min(end, total))]\n",
    "        batch_embeddings = {\"graph\": embeddings_tensor[i:min(end, total)]}\n",
    "        batch_entities = [entity] * len(batch_embeddings[\"graph\"])\n",
    "        batch_names = names[i:min(end, total)]\n",
    "        batch_metadata = [{} for _ in range(len(batch_embeddings[\"graph\"]))]\n",
    "        batch_docs = [\"\" for _ in range(len(batch_embeddings[\"graph\"]))]\n",
    "\n",
    "        vdb.create_records(\n",
    "            global_ids=batch_ids,\n",
    "            names=batch_names,\n",
    "            entities=batch_entities,\n",
    "            metadatas=batch_metadata,\n",
    "            documents=batch_docs,\n",
    "            embeddings=batch_embeddings\n",
    "        )\n",
    "\n",
    "records = vdb.list_records()\n",
    "len(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c034bca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_embeddings_tsne(entity_embeddings_dict, \n",
    "                             perplexity=35, \n",
    "                             n_iter=1000, \n",
    "                             random_state=42,\n",
    "                             figsize=(12, 8),\n",
    "                             show_individual_labels=False,\n",
    "                             font_size=8):\n",
    "    \"\"\"\n",
    "    Create a t-SNE visualization of embeddings where each entity type has multiple embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "    entity_embeddings_dict (dict): Dictionary with entity types as keys and lists of embeddings as values\n",
    "                                  e.g., {\"person\": [[emb1], [emb2], ...], \"location\": [[emb1], [emb2], ...]}\n",
    "    perplexity (int): t-SNE perplexity parameter\n",
    "    n_iter (int): Number of iterations for t-SNE\n",
    "    random_state (int): Random state for reproducibility\n",
    "    figsize (tuple): Figure size for the plot\n",
    "    show_individual_labels (bool): Whether to show individual point labels (can be cluttered)\n",
    "    font_size (int): Font size for labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # Flatten the data and keep track of entity types\n",
    "    all_embeddings = []\n",
    "    entity_labels = []\n",
    "    entity_types = []\n",
    "    \n",
    "    for entity_type, embeddings_list in entity_embeddings_dict.items():\n",
    "        embeddings_array = np.array(embeddings_list)\n",
    "        \n",
    "        # Handle different input formats\n",
    "        if embeddings_array.ndim == 1:\n",
    "            embeddings_array = embeddings_array.reshape(1, -1)\n",
    "        \n",
    "        print(f\"{entity_type}: {len(embeddings_array)} embeddings of dimension {embeddings_array.shape[1]}\")\n",
    "        \n",
    "        for i, embedding in enumerate(embeddings_array):\n",
    "            all_embeddings.append(embedding)\n",
    "            entity_labels.append(f\"{entity_type}_{i}\")\n",
    "            entity_types.append(entity_type)\n",
    "    \n",
    "    all_embeddings = np.array(all_embeddings)\n",
    "    total_points = len(all_embeddings)\n",
    "    \n",
    "    print(f\"\\nTotal: {total_points} embeddings across {len(entity_embeddings_dict)} entity types\")\n",
    "    \n",
    "    # Standardize the embeddings\n",
    "    scaler = StandardScaler()\n",
    "    embeddings_scaled = scaler.fit_transform(all_embeddings)\n",
    "    \n",
    "    # Apply t-SNE\n",
    "    print(\"Applying t-SNE...\")\n",
    "    tsne = TSNE(n_components=2, \n",
    "                perplexity=min(perplexity, total_points-1),\n",
    "                n_iter=n_iter, \n",
    "                random_state=random_state,\n",
    "                verbose=1)\n",
    "    \n",
    "    embeddings_2d = tsne.fit_transform(embeddings_scaled)\n",
    "    \n",
    "    # Create the visualization\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Create a color palette for entity types\n",
    "    unique_entity_types = list(entity_embeddings_dict.keys())\n",
    "    colors = sns.color_palette(\"husl\", len(unique_entity_types))\n",
    "    color_map = dict(zip(unique_entity_types, colors))\n",
    "    \n",
    "    # Plot points colored by entity type\n",
    "    for entity_type in unique_entity_types:\n",
    "        # Get indices for this entity type\n",
    "        indices = [i for i, et in enumerate(entity_types) if et == entity_type]\n",
    "        x_coords = embeddings_2d[indices, 0]\n",
    "        y_coords = embeddings_2d[indices, 1]\n",
    "        \n",
    "        plt.scatter(x_coords, y_coords, \n",
    "                   c=[color_map[entity_type]], \n",
    "                   label=f\"{entity_type} ({len(indices)})\",\n",
    "                   s=60, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "        \n",
    "        # Optionally add individual labels\n",
    "        if show_individual_labels:\n",
    "            for i, idx in enumerate(indices):\n",
    "                plt.annotate(f\"{entity_type}_{i}\", \n",
    "                            (embeddings_2d[idx, 0], embeddings_2d[idx, 1]),\n",
    "                            xytext=(2, 2), textcoords='offset points',\n",
    "                            fontsize=font_size, alpha=0.8)\n",
    "    \n",
    "    plt.title(f't-SNE Visualization of Entity Embeddings\\n({total_points} total embeddings, {len(unique_entity_types)} entity types)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('t-SNE Component 1', fontsize=12)\n",
    "    plt.ylabel('t-SNE Component 2', fontsize=12)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Remove ticks for cleaner look\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return embeddings_2d, entity_labels, entity_types\n",
    "\n",
    "def analyze_clusters(embeddings_2d, entity_types, entity_embeddings_dict):\n",
    "    \"\"\"\n",
    "    Analyze the clustering quality and provide statistics.\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    df = pd.DataFrame({\n",
    "        'x': embeddings_2d[:, 0],\n",
    "        'y': embeddings_2d[:, 1],\n",
    "        'entity_type': entity_types\n",
    "    })\n",
    "    \n",
    "    print(\"\\n=== Cluster Analysis ===\")\n",
    "    print(\"Entity type distribution:\")\n",
    "    type_counts = Counter(entity_types)\n",
    "    for entity_type, count in type_counts.items():\n",
    "        print(f\"  {entity_type}: {count} embeddings\")\n",
    "    \n",
    "    # Calculate centroids for each entity type\n",
    "    print(\"\\nEntity type centroids:\")\n",
    "    for entity_type in entity_embeddings_dict.keys():\n",
    "        mask = df['entity_type'] == entity_type\n",
    "        centroid_x = df[mask]['x'].mean()\n",
    "        centroid_y = df[mask]['y'].mean()\n",
    "        print(f\"  {entity_type}: ({centroid_x:.3f}, {centroid_y:.3f})\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage with sample data\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: Create sample embeddings dictionary with multiple embeddings per entity type\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    print(\"Running example with sample data...\")\n",
    "    print(\"Replace 'sample_entities' with your actual entity_embeddings_dict\")\n",
    "    \n",
    "    # Visualize the embeddings\n",
    "    tsne_coords, labels, types = visualize_embeddings_tsne(\n",
    "        embs,\n",
    "        perplexity=35,  # Lower perplexity for smaller dataset\n",
    "        figsize=(14, 10),\n",
    "        show_individual_labels=False  # Set to True if you want individual point labels\n",
    "    )\n",
    "    \n",
    "    # Analyze clusters\n",
    "    # cluster_df = analyze_clusters(tsne_coords, types, sample_entities)\n",
    "    \n",
    "    # Optional: Save the plot\n",
    "    # plt.savefig('entity_embeddings_tsne.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Usage for your actual data:\n",
    "# your_entity_embeddings = {\n",
    "#     \"person\": [\n",
    "#         [0.1, 0.2, 0.3, ...],  # embedding 1 for person\n",
    "#         [0.4, 0.5, 0.6, ...],  # embedding 2 for person\n",
    "#         [0.7, 0.8, 0.9, ...],  # embedding 3 for person\n",
    "#         # ... more person embeddings\n",
    "#     ],\n",
    "#     \"location\": [\n",
    "#         [0.2, 0.3, 0.4, ...],  # embedding 1 for location\n",
    "#         [0.5, 0.6, 0.7, ...],  # embedding 2 for location\n",
    "#         # ... more location embeddings\n",
    "#     ],\n",
    "#     # ... more entity types\n",
    "# }\n",
    "# \n",
    "# tsne_coords, labels, types = visualize_embeddings_tsne(your_entity_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
