{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a94a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "class PrimeKGLoader:\n",
    "    \"\"\"\n",
    "    Prepares PrimeKG data for efficient loading into DGL heterogeneous graphs.\n",
    "    Each node type gets sequential IDs starting from 0.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.node_type_mapping = {}  # string -> int\n",
    "        self.relationship_type_mapping = {}  # string -> int\n",
    "        self.reverse_node_type_mapping = {}  # int -> string\n",
    "        self.reverse_relationship_type_mapping = {}  # int -> string\n",
    "        self.global_to_local_mapping = {}  # For reference: global_id -> (node_type, local_id)\n",
    "        \n",
    "    def load_and_prepare_primekg(self, nodes_csv_path: str, edges_csv_path: str):\n",
    "        \"\"\"\n",
    "        Load PrimeKG data and prepare it for bulk_load_heterogeneous_graph.\n",
    "        Each node type gets sequential IDs starting from 0.\n",
    "        \n",
    "        Args:\n",
    "            nodes_csv_path: Path to nodes CSV file\n",
    "            edges_csv_path: Path to edges CSV file\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (node_types_dict, edge_types_dict) ready for DGL loading\n",
    "        \"\"\"\n",
    "        print(\"Loading PrimeKG data...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Load raw data\n",
    "        print(\"  Reading CSV files...\")\n",
    "        nodes_df = pd.read_csv(nodes_csv_path, low_memory=False)\n",
    "        edges_df = pd.read_csv(edges_csv_path, low_memory=False)\n",
    "        \n",
    "        print(f\"  Loaded {len(nodes_df):,} nodes and {len(edges_df):,} edges\")\n",
    "        \n",
    "        # Create type mappings\n",
    "        print(\"  Creating type mappings...\")\n",
    "        self._create_type_mappings(nodes_df, edges_df)\n",
    "        \n",
    "        # Prepare node data (sequential IDs starting from 0 for each type)\n",
    "        print(\"  Preparing node data...\")\n",
    "        node_types_dict = self._prepare_node_data(nodes_df)\n",
    "        \n",
    "        # Prepare edge data (using local IDs)\n",
    "        print(\"  Preparing edge data...\")\n",
    "        edge_types_dict = self._prepare_edge_data(edges_df, nodes_df)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nData preparation completed in {total_time:.2f}s\")\n",
    "        \n",
    "        # Print summary\n",
    "        self._print_summary(node_types_dict, edge_types_dict)\n",
    "        \n",
    "        return node_types_dict, edge_types_dict, self.global_to_local_mapping\n",
    "    \n",
    "    def _create_type_mappings(self, nodes_df: pd.DataFrame, edges_df: pd.DataFrame):\n",
    "        \"\"\"Create mappings between string types and integer representations.\"\"\"\n",
    "        \n",
    "        # Node type mappings\n",
    "        unique_node_types = sorted(nodes_df['node_type'].unique())\n",
    "        self.node_type_mapping = {node_type: i for i, node_type in enumerate(unique_node_types)}\n",
    "        self.reverse_node_type_mapping = {i: node_type for node_type, i in self.node_type_mapping.items()}\n",
    "        \n",
    "        # Relationship type mappings\n",
    "        unique_rel_types = sorted(edges_df['relationship_type'].unique())\n",
    "        self.relationship_type_mapping = {rel_type: i for i, rel_type in enumerate(unique_rel_types)}\n",
    "        self.reverse_relationship_type_mapping = {i: rel_type for rel_type, i in self.relationship_type_mapping.items()}\n",
    "        \n",
    "        print(f\"    Found {len(unique_node_types)} node types: {unique_node_types}\")\n",
    "        print(f\"    Found {len(unique_rel_types)} relationship types: {unique_rel_types}\")\n",
    "    \n",
    "    def _prepare_node_data(self, nodes_df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Group nodes by type and prepare DataFrames with sequential IDs starting from 0.\n",
    "        \n",
    "        Returns:\n",
    "            Dict mapping node_type_string -> DataFrame with columns ['node_id', 'name', 'metadata_source', 'node_type_id', 'original_global_id']\n",
    "        \"\"\"\n",
    "        node_types_dict = {}\n",
    "        \n",
    "        # Add numeric type ID to nodes\n",
    "        nodes_df_copy = nodes_df.copy()\n",
    "        nodes_df_copy['node_type_id'] = nodes_df_copy['node_type'].map(self.node_type_mapping)\n",
    "        \n",
    "        # Group by node type and assign sequential IDs starting from 0\n",
    "        for node_type_str, group_df in nodes_df_copy.groupby('node_type'):\n",
    "            # Sort by original ID for consistency\n",
    "            group_df = group_df.sort_values('id').reset_index(drop=True)\n",
    "            \n",
    "            # Create sequential IDs starting from 0\n",
    "            num_nodes = len(group_df)\n",
    "            \n",
    "            # Build global to local mapping for this node type\n",
    "            global_ids = group_df['id'].values\n",
    "            local_ids = np.arange(num_nodes)  # 0, 1, 2, ..., num_nodes-1\n",
    "            \n",
    "            # Store the mapping for edge processing\n",
    "            for local_id, global_id in zip(local_ids, global_ids):\n",
    "                self.global_to_local_mapping[global_id] = (node_type_str, local_id)\n",
    "            \n",
    "            # Prepare DataFrame for DGL\n",
    "            prepared_df = pd.DataFrame({\n",
    "                'node_id': local_ids,  # Sequential IDs starting from 0\n",
    "                'name': group_df['name'].values,\n",
    "                'metadata_source': group_df['metadata_source'].values,\n",
    "                'node_type_id': group_df['node_type_id'].values,\n",
    "                'original_global_id': global_ids  # Keep original for reference\n",
    "            })\n",
    "            \n",
    "            node_types_dict[node_type_str] = prepared_df\n",
    "            print(f\"    {node_type_str}: {num_nodes:,} nodes (IDs: 0 to {num_nodes-1})\")\n",
    "            \n",
    "        return node_types_dict\n",
    "    \n",
    "    def _prepare_edge_data(self, edges_df: pd.DataFrame, nodes_df: pd.DataFrame) -> Dict[Tuple[str, str, str], pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Prepare edge data grouped by (src_type, edge_type, dst_type) using local IDs.\n",
    "        \n",
    "        Returns:\n",
    "            Dict mapping (src_type, edge_type, dst_type) -> DataFrame with columns ['src', 'dst', 'relationship_type_id']\n",
    "        \"\"\"\n",
    "        # Create node ID to type mapping for fast lookup\n",
    "        node_id_to_type = dict(zip(nodes_df['id'], nodes_df['node_type']))\n",
    "        \n",
    "        # Add relationship type IDs\n",
    "        edges_df_copy = edges_df.copy()\n",
    "        edges_df_copy['relationship_type_id'] = edges_df_copy['relationship_type'].map(self.relationship_type_mapping)\n",
    "        \n",
    "        # Add source and target node types\n",
    "        edges_df_copy['src_type'] = edges_df_copy['source_id'].map(node_id_to_type)\n",
    "        edges_df_copy['dst_type'] = edges_df_copy['target_id'].map(node_id_to_type)\n",
    "        \n",
    "        # Filter out edges with unknown nodes\n",
    "        valid_mask = (edges_df_copy['src_type'].notna()) & (edges_df_copy['dst_type'].notna())\n",
    "        valid_edges = edges_df_copy[valid_mask]\n",
    "        \n",
    "        if len(valid_edges) < len(edges_df_copy):\n",
    "            print(f\"    Warning: Filtered out {len(edges_df_copy) - len(valid_edges)} edges with unknown nodes\")\n",
    "        \n",
    "        # Group by (src_type, relationship_type, dst_type)\n",
    "        edge_types_dict = {}\n",
    "        \n",
    "        for (src_type, rel_type, dst_type), group_df in valid_edges.groupby(['src_type', 'relationship_type', 'dst_type']):\n",
    "            print(f\"    Processing {src_type} --[{rel_type}]--> {dst_type}: {len(group_df):,} edges\")\n",
    "            \n",
    "            # VECTORIZED APPROACH - Much faster than loops\n",
    "            group_df_reset = group_df.reset_index(drop=True)\n",
    "            \n",
    "            # Create mapping functions for this specific edge type\n",
    "            src_type_mapping = {global_id: local_id for global_id, (nt, local_id) in self.global_to_local_mapping.items() if nt == src_type}\n",
    "            dst_type_mapping = {global_id: local_id for global_id, (nt, local_id) in self.global_to_local_mapping.items() if nt == dst_type}\n",
    "            \n",
    "            # Vectorized mapping using pandas map\n",
    "            group_df_reset['src_local'] = group_df_reset['source_id'].map(src_type_mapping)\n",
    "            group_df_reset['dst_local'] = group_df_reset['target_id'].map(dst_type_mapping)\n",
    "            \n",
    "            # Filter valid edges (both src and dst must be mapped)\n",
    "            valid_mask = (group_df_reset['src_local'].notna()) & (group_df_reset['dst_local'].notna())\n",
    "            valid_edges_df = group_df_reset[valid_mask]\n",
    "            \n",
    "            if len(valid_edges_df) == 0:\n",
    "                print(f\"      Warning: No valid edges found for {src_type}-{rel_type}->{dst_type}\")\n",
    "                continue\n",
    "            \n",
    "            # Create edge DataFrame with local node IDs\n",
    "            edge_df = pd.DataFrame({\n",
    "                'src': valid_edges_df['src_local'].astype(int).values,  # Local IDs (0-based for each node type)\n",
    "                'dst': valid_edges_df['dst_local'].astype(int).values,  # Local IDs (0-based for each node type)\n",
    "                'relationship_type_id': valid_edges_df['relationship_type_id'].values,\n",
    "                'original_src_id': valid_edges_df['source_id'].values,  # Keep original for reference\n",
    "                'original_dst_id': valid_edges_df['target_id'].values   # Keep original for reference\n",
    "            })\n",
    "            \n",
    "            edge_types_dict[(src_type, rel_type, dst_type)] = edge_df\n",
    "            print(f\"      Created {len(edge_df):,} valid edges\")\n",
    "            \n",
    "        return edge_types_dict\n",
    "    \n",
    "    def _print_summary(self, node_types_dict: Dict[str, pd.DataFrame], \n",
    "                      edge_types_dict: Dict[Tuple[str, str, str], pd.DataFrame]):\n",
    "        \"\"\"Print summary of prepared data.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PRIMEKG DATA PREPARATION SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(\"\\nNode Type Mappings:\")\n",
    "        for str_type, int_type in self.node_type_mapping.items():\n",
    "            count = len(node_types_dict.get(str_type, []))\n",
    "            print(f\"  {int_type}: {str_type} ({count:,} nodes, IDs: 0 to {count-1})\")\n",
    "        \n",
    "        print(\"\\nRelationship Type Mappings:\")\n",
    "        for str_type, int_type in self.relationship_type_mapping.items():\n",
    "            print(f\"  {int_type}: {str_type}\")\n",
    "        \n",
    "        print(\"\\nPrepared Node Types:\")\n",
    "        total_nodes = 0\n",
    "        for node_type, df in node_types_dict.items():\n",
    "            min_id = df['node_id'].min()\n",
    "            max_id = df['node_id'].max()\n",
    "            print(f\"  {node_type}: {len(df):,} nodes (local IDs: {min_id} to {max_id})\")\n",
    "            total_nodes += len(df)\n",
    "        print(f\"  TOTAL: {total_nodes:,} nodes\")\n",
    "        \n",
    "        print(\"\\nPrepared Edge Types:\")\n",
    "        total_edges = 0\n",
    "        for (src_type, edge_type, dst_type), df in edge_types_dict.items():\n",
    "            print(f\"  {src_type} --[{edge_type}]--> {dst_type}: {len(df):,} edges\")\n",
    "            total_edges += len(df)\n",
    "        print(f\"  TOTAL: {total_edges:,} edges\")\n",
    "        \n",
    "        print(\"\\nData Format Verification:\")\n",
    "        for node_type, df in node_types_dict.items():\n",
    "            assert df['node_id'].min() == 0, f\"Node IDs for {node_type} don't start at 0!\"\n",
    "            assert df['node_id'].max() == len(df) - 1, f\"Node IDs for {node_type} are not sequential!\"\n",
    "            print(f\"  ✅ {node_type}: Sequential IDs 0 to {len(df)-1}\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    def get_type_mappings(self):\n",
    "        \"\"\"Return the type mappings for reference.\"\"\"\n",
    "        return {\n",
    "            'node_types': self.node_type_mapping,\n",
    "            'relationship_types': self.relationship_type_mapping,\n",
    "            'reverse_node_types': self.reverse_node_type_mapping,\n",
    "            'reverse_relationship_types': self.reverse_relationship_type_mapping\n",
    "        }\n",
    "    \n",
    "    def get_global_to_local_mapping(self):\n",
    "        \"\"\"Return the global to local ID mapping for reference.\"\"\"\n",
    "        return self.global_to_local_mapping.copy()\n",
    "    \n",
    "    def global_id_to_local(self, global_id: int) -> Tuple[str, int]:\n",
    "        \"\"\"Convert a global node ID to (node_type, local_id).\"\"\"\n",
    "        if global_id in self.global_to_local_mapping:\n",
    "            return self.global_to_local_mapping[global_id]\n",
    "        else:\n",
    "            raise ValueError(f\"Global ID {global_id} not found in mapping\")\n",
    "    \n",
    "    def local_id_to_global(self, node_type: str, local_id: int) -> int:\n",
    "        \"\"\"Convert (node_type, local_id) to global node ID.\"\"\"\n",
    "        for global_id, (nt, lid) in self.global_to_local_mapping.items():\n",
    "            if nt == node_type and lid == local_id:\n",
    "                return global_id\n",
    "        raise ValueError(f\"Local ID ({node_type}, {local_id}) not found in mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f43c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepGraphDB import DeepGraphDB\n",
    "import torch\n",
    "\n",
    "db = DeepGraphDB()\n",
    "# Initialize the loader\n",
    "loader = PrimeKGLoader()\n",
    "\n",
    "# Load and prepare data\n",
    "nodes_csv = \"data/nodes.csv\"  # Replace with your actual path\n",
    "edges_csv = \"data/edges.csv\"  # Replace with your actual path\n",
    "\n",
    "node_types_dict, edge_types_dict, mapping = loader.load_and_prepare_primekg(nodes_csv, edges_csv)\n",
    "\n",
    "    \n",
    "# Get type mappings for reference\n",
    "mappings = loader.get_type_mappings()\n",
    "print(\"\\nType mappings created:\")\n",
    "print(\"Node types:\", mappings['node_types'])\n",
    "print(\"Relationship types:\", mappings['relationship_types'])\n",
    "\n",
    "# Verify data format\n",
    "print(\"\\nData format verification:\")\n",
    "for node_type, df in node_types_dict.items():\n",
    "    print(f\"  {node_type}: node_id range {df['node_id'].min()}-{df['node_id'].max()}\")\n",
    "\n",
    "# Now you can use this data with your DGL graph analyzer\n",
    "print(\"\\nReady to load into DGL!\")\n",
    "print(\"Use: analyzer.bulk_load_heterogeneous_graph(node_types_dict, edge_types_dict)\")\n",
    "db.bulk_load_heterogeneous_graph(node_types_dict, edge_types_dict)\n",
    "db.set_mappings(loader.node_type_mapping, loader.relationship_type_mapping)\n",
    "db.set_global_to_local_mapping(mapping)\n",
    "\n",
    "# x = torch.rand(max(db.global_to_local_mapping.keys())+1, 256)\n",
    "x = torch.load(\"/home/cc/PHD/dglframework/DeepKG/start_feats.pt\")\n",
    "db.load_node_features_for_gnn(torch.tensor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ed419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.save_graph(\"/home/cc/PHD/dglframework/DeepKG/DeepGraphDB/graphs/primekg.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14af7c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from DeepGraphDB.gnns.heteroSAGEattn import AdvancedHeteroLinkPredictor, compute_loss\n",
    "\n",
    "in_feats = {ntype: x.shape[1] for ntype in db.graph.ntypes}\n",
    "# target_entities = ['drug', 'disease', 'geneprotein', 'effectphenotype']\n",
    "target_entities = ['geneprotein', 'disease', 'pathway', 'cellular_component', 'molecular_function']\n",
    "\n",
    "# Choose multiple edge types for prediction\n",
    "target_etypes = [ctype for ctype in db.graph.canonical_etypes if ctype[0] == \"geneprotein\" or ctype[2] in \"geneprotein\"]\n",
    "# target_etypes = [ctype for ctype in db.graph.canonical_etypes if ctype[0] in target_entities and ctype[2] in target_entities]\n",
    "# target_etypes = [('disease', 'contraindication', 'drug'), ('disease', 'indication', 'drug'), ('drug', 'contraindication', 'disease'), ('drug', 'indication', 'disease')]\n",
    "\n",
    "print(f\"Target edge types for prediction: {target_etypes}\")\n",
    "\n",
    "hidden_feats = 512\n",
    "out_feats = 512\n",
    "\n",
    "model = AdvancedHeteroLinkPredictor(\n",
    "    node_types=db.graph.ntypes,  # All node types in the graph\n",
    "    edge_types=db.graph.etypes,  # All edge types for GNN layers\n",
    "    in_feats=in_feats,\n",
    "    hidden_feats=hidden_feats,\n",
    "    out_feats=out_feats,\n",
    "    num_layers=3,\n",
    "    use_attention=True,\n",
    "    predictor_type='mlp',\n",
    "    target_etypes=target_etypes  # Only target edge types for prediction\n",
    ")\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "\n",
    "embs = db.train_model(model, compute_loss, target_etypes, target_entities, 'cuda', bs=325000, num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e2951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"/home/cc/PHD/dglframework/DeepKG/model.pt\")\n",
    "\n",
    "# save_dict = {\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'model_class': model.__class__.__name__,\n",
    "# }\n",
    "# # Save the model\n",
    "# torch.save(save_dict, \"/home/cc/PHD/dglframework/DeepKG/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6334de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ChromaVDB.chroma import ChromaFramework\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "vdb = ChromaFramework(persist_directory=\"./ChromaVDB/chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caa5fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = vdb.list_records()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267cc7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95df3284",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdb.read_record(['377cd122-e832-4a8d-8bdf-e5ae3f471a93'], include_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd863cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 min x 130k nodes\n",
    "BATCH_SIZE = 5000\n",
    "\n",
    "for entity in db.graph.ntypes:\n",
    "    embeddings_tensor = embs[entity].cpu()\n",
    "    total = embeddings_tensor.shape[0]\n",
    "    names = db.node_data[entity]['name'].tolist()\n",
    "    \n",
    "    for i in tqdm(range(0, total, BATCH_SIZE)):\n",
    "        end = i + BATCH_SIZE\n",
    "\n",
    "        batch_ids = [db.reverse_node_mapping[(entity, j)] for j in range(i, min(end, total))]\n",
    "        batch_embeddings = {\"graph\": embeddings_tensor[i:min(end, total)]}\n",
    "        batch_entities = [entity] * len(batch_embeddings[\"graph\"])\n",
    "        batch_names = names[i:min(end, total)]\n",
    "        batch_metadata = [{} for _ in range(len(batch_embeddings[\"graph\"]))]\n",
    "        batch_docs = [\"\" for _ in range(len(batch_embeddings[\"graph\"]))]\n",
    "\n",
    "        vdb.create_records(\n",
    "            global_ids=batch_ids,\n",
    "            names=batch_names,\n",
    "            entities=batch_entities,\n",
    "            metadatas=batch_metadata,\n",
    "            documents=batch_docs,\n",
    "            embeddings=batch_embeddings\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca929d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = vdb.list_records()\n",
    "len(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc53940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_embeddings_tsne(entity_embeddings_dict, \n",
    "                             perplexity=35, \n",
    "                             n_iter=1000, \n",
    "                             random_state=42,\n",
    "                             figsize=(12, 8),\n",
    "                             show_individual_labels=False,\n",
    "                             font_size=8):\n",
    "    \"\"\"\n",
    "    Create a t-SNE visualization of embeddings where each entity type has multiple embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "    entity_embeddings_dict (dict): Dictionary with entity types as keys and lists of embeddings as values\n",
    "                                  e.g., {\"person\": [[emb1], [emb2], ...], \"location\": [[emb1], [emb2], ...]}\n",
    "    perplexity (int): t-SNE perplexity parameter\n",
    "    n_iter (int): Number of iterations for t-SNE\n",
    "    random_state (int): Random state for reproducibility\n",
    "    figsize (tuple): Figure size for the plot\n",
    "    show_individual_labels (bool): Whether to show individual point labels (can be cluttered)\n",
    "    font_size (int): Font size for labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # Flatten the data and keep track of entity types\n",
    "    all_embeddings = []\n",
    "    entity_labels = []\n",
    "    entity_types = []\n",
    "    \n",
    "    for entity_type, embeddings_list in entity_embeddings_dict.items():\n",
    "        embeddings_array = np.array(embeddings_list.cpu())\n",
    "        \n",
    "        # Handle different input formats\n",
    "        if embeddings_array.ndim == 1:\n",
    "            embeddings_array = embeddings_array.reshape(1, -1)\n",
    "        \n",
    "        print(f\"{entity_type}: {len(embeddings_array)} embeddings of dimension {embeddings_array.shape[1]}\")\n",
    "        \n",
    "        for i, embedding in enumerate(embeddings_array):\n",
    "            all_embeddings.append(embedding)\n",
    "            entity_labels.append(f\"{entity_type}_{i}\")\n",
    "            entity_types.append(entity_type)\n",
    "    \n",
    "    all_embeddings = np.array(all_embeddings)\n",
    "    total_points = len(all_embeddings)\n",
    "    \n",
    "    print(f\"\\nTotal: {total_points} embeddings across {len(entity_embeddings_dict)} entity types\")\n",
    "    \n",
    "    # Standardize the embeddings\n",
    "    scaler = StandardScaler()\n",
    "    embeddings_scaled = scaler.fit_transform(all_embeddings)\n",
    "    \n",
    "    # Apply t-SNE\n",
    "    print(\"Applying t-SNE...\")\n",
    "    tsne = TSNE(n_components=2, \n",
    "                perplexity=min(perplexity, total_points-1),\n",
    "                n_iter=n_iter, \n",
    "                random_state=random_state,\n",
    "                verbose=1)\n",
    "    \n",
    "    embeddings_2d = tsne.fit_transform(embeddings_scaled)\n",
    "    \n",
    "    # Create the visualization\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Create a color palette for entity types\n",
    "    unique_entity_types = list(entity_embeddings_dict.keys())\n",
    "    colors = sns.color_palette(\"husl\", len(unique_entity_types))\n",
    "    color_map = dict(zip(unique_entity_types, colors))\n",
    "    \n",
    "    # Plot points colored by entity type\n",
    "    for entity_type in unique_entity_types:\n",
    "        # Get indices for this entity type\n",
    "        indices = [i for i, et in enumerate(entity_types) if et == entity_type]\n",
    "        x_coords = embeddings_2d[indices, 0]\n",
    "        y_coords = embeddings_2d[indices, 1]\n",
    "        \n",
    "        plt.scatter(x_coords, y_coords, \n",
    "                   c=[color_map[entity_type]], \n",
    "                   label=f\"{entity_type} ({len(indices)})\",\n",
    "                   s=60, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "        \n",
    "        # Optionally add individual labels\n",
    "        if show_individual_labels:\n",
    "            for i, idx in enumerate(indices):\n",
    "                plt.annotate(f\"{entity_type}_{i}\", \n",
    "                            (embeddings_2d[idx, 0], embeddings_2d[idx, 1]),\n",
    "                            xytext=(2, 2), textcoords='offset points',\n",
    "                            fontsize=font_size, alpha=0.8)\n",
    "    \n",
    "    plt.title(f't-SNE Visualization of Entity Embeddings\\n({total_points} total embeddings, {len(unique_entity_types)} entity types)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('t-SNE Component 1', fontsize=12)\n",
    "    plt.ylabel('t-SNE Component 2', fontsize=12)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Remove ticks for cleaner look\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return embeddings_2d, entity_labels, entity_types\n",
    "\n",
    "def analyze_clusters(embeddings_2d, entity_types, entity_embeddings_dict):\n",
    "    \"\"\"\n",
    "    Analyze the clustering quality and provide statistics.\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    df = pd.DataFrame({\n",
    "        'x': embeddings_2d[:, 0],\n",
    "        'y': embeddings_2d[:, 1],\n",
    "        'entity_type': entity_types\n",
    "    })\n",
    "    \n",
    "    print(\"\\n=== Cluster Analysis ===\")\n",
    "    print(\"Entity type distribution:\")\n",
    "    type_counts = Counter(entity_types)\n",
    "    for entity_type, count in type_counts.items():\n",
    "        print(f\"  {entity_type}: {count} embeddings\")\n",
    "    \n",
    "    # Calculate centroids for each entity type\n",
    "    print(\"\\nEntity type centroids:\")\n",
    "    for entity_type in entity_embeddings_dict.keys():\n",
    "        mask = df['entity_type'] == entity_type\n",
    "        centroid_x = df[mask]['x'].mean()\n",
    "        centroid_y = df[mask]['y'].mean()\n",
    "        print(f\"  {entity_type}: ({centroid_x:.3f}, {centroid_y:.3f})\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage with sample data\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: Create sample embeddings dictionary with multiple embeddings per entity type\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    print(\"Running example with sample data...\")\n",
    "    print(\"Replace 'sample_entities' with your actual entity_embeddings_dict\")\n",
    "    \n",
    "    # Visualize the embeddings\n",
    "    tsne_coords, labels, types = visualize_embeddings_tsne(\n",
    "        embs,\n",
    "        perplexity=35,  # Lower perplexity for smaller dataset\n",
    "        figsize=(12, 8),\n",
    "        show_individual_labels=False  # Set to True if you want individual point labels\n",
    "    )\n",
    "    \n",
    "    # Analyze clusters\n",
    "    # cluster_df = analyze_clusters(tsne_coords, types, sample_entities)\n",
    "    \n",
    "    # Optional: Save the plot\n",
    "    # plt.savefig('entity_embeddings_tsne.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Usage for your actual data:\n",
    "# your_entity_embeddings = {\n",
    "#     \"person\": [\n",
    "#         [0.1, 0.2, 0.3, ...],  # embedding 1 for person\n",
    "#         [0.4, 0.5, 0.6, ...],  # embedding 2 for person\n",
    "#         [0.7, 0.8, 0.9, ...],  # embedding 3 for person\n",
    "#         # ... more person embeddings\n",
    "#     ],\n",
    "#     \"location\": [\n",
    "#         [0.2, 0.3, 0.4, ...],  # embedding 1 for location\n",
    "#         [0.5, 0.6, 0.7, ...],  # embedding 2 for location\n",
    "#         # ... more location embeddings\n",
    "#     ],\n",
    "#     # ... more entity types\n",
    "# }\n",
    "# \n",
    "# tsne_coords, labels, types = visualize_embeddings_tsne(your_entity_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc43ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "def sample_embeddings_stratified(entity_embeddings_dict, max_total_samples=10000, min_per_type=50):\n",
    "    \"\"\"\n",
    "    Stratified sampling to handle large datasets while maintaining entity type proportions.\n",
    "    \n",
    "    Parameters:\n",
    "    entity_embeddings_dict (dict): Original embeddings dictionary\n",
    "    max_total_samples (int): Maximum total number of samples to keep\n",
    "    min_per_type (int): Minimum samples per entity type\n",
    "    \n",
    "    Returns:\n",
    "    dict: Sampled embeddings dictionary\n",
    "    \"\"\"\n",
    "    total_embeddings = sum(len(embs) for embs in entity_embeddings_dict.values())\n",
    "    \n",
    "    if total_embeddings <= max_total_samples:\n",
    "        print(f\"Dataset size ({total_embeddings}) is within limit. No sampling needed.\")\n",
    "        return entity_embeddings_dict\n",
    "    \n",
    "    print(f\"Large dataset detected ({total_embeddings:,} embeddings)\")\n",
    "    print(f\"Applying stratified sampling to reduce to ~{max_total_samples:,} samples...\")\n",
    "    \n",
    "    # Calculate proportional samples per entity type\n",
    "    sampled_dict = {}\n",
    "    remaining_budget = max_total_samples\n",
    "    entity_types = list(entity_embeddings_dict.keys())\n",
    "    \n",
    "    # First, ensure minimum samples per type\n",
    "    for entity_type in entity_types:\n",
    "        available = len(entity_embeddings_dict[entity_type])\n",
    "        min_samples = min(min_per_type, available, remaining_budget)\n",
    "        \n",
    "        if available > 0 and remaining_budget > 0:\n",
    "            indices = np.random.choice(available, min_samples, replace=False)\n",
    "            sampled_dict[entity_type] = [entity_embeddings_dict[entity_type][i] for i in indices]\n",
    "            remaining_budget -= min_samples\n",
    "        else:\n",
    "            sampled_dict[entity_type] = []\n",
    "    \n",
    "    # Distribute remaining budget proportionally\n",
    "    if remaining_budget > 0:\n",
    "        total_remaining = sum(len(entity_embeddings_dict[et]) - len(sampled_dict[et]) \n",
    "                            for et in entity_types)\n",
    "        \n",
    "        for entity_type in entity_types:\n",
    "            available_remaining = len(entity_embeddings_dict[entity_type]) - len(sampled_dict[entity_type])\n",
    "            if available_remaining > 0 and total_remaining > 0:\n",
    "                proportion = available_remaining / total_remaining\n",
    "                additional_samples = min(int(remaining_budget * proportion), available_remaining)\n",
    "                \n",
    "                if additional_samples > 0:\n",
    "                    # Get indices not already sampled\n",
    "                    already_sampled = set(np.random.choice(len(entity_embeddings_dict[entity_type]), \n",
    "                                                         len(sampled_dict[entity_type]), replace=False))\n",
    "                    available_indices = [i for i in range(len(entity_embeddings_dict[entity_type])) \n",
    "                                       if i not in already_sampled]\n",
    "                    \n",
    "                    additional_indices = np.random.choice(len(available_indices), \n",
    "                                                        additional_samples, replace=False)\n",
    "                    additional_embeddings = [entity_embeddings_dict[entity_type][available_indices[i]] \n",
    "                                           for i in additional_indices]\n",
    "                    sampled_dict[entity_type].extend(additional_embeddings)\n",
    "    \n",
    "    # Print sampling summary\n",
    "    print(\"\\nSampling summary:\")\n",
    "    total_sampled = 0\n",
    "    for entity_type in entity_types:\n",
    "        original_count = len(entity_embeddings_dict[entity_type])\n",
    "        sampled_count = len(sampled_dict[entity_type])\n",
    "        total_sampled += sampled_count\n",
    "        print(f\"  {entity_type}: {sampled_count:,} / {original_count:,} \"\n",
    "              f\"({100*sampled_count/original_count:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nTotal: {total_sampled:,} / {total_embeddings:,} \"\n",
    "          f\"({100*total_sampled/total_embeddings:.1f}%)\")\n",
    "    \n",
    "    return sampled_dict\n",
    "\n",
    "def visualize_embeddings_tsne(entity_embeddings_dict, \n",
    "                             perplexity=30, \n",
    "                             n_iter=1000, \n",
    "                             random_state=42,\n",
    "                             figsize=(12, 8),\n",
    "                             show_individual_labels=False,\n",
    "                             font_size=8,\n",
    "                             max_samples=10000,\n",
    "                             use_sampling=True,\n",
    "                             use_umap=False,\n",
    "                             alpha=0.6,\n",
    "                             point_size=20):\n",
    "    \"\"\"\n",
    "    Create a t-SNE/UMAP visualization optimized for large datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    entity_embeddings_dict (dict): Dictionary with entity types as keys and lists of embeddings as values\n",
    "    perplexity (int): t-SNE perplexity parameter\n",
    "    n_iter (int): Number of iterations for t-SNE\n",
    "    random_state (int): Random state for reproducibility\n",
    "    figsize (tuple): Figure size for the plot\n",
    "    show_individual_labels (bool): Whether to show individual point labels\n",
    "    font_size (int): Font size for labels\n",
    "    max_samples (int): Maximum number of samples to use (for performance)\n",
    "    use_sampling (bool): Whether to apply sampling for large datasets\n",
    "    use_umap (bool): Use UMAP instead of t-SNE (faster for large datasets)\n",
    "    alpha (float): Point transparency (useful for dense plots)\n",
    "    point_size (int): Size of points in scatter plot\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle large datasets with sampling\n",
    "    if use_sampling:\n",
    "        entity_embeddings_dict = sample_embeddings_stratified(\n",
    "            entity_embeddings_dict, max_total_samples=max_samples\n",
    "        )\n",
    "    \n",
    "    # Flatten the data and keep track of entity types\n",
    "    all_embeddings = []\n",
    "    entity_labels = []\n",
    "    entity_types = []\n",
    "    \n",
    "    for entity_type, embeddings_list in entity_embeddings_dict.items():\n",
    "        embeddings_array = np.array(embeddings_list)\n",
    "        \n",
    "        # Handle different input formats\n",
    "        if embeddings_array.ndim == 1:\n",
    "            embeddings_array = embeddings_array.reshape(1, -1)\n",
    "        \n",
    "        print(f\"{entity_type}: {len(embeddings_array)} embeddings of dimension {embeddings_array.shape[1]}\")\n",
    "        \n",
    "        for i, embedding in enumerate(embeddings_array):\n",
    "            all_embeddings.append(embedding)\n",
    "            entity_labels.append(f\"{entity_type}_{i}\")\n",
    "            entity_types.append(entity_type)\n",
    "    \n",
    "    all_embeddings = np.array(all_embeddings)\n",
    "    total_points = len(all_embeddings)\n",
    "    \n",
    "    print(f\"\\nProcessing {total_points:,} embeddings across {len(entity_embeddings_dict)} entity types\")\n",
    "    \n",
    "    # Memory usage warning\n",
    "    memory_estimate_gb = (total_points ** 2 * 8) / (1024**3)  # Rough estimate for t-SNE\n",
    "    if memory_estimate_gb > 4 and not use_umap:\n",
    "        print(f\"⚠️  Warning: Estimated memory usage: {memory_estimate_gb:.1f}GB\")\n",
    "        print(\"Consider using UMAP (set use_umap=True) or reducing max_samples\")\n",
    "    \n",
    "    # Standardize the embeddings\n",
    "    print(\"Standardizing embeddings...\")\n",
    "    scaler = StandardScaler()\n",
    "    embeddings_scaled = scaler.fit_transform(all_embeddings)\n",
    "    \n",
    "    # Apply dimensionality reduction\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if use_umap:\n",
    "        try:\n",
    "            import umap\n",
    "            print(\"Applying UMAP...\")\n",
    "            reducer = umap.UMAP(n_components=2, \n",
    "                              n_neighbors=min(15, total_points-1),\n",
    "                              random_state=random_state,\n",
    "                              verbose=True)\n",
    "            embeddings_2d = reducer.fit_transform(embeddings_scaled)\n",
    "            method_name = \"UMAP\"\n",
    "        except ImportError:\n",
    "            print(\"UMAP not installed. Install with: pip install umap-learn\")\n",
    "            print(\"Falling back to t-SNE...\")\n",
    "            use_umap = False\n",
    "    \n",
    "    if not use_umap:\n",
    "        print(\"Applying t-SNE...\")\n",
    "        tsne = TSNE(n_components=2, \n",
    "                    perplexity=min(perplexity, total_points-1),\n",
    "                    n_iter=n_iter, \n",
    "                    random_state=random_state,\n",
    "                    verbose=1,\n",
    "                    n_jobs=-1)  # Use all CPU cores\n",
    "        embeddings_2d = tsne.fit_transform(embeddings_scaled)\n",
    "        method_name = \"t-SNE\"\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"{method_name} completed in {duration:.1f} seconds\")\n",
    "    \n",
    "    # Create the visualization\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Create a color palette for entity types\n",
    "    unique_entity_types = list(set(entity_types))\n",
    "    colors = sns.color_palette(\"husl\", len(unique_entity_types))\n",
    "    color_map = dict(zip(unique_entity_types, colors))\n",
    "    \n",
    "    # Plot points colored by entity type\n",
    "    for entity_type in unique_entity_types:\n",
    "        # Get indices for this entity type\n",
    "        indices = [i for i, et in enumerate(entity_types) if et == entity_type]\n",
    "        x_coords = embeddings_2d[indices, 0]\n",
    "        y_coords = embeddings_2d[indices, 1]\n",
    "        \n",
    "        plt.scatter(x_coords, y_coords, \n",
    "                   c=[color_map[entity_type]], \n",
    "                   label=f\"{entity_type} ({len(indices):,})\",\n",
    "                   s=point_size, alpha=alpha, \n",
    "                   edgecolors='black', linewidth=0.3)\n",
    "        \n",
    "        # Only add individual labels for small datasets\n",
    "        if show_individual_labels and total_points < 1000:\n",
    "            for i, idx in enumerate(indices[:50]):  # Limit to first 50 per type\n",
    "                plt.annotate(f\"{entity_type}_{i}\", \n",
    "                            (embeddings_2d[idx, 0], embeddings_2d[idx, 1]),\n",
    "                            xytext=(2, 2), textcoords='offset points',\n",
    "                            fontsize=font_size, alpha=0.8)\n",
    "        elif show_individual_labels:\n",
    "            print(\"Too many points for individual labels. Skipping labels.\")\n",
    "    \n",
    "    sample_note = f\" (sampled)\" if use_sampling and total_points < sum(len(embs) for embs in entity_embeddings_dict.values()) else \"\"\n",
    "    \n",
    "    plt.title(f'{method_name} Visualization of Entity Embeddings{sample_note}\\n'\n",
    "              f'({total_points:,} embeddings, {len(unique_entity_types)} entity types)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(f'{method_name} Component 1', fontsize=12)\n",
    "    plt.ylabel(f'{method_name} Component 2', fontsize=12)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Remove ticks for cleaner look\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return embeddings_2d, entity_labels, entity_types\n",
    "\n",
    "def create_density_plot(entity_embeddings_dict, max_samples=50000, figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Create a density plot for very large datasets where individual points would be too cluttered.\n",
    "    \"\"\"\n",
    "    import scipy.stats as stats\n",
    "    \n",
    "    # Sample data if too large\n",
    "    if sum(len(embs) for embs in entity_embeddings_dict.values()) > max_samples:\n",
    "        entity_embeddings_dict = sample_embeddings_stratified(entity_embeddings_dict, max_samples)\n",
    "    \n",
    "    # Get 2D coordinates (using UMAP for speed)\n",
    "    coords_2d, labels, types = visualize_embeddings_tsne(\n",
    "        entity_embeddings_dict, use_umap=True, figsize=(1, 1)\n",
    "    )\n",
    "    plt.close()  # Close the scatter plot\n",
    "    \n",
    "    # Create density plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    \n",
    "    # Overall density plot\n",
    "    ax = axes[0, 0]\n",
    "    ax.hexbin(coords_2d[:, 0], coords_2d[:, 1], gridsize=50, cmap='Blues')\n",
    "    ax.set_title('Overall Density')\n",
    "    ax.set_xlabel('Component 1')\n",
    "    ax.set_ylabel('Component 2')\n",
    "    \n",
    "    # Density by entity type\n",
    "    unique_types = list(set(types))\n",
    "    colors = sns.color_palette(\"husl\", len(unique_types))\n",
    "    \n",
    "    ax = axes[0, 1]\n",
    "    for i, entity_type in enumerate(unique_types):\n",
    "        mask = np.array(types) == entity_type\n",
    "        if np.sum(mask) > 10:  # Only plot if enough points\n",
    "            ax.scatter(coords_2d[mask, 0], coords_2d[mask, 1], \n",
    "                      c=[colors[i]], alpha=0.3, s=1, label=entity_type)\n",
    "    ax.set_title('All Entity Types')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Individual density plots for top 2 entity types\n",
    "    type_counts = Counter(types)\n",
    "    top_types = [t[0] for t in type_counts.most_common(2)]\n",
    "    \n",
    "    for i, entity_type in enumerate(top_types):\n",
    "        ax = axes[1, i]\n",
    "        mask = np.array(types) == entity_type\n",
    "        ax.hexbin(coords_2d[mask, 0], coords_2d[mask, 1], \n",
    "                 gridsize=30, cmap='Reds')\n",
    "        ax.set_title(f'{entity_type} Density ({np.sum(mask):,} points)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Performance comparison function\n",
    "def compare_methods(entity_embeddings_dict, sample_size=5000):\n",
    "    \"\"\"\n",
    "    Compare t-SNE vs UMAP performance on a sample of your data.\n",
    "    \"\"\"\n",
    "    # Sample data for fair comparison\n",
    "    sampled_data = sample_embeddings_stratified(entity_embeddings_dict, sample_size)\n",
    "    \n",
    "    print(f\"\\nPerformance comparison on {sample_size:,} samples:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Test t-SNE\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        coords_tsne, _, _ = visualize_embeddings_tsne(\n",
    "            sampled_data, use_sampling=False, use_umap=False, figsize=(8, 6)\n",
    "        )\n",
    "        tsne_time = time.time() - start_time\n",
    "        print(f\"t-SNE: {tsne_time:.1f} seconds\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"t-SNE failed: {e}\")\n",
    "        tsne_time = float('inf')\n",
    "    \n",
    "    # Test UMAP\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        coords_umap, _, _ = visualize_embeddings_tsne(\n",
    "            sampled_data, use_sampling=False, use_umap=True, figsize=(8, 6)\n",
    "        )\n",
    "        umap_time = time.time() - start_time\n",
    "        print(f\"UMAP: {umap_time:.1f} seconds\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"UMAP failed: {e}\")\n",
    "        umap_time = float('inf')\n",
    "    \n",
    "    if tsne_time < float('inf') and umap_time < float('inf'):\n",
    "        speedup = tsne_time / umap_time\n",
    "        print(f\"\\nUMAP is {speedup:.1f}x faster than t-SNE\")\n",
    "        \n",
    "        if sample_size < 50000:\n",
    "            full_tsne_estimate = tsne_time * (sum(len(embs) for embs in entity_embeddings_dict.values()) / sample_size) ** 1.5\n",
    "            full_umap_estimate = umap_time * (sum(len(embs) for embs in entity_embeddings_dict.values()) / sample_size)\n",
    "            print(f\"Estimated time for full dataset:\")\n",
    "            print(f\"  t-SNE: {full_tsne_estimate/60:.1f} minutes\")\n",
    "            print(f\"  UMAP: {full_umap_estimate/60:.1f} minutes\")\n",
    "\n",
    "# Example usage optimized for large datasets\n",
    "if __name__ == \"__main__\":\n",
    "    # Example with larger sample data\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Simulate larger dataset\n",
    "    def create_large_sample_data(n_types=5, embeddings_per_type_range=(1000, 5000), embedding_dim=128):\n",
    "        \"\"\"Create sample data that mimics a large real dataset.\"\"\"\n",
    "        large_entities = {}\n",
    "        \n",
    "        for i in range(n_types):\n",
    "            entity_type = f\"entity_type_{i}\"\n",
    "            n_embeddings = np.random.randint(*embeddings_per_type_range)\n",
    "            \n",
    "            # Create embeddings with some structure\n",
    "            base_vector = np.random.randn(embedding_dim) * 2\n",
    "            embeddings = []\n",
    "            \n",
    "            for _ in range(n_embeddings):\n",
    "                # Add noise to base vector to create similar but distinct embeddings\n",
    "                embedding = base_vector + np.random.randn(embedding_dim) * 0.5\n",
    "                embeddings.append(embedding.tolist())\n",
    "            \n",
    "            large_entities[entity_type] = embeddings\n",
    "        \n",
    "        return large_entities\n",
    "    \n",
    "    total_embeddings = sum(len(emb) for emb in embs.values())\n",
    "    print(f\"Created sample dataset with {total_embeddings:,} embeddings\")\n",
    "    \n",
    "    # Demonstrate different visualization strategies\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STRATEGY 1: Sampled t-SNE (recommended for exploration)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    coords, labels, types = visualize_embeddings_tsne(\n",
    "        embs,\n",
    "        max_samples=25000,  # Reduce for performance\n",
    "        use_sampling=True,\n",
    "        use_umap=False,\n",
    "        perplexity=30,\n",
    "        alpha=0.7,\n",
    "        point_size=15\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STRATEGY 2: UMAP (faster, good for large datasets)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    coords_umap, labels_umap, types_umap = visualize_embeddings_tsne(\n",
    "        embs,\n",
    "        max_samples=35000,  # Can handle more with UMAP\n",
    "        use_sampling=True,\n",
    "        use_umap=True,\n",
    "        alpha=0.7,\n",
    "        point_size=15\n",
    "    )\n",
    "\n",
    "# Usage recommendations for your large dataset:\n",
    "\"\"\"\n",
    "For 100k+ embeddings, recommended approaches:\n",
    "\n",
    "1. SAMPLED t-SNE (best for initial exploration):\n",
    "   visualize_embeddings_tsne(your_data, max_samples=10000, use_umap=False)\n",
    "\n",
    "2. UMAP (faster, can handle more points):\n",
    "   visualize_embeddings_tsne(your_data, max_samples=25000, use_umap=True)\n",
    "\n",
    "3. DENSITY PLOTS (for very large datasets):\n",
    "   create_density_plot(your_data, max_samples=50000)\n",
    "\n",
    "4. PERFORMANCE COMPARISON:\n",
    "   compare_methods(your_data, sample_size=5000)\n",
    "\n",
    "Example with your data:\n",
    "your_entity_embeddings = {\n",
    "    \"person\": [[emb1], [emb2], ...],     # thousands of person embeddings\n",
    "    \"location\": [[emb1], [emb2], ...],   # thousands of location embeddings\n",
    "    # ... more entity types\n",
    "}\n",
    "\n",
    "# For quick exploration (recommended starting point):\n",
    "visualize_embeddings_tsne(your_entity_embeddings, max_samples=15000, use_umap=True)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
