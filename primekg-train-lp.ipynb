{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a94a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "class PrimeKGLoader:\n",
    "    \"\"\"\n",
    "    Prepares PrimeKG data for efficient loading into DGL heterogeneous graphs.\n",
    "    Each node type gets sequential IDs starting from 0.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.node_type_mapping = {}  # string -> int\n",
    "        self.relationship_type_mapping = {}  # string -> int\n",
    "        self.reverse_node_type_mapping = {}  # int -> string\n",
    "        self.reverse_relationship_type_mapping = {}  # int -> string\n",
    "        self.global_to_local_mapping = {}  # For reference: global_id -> (node_type, local_id)\n",
    "        \n",
    "    def load_and_prepare_primekg(self, nodes_csv_path: str, edges_csv_path: str):\n",
    "        \"\"\"\n",
    "        Load PrimeKG data and prepare it for bulk_load_heterogeneous_graph.\n",
    "        Each node type gets sequential IDs starting from 0.\n",
    "        \n",
    "        Args:\n",
    "            nodes_csv_path: Path to nodes CSV file\n",
    "            edges_csv_path: Path to edges CSV file\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (node_types_dict, edge_types_dict) ready for DGL loading\n",
    "        \"\"\"\n",
    "        print(\"Loading PrimeKG data...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Load raw data\n",
    "        print(\"  Reading CSV files...\")\n",
    "        nodes_df = pd.read_csv(nodes_csv_path, low_memory=False)\n",
    "        edges_df = pd.read_csv(edges_csv_path, low_memory=False)\n",
    "        \n",
    "        print(f\"  Loaded {len(nodes_df):,} nodes and {len(edges_df):,} edges\")\n",
    "        \n",
    "        # Create type mappings\n",
    "        print(\"  Creating type mappings...\")\n",
    "        self._create_type_mappings(nodes_df, edges_df)\n",
    "        \n",
    "        # Prepare node data (sequential IDs starting from 0 for each type)\n",
    "        print(\"  Preparing node data...\")\n",
    "        node_types_dict = self._prepare_node_data(nodes_df)\n",
    "        \n",
    "        # Prepare edge data (using local IDs)\n",
    "        print(\"  Preparing edge data...\")\n",
    "        edge_types_dict = self._prepare_edge_data(edges_df, nodes_df)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nData preparation completed in {total_time:.2f}s\")\n",
    "        \n",
    "        # Print summary\n",
    "        self._print_summary(node_types_dict, edge_types_dict)\n",
    "        \n",
    "        return node_types_dict, edge_types_dict, self.global_to_local_mapping\n",
    "    \n",
    "    def _create_type_mappings(self, nodes_df: pd.DataFrame, edges_df: pd.DataFrame):\n",
    "        \"\"\"Create mappings between string types and integer representations.\"\"\"\n",
    "        \n",
    "        # Node type mappings\n",
    "        unique_node_types = sorted(nodes_df['node_type'].unique())\n",
    "        self.node_type_mapping = {node_type: i for i, node_type in enumerate(unique_node_types)}\n",
    "        self.reverse_node_type_mapping = {i: node_type for node_type, i in self.node_type_mapping.items()}\n",
    "        \n",
    "        # Relationship type mappings\n",
    "        unique_rel_types = sorted(edges_df['relationship_type'].unique())\n",
    "        self.relationship_type_mapping = {rel_type: i for i, rel_type in enumerate(unique_rel_types)}\n",
    "        self.reverse_relationship_type_mapping = {i: rel_type for rel_type, i in self.relationship_type_mapping.items()}\n",
    "        \n",
    "        print(f\"    Found {len(unique_node_types)} node types: {unique_node_types}\")\n",
    "        print(f\"    Found {len(unique_rel_types)} relationship types: {unique_rel_types}\")\n",
    "    \n",
    "    def _prepare_node_data(self, nodes_df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Group nodes by type and prepare DataFrames with sequential IDs starting from 0.\n",
    "        \n",
    "        Returns:\n",
    "            Dict mapping node_type_string -> DataFrame with columns ['node_id', 'name', 'metadata_source', 'node_type_id', 'original_global_id']\n",
    "        \"\"\"\n",
    "        node_types_dict = {}\n",
    "        \n",
    "        # Add numeric type ID to nodes\n",
    "        nodes_df_copy = nodes_df.copy()\n",
    "        nodes_df_copy['node_type_id'] = nodes_df_copy['node_type'].map(self.node_type_mapping)\n",
    "        \n",
    "        # Group by node type and assign sequential IDs starting from 0\n",
    "        for node_type_str, group_df in nodes_df_copy.groupby('node_type'):\n",
    "            # Sort by original ID for consistency\n",
    "            group_df = group_df.sort_values('id').reset_index(drop=True)\n",
    "            \n",
    "            # Create sequential IDs starting from 0\n",
    "            num_nodes = len(group_df)\n",
    "            \n",
    "            # Build global to local mapping for this node type\n",
    "            global_ids = group_df['id'].values\n",
    "            local_ids = np.arange(num_nodes)  # 0, 1, 2, ..., num_nodes-1\n",
    "            \n",
    "            # Store the mapping for edge processing\n",
    "            for local_id, global_id in zip(local_ids, global_ids):\n",
    "                self.global_to_local_mapping[global_id] = (node_type_str, local_id)\n",
    "            \n",
    "            # Prepare DataFrame for DGL\n",
    "            prepared_df = pd.DataFrame({\n",
    "                'node_id': local_ids,  # Sequential IDs starting from 0\n",
    "                'name': group_df['name'].values,\n",
    "                'metadata_source': group_df['metadata_source'].values,\n",
    "                'node_type_id': group_df['node_type_id'].values,\n",
    "                'original_global_id': global_ids  # Keep original for reference\n",
    "            })\n",
    "            \n",
    "            node_types_dict[node_type_str] = prepared_df\n",
    "            print(f\"    {node_type_str}: {num_nodes:,} nodes (IDs: 0 to {num_nodes-1})\")\n",
    "            \n",
    "        return node_types_dict\n",
    "    \n",
    "    def _prepare_edge_data(self, edges_df: pd.DataFrame, nodes_df: pd.DataFrame) -> Dict[Tuple[str, str, str], pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Prepare edge data grouped by (src_type, edge_type, dst_type) using local IDs.\n",
    "        \n",
    "        Returns:\n",
    "            Dict mapping (src_type, edge_type, dst_type) -> DataFrame with columns ['src', 'dst', 'relationship_type_id']\n",
    "        \"\"\"\n",
    "        # Create node ID to type mapping for fast lookup\n",
    "        node_id_to_type = dict(zip(nodes_df['id'], nodes_df['node_type']))\n",
    "        \n",
    "        # Add relationship type IDs\n",
    "        edges_df_copy = edges_df.copy()\n",
    "        edges_df_copy['relationship_type_id'] = edges_df_copy['relationship_type'].map(self.relationship_type_mapping)\n",
    "        \n",
    "        # Add source and target node types\n",
    "        edges_df_copy['src_type'] = edges_df_copy['source_id'].map(node_id_to_type)\n",
    "        edges_df_copy['dst_type'] = edges_df_copy['target_id'].map(node_id_to_type)\n",
    "        \n",
    "        # Filter out edges with unknown nodes\n",
    "        valid_mask = (edges_df_copy['src_type'].notna()) & (edges_df_copy['dst_type'].notna())\n",
    "        valid_edges = edges_df_copy[valid_mask]\n",
    "        \n",
    "        if len(valid_edges) < len(edges_df_copy):\n",
    "            print(f\"    Warning: Filtered out {len(edges_df_copy) - len(valid_edges)} edges with unknown nodes\")\n",
    "        \n",
    "        # Group by (src_type, relationship_type, dst_type)\n",
    "        edge_types_dict = {}\n",
    "        \n",
    "        for (src_type, rel_type, dst_type), group_df in valid_edges.groupby(['src_type', 'relationship_type', 'dst_type']):\n",
    "            print(f\"    Processing {src_type} --[{rel_type}]--> {dst_type}: {len(group_df):,} edges\")\n",
    "            \n",
    "            # VECTORIZED APPROACH - Much faster than loops\n",
    "            group_df_reset = group_df.reset_index(drop=True)\n",
    "            \n",
    "            # Create mapping functions for this specific edge type\n",
    "            src_type_mapping = {global_id: local_id for global_id, (nt, local_id) in self.global_to_local_mapping.items() if nt == src_type}\n",
    "            dst_type_mapping = {global_id: local_id for global_id, (nt, local_id) in self.global_to_local_mapping.items() if nt == dst_type}\n",
    "            \n",
    "            # Vectorized mapping using pandas map\n",
    "            group_df_reset['src_local'] = group_df_reset['source_id'].map(src_type_mapping)\n",
    "            group_df_reset['dst_local'] = group_df_reset['target_id'].map(dst_type_mapping)\n",
    "            \n",
    "            # Filter valid edges (both src and dst must be mapped)\n",
    "            valid_mask = (group_df_reset['src_local'].notna()) & (group_df_reset['dst_local'].notna())\n",
    "            valid_edges_df = group_df_reset[valid_mask]\n",
    "            \n",
    "            if len(valid_edges_df) == 0:\n",
    "                print(f\"      Warning: No valid edges found for {src_type}-{rel_type}->{dst_type}\")\n",
    "                continue\n",
    "            \n",
    "            # Create edge DataFrame with local node IDs\n",
    "            edge_df = pd.DataFrame({\n",
    "                'src': valid_edges_df['src_local'].astype(int).values,  # Local IDs (0-based for each node type)\n",
    "                'dst': valid_edges_df['dst_local'].astype(int).values,  # Local IDs (0-based for each node type)\n",
    "                'relationship_type_id': valid_edges_df['relationship_type_id'].values,\n",
    "                'original_src_id': valid_edges_df['source_id'].values,  # Keep original for reference\n",
    "                'original_dst_id': valid_edges_df['target_id'].values   # Keep original for reference\n",
    "            })\n",
    "            \n",
    "            edge_types_dict[(src_type, rel_type, dst_type)] = edge_df\n",
    "            print(f\"      Created {len(edge_df):,} valid edges\")\n",
    "            \n",
    "        return edge_types_dict\n",
    "    \n",
    "    def _print_summary(self, node_types_dict: Dict[str, pd.DataFrame], \n",
    "                      edge_types_dict: Dict[Tuple[str, str, str], pd.DataFrame]):\n",
    "        \"\"\"Print summary of prepared data.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PRIMEKG DATA PREPARATION SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(\"\\nNode Type Mappings:\")\n",
    "        for str_type, int_type in self.node_type_mapping.items():\n",
    "            count = len(node_types_dict.get(str_type, []))\n",
    "            print(f\"  {int_type}: {str_type} ({count:,} nodes, IDs: 0 to {count-1})\")\n",
    "        \n",
    "        print(\"\\nRelationship Type Mappings:\")\n",
    "        for str_type, int_type in self.relationship_type_mapping.items():\n",
    "            print(f\"  {int_type}: {str_type}\")\n",
    "        \n",
    "        print(\"\\nPrepared Node Types:\")\n",
    "        total_nodes = 0\n",
    "        for node_type, df in node_types_dict.items():\n",
    "            min_id = df['node_id'].min()\n",
    "            max_id = df['node_id'].max()\n",
    "            print(f\"  {node_type}: {len(df):,} nodes (local IDs: {min_id} to {max_id})\")\n",
    "            total_nodes += len(df)\n",
    "        print(f\"  TOTAL: {total_nodes:,} nodes\")\n",
    "        \n",
    "        print(\"\\nPrepared Edge Types:\")\n",
    "        total_edges = 0\n",
    "        for (src_type, edge_type, dst_type), df in edge_types_dict.items():\n",
    "            print(f\"  {src_type} --[{edge_type}]--> {dst_type}: {len(df):,} edges\")\n",
    "            total_edges += len(df)\n",
    "        print(f\"  TOTAL: {total_edges:,} edges\")\n",
    "        \n",
    "        print(\"\\nData Format Verification:\")\n",
    "        for node_type, df in node_types_dict.items():\n",
    "            assert df['node_id'].min() == 0, f\"Node IDs for {node_type} don't start at 0!\"\n",
    "            assert df['node_id'].max() == len(df) - 1, f\"Node IDs for {node_type} are not sequential!\"\n",
    "            print(f\"  âœ… {node_type}: Sequential IDs 0 to {len(df)-1}\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    def get_type_mappings(self):\n",
    "        \"\"\"Return the type mappings for reference.\"\"\"\n",
    "        return {\n",
    "            'node_types': self.node_type_mapping,\n",
    "            'relationship_types': self.relationship_type_mapping,\n",
    "            'reverse_node_types': self.reverse_node_type_mapping,\n",
    "            'reverse_relationship_types': self.reverse_relationship_type_mapping\n",
    "        }\n",
    "    \n",
    "    def get_global_to_local_mapping(self):\n",
    "        \"\"\"Return the global to local ID mapping for reference.\"\"\"\n",
    "        return self.global_to_local_mapping.copy()\n",
    "    \n",
    "    def global_id_to_local(self, global_id: int) -> Tuple[str, int]:\n",
    "        \"\"\"Convert a global node ID to (node_type, local_id).\"\"\"\n",
    "        if global_id in self.global_to_local_mapping:\n",
    "            return self.global_to_local_mapping[global_id]\n",
    "        else:\n",
    "            raise ValueError(f\"Global ID {global_id} not found in mapping\")\n",
    "    \n",
    "    def local_id_to_global(self, node_type: str, local_id: int) -> int:\n",
    "        \"\"\"Convert (node_type, local_id) to global node ID.\"\"\"\n",
    "        for global_id, (nt, lid) in self.global_to_local_mapping.items():\n",
    "            if nt == node_type and lid == local_id:\n",
    "                return global_id\n",
    "        raise ValueError(f\"Local ID ({node_type}, {local_id}) not found in mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f43c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepGraphDB import DeepGraphDB\n",
    "import torch\n",
    "\n",
    "db = DeepGraphDB()\n",
    "# Initialize the loader\n",
    "loader = PrimeKGLoader()\n",
    "\n",
    "# Load and prepare data\n",
    "nodes_csv = \"data/nodes.csv\"  # Replace with your actual path\n",
    "edges_csv = \"data/edges.csv\"  # Replace with your actual path\n",
    "\n",
    "node_types_dict, edge_types_dict, mapping = loader.load_and_prepare_primekg(nodes_csv, edges_csv)\n",
    "\n",
    "    \n",
    "# Get type mappings for reference\n",
    "mappings = loader.get_type_mappings()\n",
    "print(\"\\nType mappings created:\")\n",
    "print(\"Node types:\", mappings['node_types'])\n",
    "print(\"Relationship types:\", mappings['relationship_types'])\n",
    "\n",
    "# Verify data format\n",
    "print(\"\\nData format verification:\")\n",
    "for node_type, df in node_types_dict.items():\n",
    "    print(f\"  {node_type}: node_id range {df['node_id'].min()}-{df['node_id'].max()}\")\n",
    "\n",
    "# Now you can use this data with your DGL graph analyzer\n",
    "print(\"\\nReady to load into DGL!\")\n",
    "print(\"Use: analyzer.bulk_load_heterogeneous_graph(node_types_dict, edge_types_dict)\")\n",
    "db.bulk_load_heterogeneous_graph(node_types_dict, edge_types_dict)\n",
    "db.set_mappings(loader.node_type_mapping, loader.relationship_type_mapping)\n",
    "db.set_global_to_local_mapping(mapping)\n",
    "\n",
    "x = torch.rand(max(db.global_to_local_mapping.keys())+1, 256)\n",
    "db.load_node_features_for_gnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14af7c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from DeepGraphDB.gnns.heteroSAGEattn import AdvancedHeteroLinkPredictor, compute_loss\n",
    "\n",
    "in_feats = {ntype: 256 for ntype in db.graph.ntypes}\n",
    "target_entities = ['drug', 'disease', 'geneprotein', 'effectphenotype']\n",
    "\n",
    "# Choose multiple edge types for prediction\n",
    "target_etypes = [ctype for ctype in db.graph.canonical_etypes if ctype[0] in target_entities and ctype[2] in target_entities]\n",
    "# target_etypes = [ctype for ctype in graph.canonical_etypes]\n",
    "\n",
    "print(f\"Target edge types for prediction: {target_etypes}\")\n",
    "\n",
    "hidden_feats = 256\n",
    "out_feats = 256\n",
    "\n",
    "model = AdvancedHeteroLinkPredictor(\n",
    "    node_types=db.graph.ntypes,  # All node types in the graph\n",
    "    edge_types=db.graph.etypes,  # All edge types for GNN layers\n",
    "    in_feats=in_feats,\n",
    "    hidden_feats=hidden_feats,\n",
    "    out_feats=out_feats,\n",
    "    num_layers=3,\n",
    "    use_attention=True,\n",
    "    predictor_type='multi_edge',\n",
    "    target_etypes=target_etypes  # Only target edge types for prediction\n",
    ")\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "\n",
    "embs = db.train_model(model, compute_loss, target_etypes, target_entities, 'cuda', num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6334de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ChromaVDB.chroma import ChromaFramework\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "vdb = ChromaFramework(persist_directory=\"./ChromaVDB/chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd863cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5 min x 130k nodes\n",
    "# BATCH_SIZE = 5000\n",
    "\n",
    "# for entity in db.graph.ntypes:\n",
    "#     embeddings_tensor = embs[entity].cpu()\n",
    "#     total = embeddings_tensor.shape[0]\n",
    "#     names = db.node_data[entity]['name'].tolist()\n",
    "    \n",
    "#     for i in tqdm(range(0, total, BATCH_SIZE)):\n",
    "#         end = i + BATCH_SIZE\n",
    "\n",
    "#         batch_ids = [db.reverse_node_mapping[(entity, j)] for j in range(i, min(end, total))]\n",
    "#         batch_embeddings = {\"graph\": embeddings_tensor[i:min(end, total)]}\n",
    "#         batch_entities = [entity] * len(batch_embeddings[\"graph\"])\n",
    "#         batch_names = names[i:min(end, total)]\n",
    "#         batch_metadata = [{} for _ in range(len(batch_embeddings[\"graph\"]))]\n",
    "#         batch_docs = [\"\" for _ in range(len(batch_embeddings[\"graph\"]))]\n",
    "\n",
    "#         vdb.create_records(\n",
    "#             global_ids=batch_ids,\n",
    "#             names=batch_names,\n",
    "#             entities=batch_entities,\n",
    "#             metadatas=batch_metadata,\n",
    "#             documents=batch_docs,\n",
    "#             embeddings=batch_embeddings\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca929d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = vdb.list_records()\n",
    "len(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d82ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdb.global_to_vids_mapping[63120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4ff266",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdb.global_to_vids_mapping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
