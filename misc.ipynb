{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa1ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from ChromaVDB.chroma import ChromaFramework\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "vdb = ChromaFramework(persist_directory=\"./ChromaVDB/chroma_db\")\n",
    "records = vdb.list_records()\n",
    "names = [record['name'] for record in records]\n",
    "embs = [record['embeddings'] for record in records]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc277e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('data/2025_03_29.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba05006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene_set = \"LNF\"\n",
    "gene_set = \"plasma\"\n",
    "# gene_measure = \"MUT\"\n",
    "gene_measure = \"VAF\"\n",
    "\n",
    "gene_data = data[[col for col in data.columns if gene_set in col and gene_measure in col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cd8d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = list(set([ gene.split('_')[0] for gene in gene_data.columns  ]))\n",
    "\n",
    "final_columns = []\n",
    "embeddings = []\n",
    "\n",
    "for gene in genes:\n",
    "    if gene in names:\n",
    "        final_columns.append(gene+\"_\"+gene_set+\"_\"+gene_measure)\n",
    "        embeddings.append(embs[names.index(gene)])\n",
    "    else:\n",
    "        print(gene)\n",
    "\n",
    "print(len(final_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f2d849",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_data = gene_data[final_columns]\n",
    "gene_data['pfs'] = data['PFS_Cens_updated']\n",
    "# gene_data = gene_data.dropna()\n",
    "gene_data = gene_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f4612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "                            roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# GPU Configuration and Optimization\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"GPU device:\", torch.cuda.get_device_name(0))\n",
    "    \n",
    "    # Set GPU memory management\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "\n",
    "class PFSClassificationMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP model that combines gene embeddings with patient binary mutation features\n",
    "    to predict binary PFS outcomes (0=poor prognosis, 1=good prognosis).\n",
    "    \n",
    "    Data structure:\n",
    "    - gene_embeddings: (n_genes, embedding_dim) - GNN embeddings for each gene\n",
    "    - patient_mutations: (n_patients, n_genes) - binary mutation matrix\n",
    "    - pfs_binary: (n_patients,) - binary PFS outcome for each patient\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_genes, embedding_dim, hidden_dims=[512, 256, 128], \n",
    "                 dropout_rate=0.3, combination_method='weighted_sum'):\n",
    "        super(PFSClassificationMLP, self).__init__()\n",
    "        \n",
    "        self.n_genes = n_genes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.combination_method = combination_method\n",
    "        \n",
    "        # Gene-level fusion methods\n",
    "        if combination_method == 'weighted_sum':\n",
    "            # Weight each gene embedding by mutation status and sum across genes\n",
    "            self.gene_weight = nn.Linear(1, 1, bias=False)  # Learn importance of mutation\n",
    "            patient_repr_dim = embedding_dim\n",
    "            \n",
    "        elif combination_method == 'attention':\n",
    "            # Use attention mechanism over genes\n",
    "            self.gene_attention = nn.MultiheadAttention(\n",
    "                embed_dim=embedding_dim, \n",
    "                num_heads=8, \n",
    "                batch_first=True\n",
    "            )\n",
    "            # Project mutation status to same dim as embeddings for attention\n",
    "            self.mutation_proj = nn.Linear(1, embedding_dim)\n",
    "            patient_repr_dim = embedding_dim\n",
    "            \n",
    "        elif combination_method == 'transformer':\n",
    "            # More sophisticated gene-level transformer\n",
    "            self.gene_transform = nn.TransformerEncoder(\n",
    "                nn.TransformerEncoderLayer(\n",
    "                    d_model=embedding_dim + 1,  # embedding + mutation status\n",
    "                    nhead=8,\n",
    "                    dim_feedforward=embedding_dim * 2,\n",
    "                    dropout=dropout_rate,\n",
    "                    batch_first=True\n",
    "                ),\n",
    "                num_layers=2\n",
    "            )\n",
    "            patient_repr_dim = embedding_dim + 1\n",
    "            \n",
    "        elif combination_method == 'gene_mlp':\n",
    "            # MLP for each gene, then aggregate\n",
    "            self.gene_mlp = nn.Sequential(\n",
    "                nn.Linear(embedding_dim + 1, embedding_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.Linear(embedding_dim, embedding_dim // 2)\n",
    "            )\n",
    "            patient_repr_dim = embedding_dim // 2\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"combination_method must be 'weighted_sum', 'attention', 'transformer', or 'gene_mlp'\")\n",
    "        \n",
    "        # Patient-level MLP for final binary classification\n",
    "        layers = []\n",
    "        prev_dim = patient_repr_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.BatchNorm1d(hidden_dim)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer for binary classification (logits)\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.patient_mlp = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, patient_mutations, gene_embeddings):\n",
    "        \"\"\"\n",
    "        Forward pass combining gene embeddings with patient mutations.\n",
    "        \n",
    "        Args:\n",
    "            patient_mutations: (batch_size, n_genes) binary mutation matrix\n",
    "            gene_embeddings: (n_genes, embedding_dim) gene embeddings from GNN\n",
    "        \n",
    "        Returns:\n",
    "            logits: (batch_size, 1) logits for binary classification\n",
    "        \"\"\"\n",
    "        batch_size = patient_mutations.shape[0]\n",
    "        \n",
    "        if self.combination_method == 'weighted_sum':\n",
    "            # For each patient, weight gene embeddings by mutation status\n",
    "            mutations_expanded = patient_mutations.unsqueeze(-1)  # (batch_size, n_genes, 1)\n",
    "            \n",
    "            # Apply learned weight to mutation status\n",
    "            weighted_mutations = self.gene_weight(mutations_expanded)  # (batch_size, n_genes, 1)\n",
    "            \n",
    "            # Weight gene embeddings by mutation status\n",
    "            gene_emb_expanded = gene_embeddings.unsqueeze(0)  # (1, n_genes, embedding_dim)\n",
    "            \n",
    "            # Broadcast multiply: (batch_size, n_genes, 1) * (1, n_genes, embedding_dim)\n",
    "            weighted_gene_embs = weighted_mutations * gene_emb_expanded  # (batch_size, n_genes, embedding_dim)\n",
    "            \n",
    "            # Sum across genes for each patient\n",
    "            patient_repr = weighted_gene_embs.sum(dim=1)  # (batch_size, embedding_dim)\n",
    "            \n",
    "        elif self.combination_method == 'attention':\n",
    "            # Use attention mechanism to focus on relevant genes\n",
    "            mutations_expanded = patient_mutations.unsqueeze(-1)  # (batch_size, n_genes, 1)\n",
    "            \n",
    "            # Project mutations to embedding dimension\n",
    "            mutation_features = self.mutation_proj(mutations_expanded)  # (batch_size, n_genes, embedding_dim)\n",
    "            \n",
    "            # Add gene embeddings to mutation features\n",
    "            gene_emb_batch = gene_embeddings.unsqueeze(0).repeat(batch_size, 1, 1)  # (batch_size, n_genes, embedding_dim)\n",
    "            combined_features = gene_emb_batch + mutation_features  # (batch_size, n_genes, embedding_dim)\n",
    "            \n",
    "            # Self-attention over genes\n",
    "            attended_features, _ = self.gene_attention(\n",
    "                combined_features, combined_features, combined_features\n",
    "            )  # (batch_size, n_genes, embedding_dim)\n",
    "            \n",
    "            # Average pool across genes\n",
    "            patient_repr = attended_features.mean(dim=1)  # (batch_size, embedding_dim)\n",
    "            \n",
    "        elif self.combination_method == 'transformer':\n",
    "            # Concatenate gene embeddings with mutation status\n",
    "            mutations_expanded = patient_mutations.unsqueeze(-1)  # (batch_size, n_genes, 1)\n",
    "            gene_emb_batch = gene_embeddings.unsqueeze(0).repeat(batch_size, 1, 1)  # (batch_size, n_genes, embedding_dim)\n",
    "            \n",
    "            # Concatenate: (batch_size, n_genes, embedding_dim + 1)\n",
    "            gene_features = torch.cat([gene_emb_batch, mutations_expanded], dim=-1)\n",
    "            \n",
    "            # Apply transformer encoder\n",
    "            transformed = self.gene_transform(gene_features)  # (batch_size, n_genes, embedding_dim + 1)\n",
    "            \n",
    "            # Global average pooling across genes\n",
    "            patient_repr = transformed.mean(dim=1)  # (batch_size, embedding_dim + 1)\n",
    "            \n",
    "        elif self.combination_method == 'gene_mlp':\n",
    "            # Process each gene with MLP, then aggregate\n",
    "            mutations_expanded = patient_mutations.unsqueeze(-1)  # (batch_size, n_genes, 1)\n",
    "            gene_emb_batch = gene_embeddings.unsqueeze(0).repeat(batch_size, 1, 1)  # (batch_size, n_genes, embedding_dim)\n",
    "            \n",
    "            # Concatenate and reshape for MLP processing\n",
    "            gene_features = torch.cat([gene_emb_batch, mutations_expanded], dim=-1)  # (batch_size, n_genes, embedding_dim + 1)\n",
    "            \n",
    "            # Reshape to process all genes at once: (batch_size * n_genes, embedding_dim + 1)\n",
    "            gene_features_flat = gene_features.view(-1, self.embedding_dim + 1)\n",
    "            \n",
    "            # Apply MLP to each gene\n",
    "            gene_processed = self.gene_mlp(gene_features_flat)  # (batch_size * n_genes, embedding_dim // 2)\n",
    "            \n",
    "            # Reshape back: (batch_size, n_genes, embedding_dim // 2)\n",
    "            gene_processed = gene_processed.view(batch_size, self.n_genes, -1)\n",
    "            \n",
    "            # Sum across genes for each patient\n",
    "            patient_repr = gene_processed.sum(dim=1)  # (batch_size, embedding_dim // 2)\n",
    "        \n",
    "        # Final MLP for binary classification (returns logits)\n",
    "        logits = self.patient_mlp(patient_repr)\n",
    "        return logits\n",
    "\n",
    "class PFSClassifier:\n",
    "    \"\"\"\n",
    "    Main class for training and evaluating binary PFS classification models with gene-level fusion.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, combination_method='weighted_sum', device='cuda'):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = device\n",
    "            torch.cuda.empty_cache()\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "            print(f\"Using GPU: {gpu_name} ({gpu_memory:.1f}GB)\")\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "            print(\"CUDA not available, using CPU\")\n",
    "                \n",
    "        self.combination_method = combination_method\n",
    "        self.model = None\n",
    "        \n",
    "    def prepare_data(self, patient_mutations, gene_embeddings, pfs_binary):\n",
    "        \"\"\"\n",
    "        Prepare data for training.\n",
    "        \n",
    "        Args:\n",
    "            patient_mutations: (n_patients, n_genes) binary mutation matrix\n",
    "            gene_embeddings: (n_genes, embedding_dim) gene embeddings from GNN\n",
    "            pfs_binary: (n_patients,) binary PFS outcomes (0/1)\n",
    "        \n",
    "        Returns:\n",
    "            Tensors ready for training\n",
    "        \"\"\"\n",
    "        # Convert to tensors (no normalization needed for binary classification)\n",
    "        mutations_tensor = torch.FloatTensor(patient_mutations)  # Keep binary\n",
    "        embeddings_tensor = torch.FloatTensor(gene_embeddings)   # Already normalized from GNN\n",
    "        pfs_tensor = torch.FloatTensor(pfs_binary)               # Binary labels\n",
    "        \n",
    "        return mutations_tensor, embeddings_tensor, pfs_tensor\n",
    "    \n",
    "    def create_model(self, n_genes, embedding_dim, **model_kwargs):\n",
    "        \"\"\"Create and initialize the model.\"\"\"\n",
    "        self.model = PFSClassificationMLP(\n",
    "            n_genes=n_genes,\n",
    "            embedding_dim=embedding_dim,\n",
    "            combination_method=self.combination_method,\n",
    "            **model_kwargs\n",
    "        ).to(self.device)\n",
    "        \n",
    "    def train(self, patient_mutations, gene_embeddings, pfs_binary, \n",
    "              test_size=0.15, validation_size=0.20, batch_size=512, \n",
    "              epochs=200, lr=0.001, weight_decay=1e-4, \n",
    "              early_stopping_patience=50, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the binary PFS classification model.\n",
    "        \n",
    "        Args:\n",
    "            patient_mutations: (n_patients, n_genes) binary mutation matrix\n",
    "            gene_embeddings: (n_genes, embedding_dim) gene embeddings from GNN\n",
    "            pfs_binary: (n_patients,) binary PFS outcomes (0/1)\n",
    "            \n",
    "        Returns:\n",
    "            training_history: dict with loss curves and metrics\n",
    "        \"\"\"\n",
    "        \n",
    "        # Prepare data\n",
    "        mut_tensor, emb_tensor, pfs_tensor = self.prepare_data(patient_mutations, gene_embeddings, pfs_binary)\n",
    "        n_patients = len(patient_mutations)\n",
    "        n_genes, embedding_dim = gene_embeddings.shape\n",
    "        \n",
    "        # Check class distribution and ensure minimum samples per class\n",
    "        class_counts = np.bincount(pfs_binary.astype(int))\n",
    "        if min(class_counts) < 5:\n",
    "            print(f\"Warning: Very few samples in minority class: {class_counts}\")\n",
    "            print(\"Consider collecting more data or using different evaluation strategy\")\n",
    "        \n",
    "        # Calculate class weights more robustly\n",
    "        if class_counts[1] == 0:\n",
    "            pos_weight = torch.FloatTensor([1.0]).to(self.device)\n",
    "        elif class_counts[0] == 0:\n",
    "            pos_weight = torch.FloatTensor([1.0]).to(self.device)\n",
    "        else:\n",
    "            pos_weight = torch.FloatTensor([class_counts[0] / class_counts[1]]).to(self.device)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Training binary classifier on {self.device.upper()}\")\n",
    "            print(f\"Data: {n_patients} patients, {n_genes} genes, {embedding_dim}D gene embeddings\")\n",
    "            print(f\"Class distribution: {class_counts[0]} negative (0), {class_counts[1]} positive (1)\")\n",
    "            print(f\"Class balance ratio: {class_counts[1] / len(pfs_binary):.3f}\")\n",
    "            print(f\"Positive class weight: {pos_weight.item():.3f}\")\n",
    "        \n",
    "        # Split data with better stratification\n",
    "        indices = np.arange(n_patients)\n",
    "        \n",
    "        # Ensure both classes are present in splits\n",
    "        if len(np.unique(pfs_binary)) < 2:\n",
    "            print(\"Warning: Only one class present in dataset!\")\n",
    "            # Use regular split without stratification\n",
    "            train_idx, temp_idx = train_test_split(indices, test_size=test_size+validation_size, random_state=42)\n",
    "            val_idx, test_idx = train_test_split(temp_idx, test_size=test_size/(test_size+validation_size), random_state=42)\n",
    "        else:\n",
    "            try:\n",
    "                train_idx, temp_idx = train_test_split(indices, test_size=test_size+validation_size, \n",
    "                                                     random_state=42, stratify=pfs_binary)\n",
    "                val_idx, test_idx = train_test_split(temp_idx, test_size=test_size/(test_size+validation_size), \n",
    "                                                   random_state=42, stratify=pfs_binary[temp_idx])\n",
    "            except ValueError as e:\n",
    "                print(f\"Stratification failed: {e}. Using random split.\")\n",
    "                train_idx, temp_idx = train_test_split(indices, test_size=test_size+validation_size, random_state=42)\n",
    "                val_idx, test_idx = train_test_split(temp_idx, test_size=test_size/(test_size+validation_size), random_state=42)\n",
    "        \n",
    "        # Check class distribution in splits\n",
    "        train_classes = np.bincount(pfs_binary[train_idx].astype(int), minlength=2)\n",
    "        val_classes = np.bincount(pfs_binary[val_idx].astype(int), minlength=2)\n",
    "        test_classes = np.bincount(pfs_binary[test_idx].astype(int), minlength=2)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Train split: {train_classes[0]} negative, {train_classes[1]} positive\")\n",
    "            print(f\"Val split: {val_classes[0]} negative, {val_classes[1]} positive\") \n",
    "            print(f\"Test split: {test_classes[0]} negative, {test_classes[1]} positive\")\n",
    "        \n",
    "        # Create splits and move to GPU\n",
    "        train_mut = mut_tensor[train_idx].to(self.device)\n",
    "        train_pfs = pfs_tensor[train_idx].to(self.device)\n",
    "        \n",
    "        val_mut = mut_tensor[val_idx].to(self.device)\n",
    "        val_pfs = pfs_tensor[val_idx].to(self.device)\n",
    "        \n",
    "        test_mut = mut_tensor[test_idx].to(self.device)\n",
    "        test_pfs = pfs_tensor[test_idx].to(self.device)\n",
    "        \n",
    "        # Gene embeddings stay constant (move to GPU once)\n",
    "        gene_emb_gpu = emb_tensor.to(self.device)\n",
    "        \n",
    "        if verbose and self.device == 'cuda':\n",
    "            print(f\"Data moved to GPU. Memory usage: {torch.cuda.memory_allocated()/1024**2:.1f}MB\")\n",
    "        \n",
    "        # Create model with better initialization\n",
    "        if self.model is None:\n",
    "            self.create_model(n_genes, embedding_dim)\n",
    "            # Initialize weights properly\n",
    "            for module in self.model.modules():\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(module.weight)\n",
    "                    if module.bias is not None:\n",
    "                        nn.init.zeros_(module.bias)\n",
    "                elif isinstance(module, nn.BatchNorm1d):\n",
    "                    nn.init.ones_(module.weight)\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            \n",
    "        if verbose and self.device == 'cuda':\n",
    "            model_params = sum(p.numel() for p in self.model.parameters())\n",
    "            print(f\"Model created with {model_params:,} parameters. GPU memory: {torch.cuda.memory_allocated()/1024**2:.1f}MB\")\n",
    "        \n",
    "        # Create data loader\n",
    "        train_dataset = TensorDataset(train_mut, train_pfs)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=False)\n",
    "        \n",
    "        # Optimizer and loss (with class weighting)\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5, mode='max')\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        \n",
    "        # Training history\n",
    "        history = {\n",
    "            'train_losses': [], 'val_losses': [], 'test_losses': [],\n",
    "            'train_acc': [], 'val_acc': [], 'test_acc': [],\n",
    "            'train_auc': [], 'val_auc': [], 'test_auc': []\n",
    "        }\n",
    "        \n",
    "        best_val_auc = 0.0\n",
    "        patience_counter = 0\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Starting training for {epochs} epochs...\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            epoch_train_loss = 0\n",
    "            train_logits = []\n",
    "            train_targets = []\n",
    "            \n",
    "            for batch_mut, batch_pfs in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(batch_mut, gene_emb_gpu).squeeze()\n",
    "                loss = criterion(logits, batch_pfs)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_train_loss += loss.item()\n",
    "                train_logits.append(logits.detach().cpu().numpy())\n",
    "                train_targets.append(batch_pfs.detach().cpu().numpy())\n",
    "            \n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_logits = self.model(val_mut, gene_emb_gpu).squeeze()\n",
    "                val_loss = criterion(val_logits, val_pfs)\n",
    "                \n",
    "                # Test evaluation\n",
    "                test_logits = self.model(test_mut, gene_emb_gpu).squeeze()\n",
    "                test_loss = criterion(test_logits, test_pfs)\n",
    "            \n",
    "            # Calculate metrics with robust error handling\n",
    "            train_logits_all = np.concatenate(train_logits)\n",
    "            train_targets_all = np.concatenate(train_targets)\n",
    "            train_probs = torch.sigmoid(torch.tensor(train_logits_all)).numpy()\n",
    "            train_preds = (train_probs > 0.5).astype(int)\n",
    "            \n",
    "            val_probs = torch.sigmoid(val_logits).cpu().numpy()\n",
    "            val_preds = (val_probs > 0.5).astype(int)\n",
    "            val_targets_np = val_pfs.cpu().numpy()\n",
    "            \n",
    "            test_probs = torch.sigmoid(test_logits).cpu().numpy()\n",
    "            test_preds = (test_probs > 0.5).astype(int)\n",
    "            test_targets_np = test_pfs.cpu().numpy()\n",
    "            \n",
    "            # Use robust metrics calculation\n",
    "            train_metrics = calculate_robust_metrics(train_targets_all, train_preds, train_probs)\n",
    "            val_metrics = calculate_robust_metrics(val_targets_np, val_preds, val_probs)\n",
    "            test_metrics = calculate_robust_metrics(test_targets_np, test_preds, test_probs)\n",
    "            \n",
    "            train_acc = train_metrics['accuracy']\n",
    "            val_acc = val_metrics['accuracy']\n",
    "            test_acc = test_metrics['accuracy']\n",
    "            \n",
    "            train_auc = train_metrics['auc']\n",
    "            val_auc = val_metrics['auc']\n",
    "            test_auc = test_metrics['auc']\n",
    "            \n",
    "            # Update history\n",
    "            history['train_losses'].append(epoch_train_loss / len(train_loader))\n",
    "            history['val_losses'].append(val_loss.item())\n",
    "            history['test_losses'].append(test_loss.item())\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            history['test_acc'].append(test_acc)\n",
    "            history['train_auc'].append(train_auc)\n",
    "            history['val_auc'].append(val_auc)\n",
    "            history['test_auc'].append(test_auc)\n",
    "            \n",
    "            # Learning rate scheduling (based on validation AUC)\n",
    "            scheduler.step(val_auc)\n",
    "            \n",
    "            # Early stopping (based on validation AUC)\n",
    "            if val_auc > best_val_auc:\n",
    "                best_val_auc = val_auc\n",
    "                patience_counter = 0\n",
    "                self.best_model_state = {k: v.clone() for k, v in self.model.state_dict().items()}\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "            \n",
    "            if verbose and epoch % 25 == 0:\n",
    "                print(f'Epoch {epoch}: Train Loss = {history[\"train_losses\"][-1]:.4f}, '\n",
    "                      f'Val Loss = {history[\"val_losses\"][-1]:.4f}, '\n",
    "                      f'Val Acc = {val_acc:.4f}, Val AUC = {val_auc:.4f}')\n",
    "        \n",
    "        # Load best model\n",
    "        if hasattr(self, 'best_model_state'):\n",
    "            self.model.load_state_dict(self.best_model_state)\n",
    "        \n",
    "        # Final evaluation with detailed metrics\n",
    "        if verbose:\n",
    "            print(\"\\n=== Final Training Summary ===\")\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Evaluate on all splits\n",
    "                train_logits_final = self.model(train_mut, gene_emb_gpu).squeeze()\n",
    "                val_logits_final = self.model(val_mut, gene_emb_gpu).squeeze()\n",
    "                test_logits_final = self.model(test_mut, gene_emb_gpu).squeeze()\n",
    "                \n",
    "                train_probs_final = torch.sigmoid(train_logits_final).cpu().numpy()\n",
    "                val_probs_final = torch.sigmoid(val_logits_final).cpu().numpy()\n",
    "                test_probs_final = torch.sigmoid(test_logits_final).cpu().numpy()\n",
    "                \n",
    "                train_preds_final = (train_probs_final > 0.5).astype(int)\n",
    "                val_preds_final = (val_probs_final > 0.5).astype(int)\n",
    "                test_preds_final = (test_probs_final > 0.5).astype(int)\n",
    "                \n",
    "                print(f\"Train - Predictions: {np.bincount(train_preds_final, minlength=2)}, Actual: {np.bincount(train_pfs.cpu().numpy().astype(int), minlength=2)}\")\n",
    "                print(f\"Val   - Predictions: {np.bincount(val_preds_final, minlength=2)}, Actual: {np.bincount(val_pfs.cpu().numpy().astype(int), minlength=2)}\")\n",
    "                print(f\"Test  - Predictions: {np.bincount(test_preds_final, minlength=2)}, Actual: {np.bincount(test_pfs.cpu().numpy().astype(int), minlength=2)}\")\n",
    "                \n",
    "                # Show probability distributions\n",
    "                print(f\"Val probability range: [{val_probs_final.min():.3f}, {val_probs_final.max():.3f}]\")\n",
    "                print(f\"Val probability mean: {val_probs_final.mean():.3f}\")\n",
    "        \n",
    "        # Clear GPU cache\n",
    "        if self.device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict(self, patient_mutations, gene_embeddings, batch_size=256, return_probs=False):\n",
    "        \"\"\"Make predictions on new data.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        all_logits = []\n",
    "        n_patients = len(patient_mutations)\n",
    "        \n",
    "        # Move gene embeddings to GPU once\n",
    "        gene_emb_gpu = torch.FloatTensor(gene_embeddings).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, n_patients, batch_size):\n",
    "                end_idx = min(i + batch_size, n_patients)\n",
    "                \n",
    "                batch_mut = torch.FloatTensor(patient_mutations[i:end_idx]).to(self.device)\n",
    "                batch_logits = self.model(batch_mut, gene_emb_gpu).cpu().numpy()\n",
    "                all_logits.append(batch_logits)\n",
    "        \n",
    "        # Combine all logits\n",
    "        logits = np.concatenate(all_logits, axis=0).flatten()\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        probabilities = torch.sigmoid(torch.tensor(logits)).numpy()\n",
    "        \n",
    "        if return_probs:\n",
    "            return probabilities\n",
    "        else:\n",
    "            # Return binary predictions\n",
    "            return (probabilities > 0.5).astype(int)\n",
    "    \n",
    "    def evaluate(self, patient_mutations, gene_embeddings, true_labels):\n",
    "        \"\"\"Evaluate model performance with classification metrics.\"\"\"\n",
    "        probabilities = self.predict(patient_mutations, gene_embeddings, return_probs=True)\n",
    "        predictions = (probabilities > 0.5).astype(int)\n",
    "        \n",
    "        # Classification metrics\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        precision = precision_score(true_labels, predictions, zero_division=0)\n",
    "        recall = recall_score(true_labels, predictions, zero_division=0)\n",
    "        f1 = f1_score(true_labels, predictions, zero_division=0)\n",
    "        \n",
    "        try:\n",
    "            auc = roc_auc_score(true_labels, probabilities)\n",
    "        except ValueError:\n",
    "            auc = 0.5  # Random performance if only one class\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'auc': auc\n",
    "        }\n",
    "        \n",
    "        return metrics, predictions, probabilities\n",
    "\n",
    "def calculate_robust_metrics(true_labels, predictions, probabilities):\n",
    "    \"\"\"Calculate metrics with robust error handling.\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Basic counts\n",
    "    tp = np.sum((true_labels == 1) & (predictions == 1))\n",
    "    tn = np.sum((true_labels == 0) & (predictions == 0))\n",
    "    fp = np.sum((true_labels == 0) & (predictions == 1))\n",
    "    fn = np.sum((true_labels == 1) & (predictions == 0))\n",
    "    \n",
    "    # Accuracy\n",
    "    metrics['accuracy'] = (tp + tn) / len(true_labels) if len(true_labels) > 0 else 0.0\n",
    "    \n",
    "    # Precision\n",
    "    metrics['precision'] = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    \n",
    "    # Recall (Sensitivity)\n",
    "    metrics['recall'] = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    \n",
    "    # Specificity\n",
    "    metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    \n",
    "    # F1 Score\n",
    "    prec = metrics['precision']\n",
    "    rec = metrics['recall']\n",
    "    metrics['f1'] = 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "    \n",
    "    # Balanced Accuracy\n",
    "    metrics['balanced_accuracy'] = (metrics['recall'] + metrics['specificity']) / 2\n",
    "    \n",
    "    # AUC\n",
    "    try:\n",
    "        if len(np.unique(true_labels)) > 1:\n",
    "            metrics['auc'] = roc_auc_score(true_labels, probabilities)\n",
    "        else:\n",
    "            metrics['auc'] = 0.5\n",
    "    except:\n",
    "        metrics['auc'] = 0.5\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f23f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfs_binary = gene_data['pfs'].values\n",
    "mutants = gene_data.drop(columns=['pfs'])\n",
    "patient_mutations = mutants.values\n",
    "gene_embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1b11ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e9a90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_mutations.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b2013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalize_embeddings(embeddings, method='l2'):\n",
    "    \"\"\"\n",
    "    Normalize embedding vectors.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: numpy array of shape (n_vectors, emb_dim)\n",
    "        method: 'l2', 'standard', or 'minmax'\n",
    "    \n",
    "    Returns:\n",
    "        normalized embeddings\n",
    "    \"\"\"\n",
    "    if method == 'l2':\n",
    "        # L2 normalization (unit vectors)\n",
    "        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        # Avoid division by zero\n",
    "        norms = np.where(norms == 0, 1, norms)\n",
    "        return embeddings / norms\n",
    "    \n",
    "    elif method == 'standard':\n",
    "        # Standardization (zero mean, unit variance)\n",
    "        scaler = StandardScaler()\n",
    "        return scaler.fit_transform(embeddings)\n",
    "    \n",
    "    elif method == 'minmax':\n",
    "        # Min-max normalization (0-1 range)\n",
    "        scaler = MinMaxScaler()\n",
    "        return scaler.fit_transform(embeddings)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'l2', 'standard', or 'minmax'\")\n",
    "\n",
    "def compute_patient_embeddings_vectorized(embeddings, mutations):\n",
    "    \"\"\"\n",
    "    Vectorized version for better performance.\n",
    "    \"\"\"\n",
    "    # Create mask for valid mutations\n",
    "    valid_mask = ~(np.isnan(mutations) | (mutations == 0))\n",
    "    \n",
    "    embeddings = normalize_embeddings(embeddings, method='l2')\n",
    "\n",
    "    # Set invalid mutations to 0 for computation\n",
    "    weights = np.where(valid_mask, mutations, 0)\n",
    "    \n",
    "    # Compute weighted embeddings\n",
    "    weighted_embeddings = weights @ embeddings  # (n_patients, emb_dim)\n",
    "    \n",
    "    # Compute total weights per patient\n",
    "    total_weights = np.sum(weights, axis=1, keepdims=True)  # (n_patients, 1)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    total_weights = np.where(total_weights == 0, 1, total_weights)\n",
    "    \n",
    "    # Compute mean embeddings\n",
    "    patient_embeddings = weighted_embeddings / total_weights\n",
    "    \n",
    "    # Set to zero vector for patients with no valid mutations\n",
    "    no_mutations_mask = np.sum(valid_mask, axis=1) == 0\n",
    "    patient_embeddings[no_mutations_mask] = 0\n",
    "    \n",
    "    return patient_embeddings\n",
    "\n",
    "# Alternative with more customization options\n",
    "def visualize_embeddings_advanced(patient_embeddings, patient_classes, \n",
    "                                 class_names=None, colors=None,\n",
    "                                 perplexity=50, random_state=42,\n",
    "                                 figsize=(12, 8)):\n",
    "    \"\"\"\n",
    "    Advanced visualization with customizable options.\n",
    "    \"\"\"\n",
    "    # Standardize embeddings\n",
    "    scaler = StandardScaler()\n",
    "    embeddings_scaled = scaler.fit_transform(patient_embeddings)\n",
    "    \n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, \n",
    "                random_state=random_state, n_iter=10000)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings_scaled)\n",
    "    \n",
    "    # Set up colors and labels\n",
    "    unique_classes = np.unique(patient_classes)\n",
    "    if colors is None:\n",
    "        colors = plt.cm.Set1(np.linspace(0, 1, len(unique_classes)))\n",
    "    if class_names is None:\n",
    "        class_names = [f'Class {c}' for c in unique_classes]\n",
    "    \n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    for i, class_label in enumerate(unique_classes):\n",
    "        mask = patient_classes == class_label\n",
    "        ax.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1], \n",
    "                  c=[colors[i]], label=class_names[i], \n",
    "                  alpha=0.7, s=60, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax.set_xlabel('t-SNE Component 1', fontsize=12)\n",
    "    ax.set_ylabel('t-SNE Component 2', fontsize=12)\n",
    "    ax.set_title('Patient Gene Mutation Embeddings (t-SNE)', fontsize=14, fontweight='bold')\n",
    "    ax.legend(frameon=True, fancybox=True, shadow=True)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add class distribution info\n",
    "    class_counts = [np.sum(patient_classes == c) for c in unique_classes]\n",
    "    info_text = f\"Total patients: {len(patient_classes)}\\n\"\n",
    "    for i, (name, count) in enumerate(zip(class_names, class_counts)):\n",
    "        info_text += f\"{name}: {count}\\n\"\n",
    "    \n",
    "    ax.text(0.02, 0.98, info_text, transform=ax.transAxes, \n",
    "            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return embeddings_2d\n",
    "\n",
    "final_embs = compute_patient_embeddings_vectorized(gene_embeddings, patient_mutations)\n",
    "\n",
    "embeddings_2d = visualize_embeddings_advanced(final_embs, pfs_binary, \n",
    "                                             class_names=['Control', 'Disease'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a7d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data shapes:\")\n",
    "print(f\"Patient mutations: {patient_mutations.shape} (n_patients, n_genes)\")\n",
    "print(f\"Gene embeddings: {gene_embeddings.shape} (n_genes, embedding_dim)\")\n",
    "print(f\"PFS binary: {pfs_binary.shape} (n_patients,)\")\n",
    "print(f\"Class distribution: {np.bincount(pfs_binary)} (0=poor, 1=good)\")\n",
    "print(f\"Mutation rate: {patient_mutations.mean():.3f}\")\n",
    "\n",
    "# Test different gene-level combination methods\n",
    "methods = ['weighted_sum', 'attention', 'gene_mlp']\n",
    "results = {}\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"\\n--- Training with {method} method ---\")\n",
    "    \n",
    "    classifier = PFSClassifier(combination_method=method)\n",
    "    \n",
    "    # Train model\n",
    "    history = classifier.train(\n",
    "        patient_mutations, gene_embeddings, pfs_binary,\n",
    "        epochs=150, batch_size=512, early_stopping_patience=75, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics, predictions, probabilities = classifier.evaluate(patient_mutations, gene_embeddings, pfs_binary)\n",
    "    results[method] = {\n",
    "        'metrics': metrics, \n",
    "        'predictions': predictions, \n",
    "        'probabilities': probabilities,\n",
    "        'history': history\n",
    "    }\n",
    "    \n",
    "    print(f\"Results for {method}:\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  F1 Score: {metrics['f1']:.4f}\")\n",
    "    print(f\"  AUC-ROC: {metrics['auc']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Find best method based on AUC\n",
    "best_method = max(results.keys(), key=lambda x: results[x]['metrics']['auc'])\n",
    "print(f\"\\nBest method: {best_method} (AUC = {results[best_method]['metrics']['auc']:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
