{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa1ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ChromaVDB.chroma import ChromaFramework\n",
    "from DeepGraphDB import DeepGraphDB\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "gdb = DeepGraphDB()\n",
    "gdb.load_graph(\"/home/cc/PHD/dglframework/DeepKG/DeepGraphDB/graphs/primekg.bin\")\n",
    "\n",
    "vdb = ChromaFramework(persist_directory=\"./ChromaVDB/chroma_db\")\n",
    "records = vdb.list_records()\n",
    "\n",
    "names = [record['name'] for record in records]\n",
    "embs = [record['embeddings'] for record in records]\n",
    "ids = [record['id'] for record in records]\n",
    "\n",
    "data = pd.read_excel('data/2025_03_29.xlsx') # Provare ad usare anche stadio-avanzato, IPI e Log10hGE\n",
    "# type: DLBCL (Diffuse Large B-cell Lymphoma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba05006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene_set = \"LNF\"\n",
    "gene_set = \"plasma\"\n",
    "# gene_measure = \"MUT\"\n",
    "gene_measure = \"VAF\"\n",
    "\n",
    "gene_data = data[[col for col in data.columns if gene_set in col and gene_measure in col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cd8d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = list(set([ gene.split('_')[0] for gene in gene_data.columns  ]))\n",
    "\n",
    "final_columns = []\n",
    "embeddings = []\n",
    "record_ids = []\n",
    "\n",
    "for gene in genes:\n",
    "    if gene in names:\n",
    "        final_columns.append(gene+\"_\"+gene_set+\"_\"+gene_measure)\n",
    "        embeddings.append(embs[names.index(gene)])\n",
    "        record_ids.append(ids[names.index(gene)])\n",
    "    else:\n",
    "        print(gene)\n",
    "\n",
    "print(len(final_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f2d849",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_data = gene_data[final_columns]\n",
    "gene_data['pfs'] = data['PFS_Cens_updated']\n",
    "# gene_data = gene_data.dropna()\n",
    "gene_data = gene_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1a1217",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfs_binary = gene_data['pfs'].values\n",
    "mutants = gene_data.drop(columns=['pfs'])\n",
    "patient_mutations = mutants.values\n",
    "gene_embeddings = np.array(embeddings)\n",
    "\n",
    "# # L2 normalization\n",
    "# norms = np.linalg.norm(gene_embeddings, axis=1, keepdims=True)\n",
    "# # Avoid division by zero\n",
    "# norms = np.where(norms == 0, 1, norms)\n",
    "# gene_embeddings = gene_embeddings / norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae4851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, balanced_accuracy_score\n",
    "\n",
    "class PFSClassificationMLP(nn.Module):\n",
    "    def __init__(self, n_genes, embedding_dim, hidden_dims=[512, 256]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_genes = n_genes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Gene-level weighted sum\n",
    "        self.gene_weight = nn.Linear(1, 1, bias=False)\n",
    "        \n",
    "        # Patient-level MLP\n",
    "        layers = []\n",
    "        prev_dim = embedding_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.patient_mlp = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, patient_mutations, gene_embeddings):\n",
    "        batch_size = patient_mutations.shape[0]\n",
    "        \n",
    "        # Weight gene embeddings by mutation status\n",
    "        mutations_expanded = patient_mutations.unsqueeze(-1)\n",
    "        weighted_mutations = self.gene_weight(mutations_expanded)\n",
    "        \n",
    "        gene_emb_expanded = gene_embeddings.unsqueeze(0)\n",
    "        weighted_gene_embs = weighted_mutations * gene_emb_expanded\n",
    "        \n",
    "        # Sum across genes\n",
    "        patient_repr = weighted_gene_embs.sum(dim=1)\n",
    "\n",
    "        # Normalize patient representations\n",
    "        # patient_repr = torch.nn.functional.normalize(patient_repr, p=2, dim=1)\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.patient_mlp(patient_repr)\n",
    "        return logits\n",
    "    \n",
    "class PFSAttentionMLP(nn.Module):\n",
    "    def __init__(self, n_genes, embedding_dim, hidden_dims=[512, 256], num_heads=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_genes = n_genes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = min(num_heads, embedding_dim // 16)  # Ensure reasonable head size\n",
    "        \n",
    "        # Project mutation features to embedding dimension\n",
    "        self.mutation_proj = nn.Sequential(\n",
    "            nn.Linear(1, embedding_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(embedding_dim // 2, embedding_dim),\n",
    "            nn.LayerNorm(embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Multi-head attention between embeddings and mutations\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=embedding_dim,\n",
    "            num_heads=self.num_heads,\n",
    "            batch_first=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        \n",
    "        # Output projection after attention\n",
    "        self.attention_proj = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Final MLP for classification\n",
    "        layers = []\n",
    "        prev_dim = embedding_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.classifier = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, patient_mutations, gene_embeddings):\n",
    "        batch_size = patient_mutations.shape[0]\n",
    "        \n",
    "        # Project mutations to embedding space\n",
    "        mutations_expanded = patient_mutations.unsqueeze(-1)  # (batch_size, n_genes, 1)\n",
    "        mutation_features = self.mutation_proj(mutations_expanded)  # (batch_size, n_genes, embedding_dim)\n",
    "        \n",
    "        # Prepare gene embeddings for attention\n",
    "        gene_emb_batch = gene_embeddings.unsqueeze(0).repeat(batch_size, 1, 1)  # (batch_size, n_genes, embedding_dim)\n",
    "        \n",
    "        # Use mutations as queries, gene embeddings as keys and values\n",
    "        attended_features, attention_weights = self.attention(\n",
    "            query=mutation_features,\n",
    "            key=gene_emb_batch,\n",
    "            value=gene_emb_batch\n",
    "        )  # (batch_size, n_genes, embedding_dim)\n",
    "        \n",
    "        # Apply projection after attention\n",
    "        attended_features = self.attention_proj(attended_features)\n",
    "        \n",
    "        # Weighted pooling based on mutation status\n",
    "        mutation_weights = patient_mutations.unsqueeze(-1)  # (batch_size, n_genes, 1)\n",
    "        weighted_features = attended_features * (mutation_weights + 0.1)  # Small bias for non-mutated genes\n",
    "        \n",
    "        # Global average pooling with normalization\n",
    "        mutation_counts = patient_mutations.sum(dim=1, keepdim=True).unsqueeze(-1) + 1e-6  # (batch_size, 1, 1)\n",
    "        patient_repr = weighted_features.sum(dim=1) / mutation_counts.squeeze(-1)  # (batch_size, embedding_dim)\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.classifier(patient_repr)\n",
    "        return logits\n",
    "\n",
    "class PFSMutationOnlyMLP(nn.Module):\n",
    "    def __init__(self, n_genes, hidden_dims=[512, 256]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_genes = n_genes\n",
    "        \n",
    "        # Direct MLP on mutation features\n",
    "        layers = []\n",
    "        prev_dim = n_genes\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, patient_mutations):\n",
    "        # Direct classification from mutation features\n",
    "        logits = self.mlp(patient_mutations)\n",
    "        return logits\n",
    "\n",
    "class PFSClassifier:\n",
    "    def __init__(self, use_kg = True,  device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.device = device\n",
    "        self.model = None\n",
    "        self.use_kg = use_kg\n",
    "        \n",
    "    def create_model(self, n_genes, embedding_dim, **kwargs):\n",
    "        if self.use_kg:\n",
    "            self.model = PFSClassificationMLP(\n",
    "            #self.model = PFSAttentionMLP(\n",
    "                n_genes=n_genes,\n",
    "                embedding_dim=embedding_dim,\n",
    "                **kwargs\n",
    "            ).to(self.device)\n",
    "        else:\n",
    "            self.model = PFSMutationOnlyMLP(\n",
    "                n_genes=n_genes,\n",
    "                **kwargs\n",
    "            ).to(self.device)\n",
    "        \n",
    "        # Initialize weights\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def train_fold(self, train_mutations, train_labels, val_mutations, val_labels, \n",
    "                   gene_embeddings, epochs=150, lr=0.001, max_patience=50):\n",
    "        \n",
    "        # Convert to tensors and move to device\n",
    "        train_mut = torch.FloatTensor(train_mutations).to(self.device)\n",
    "        train_pfs = torch.FloatTensor(train_labels).to(self.device)\n",
    "        val_mut = torch.FloatTensor(val_mutations).to(self.device)\n",
    "        val_pfs = torch.FloatTensor(val_labels).to(self.device)\n",
    "        gene_emb = torch.FloatTensor(gene_embeddings).to(self.device)\n",
    "        \n",
    "        # Calculate class weights for imbalanced data\n",
    "        class_counts = np.bincount(train_labels.astype(int))\n",
    "        if len(class_counts) == 2 and class_counts[1] > 0:\n",
    "            pos_weight = torch.FloatTensor([class_counts[0] / class_counts[1]]).to(self.device)\n",
    "        else:\n",
    "            pos_weight = torch.FloatTensor([1.0]).to(self.device)\n",
    "        \n",
    "        # Setup training with class balancing\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        \n",
    "        best_val_f1 = 0.0\n",
    "        patience = 0\n",
    "        best_state = None\n",
    "        best_auc = 0.0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            optimizer.zero_grad()\n",
    "            if self.use_kg:\n",
    "                logits = self.model(train_mut, gene_emb).squeeze()\n",
    "            else:\n",
    "                logits = self.model(train_mut).squeeze()\n",
    "            loss = criterion(logits, train_pfs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                if self.use_kg:\n",
    "                    val_logits = self.model(val_mut, gene_emb).squeeze()\n",
    "                else:\n",
    "                    val_logits = self.model(val_mut).squeeze()\n",
    "                val_probs = torch.sigmoid(val_logits).cpu().numpy()\n",
    "            \n",
    "            val_preds = (val_probs > 0.5).astype(int)\n",
    "            if len(np.unique(val_labels)) > 1:\n",
    "                val_auc = roc_auc_score(val_labels, val_probs)\n",
    "            else:\n",
    "                val_auc = 0.5\n",
    "            val_f1 = f1_score(val_labels, val_preds, zero_division=0)\n",
    "            val_balanced_acc = balanced_accuracy_score(val_labels, val_preds)\n",
    "            \n",
    "            # Early stopping based on F1 score\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                best_auc = val_auc\n",
    "                best_balanced_acc = val_balanced_acc\n",
    "                patience = 0\n",
    "                best_state = {k: v.clone() for k, v in self.model.state_dict().items()}\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= max_patience:  # Increased patience\n",
    "                    break\n",
    "        \n",
    "        # Load best model\n",
    "        if best_state is not None:\n",
    "            self.model.load_state_dict(best_state)\n",
    "        \n",
    "        return best_auc, best_val_f1, best_balanced_acc, patience\n",
    "    \n",
    "    def cross_validate(self, patient_mutations, gene_embeddings, pfs_binary, \n",
    "                      n_splits=5, n_repeats=3, epochs=100, lr=0.001, **model_kwargs):\n",
    "        \n",
    "        n_genes, embedding_dim = gene_embeddings.shape\n",
    "        \n",
    "        # Setup repeated stratified k-fold cross-validation\n",
    "        rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=42)\n",
    "        \n",
    "        fold_aucs = []\n",
    "        fold_f1s = []\n",
    "        fold_balanced_accs = []\n",
    "        \n",
    "        fold_count = 0\n",
    "        for train_idx, val_idx in rskf.split(patient_mutations, pfs_binary):\n",
    "            fold_count += 1\n",
    "            \n",
    "            # Create fresh model for each fold\n",
    "            self.create_model(n_genes, embedding_dim, **model_kwargs)\n",
    "            \n",
    "            # Get fold data\n",
    "            train_mut = patient_mutations[train_idx]\n",
    "            train_pfs = pfs_binary[train_idx]\n",
    "            val_mut = patient_mutations[val_idx]\n",
    "            val_pfs = pfs_binary[val_idx]\n",
    "            \n",
    "            # Train fold\n",
    "            val_auc, val_f1, val_balanced_acc, tot_epochs = self.train_fold(train_mut, train_pfs, val_mut, val_pfs, \n",
    "                                                              gene_embeddings, epochs, lr)\n",
    "            \n",
    "            fold_aucs.append(val_auc)\n",
    "            fold_f1s.append(val_f1)\n",
    "            fold_balanced_accs.append(val_balanced_acc)\n",
    "            \n",
    "            print(f\"Fold {fold_count}: AUC = {val_auc:.3f}, F1 = {val_f1:.3f}, Balanced Acc = {val_balanced_acc:.3f}, Max epoch = {tot_epochs}\")\n",
    "        \n",
    "        mean_auc = np.mean(fold_aucs)\n",
    "        std_auc = np.std(fold_aucs)\n",
    "        mean_f1 = np.mean(fold_f1s)\n",
    "        std_f1 = np.std(fold_f1s)\n",
    "        mean_balanced_acc = np.mean(fold_balanced_accs)\n",
    "        std_balanced_acc = np.std(fold_balanced_accs)\n",
    "        \n",
    "        print(f\"Cross-validation results ({n_splits}-fold, {n_repeats} repeats):\")\n",
    "        print(f\"AUC:          {mean_auc:.3f} ± {std_auc:.3f}\")\n",
    "        print(f\"F1:           {mean_f1:.3f} ± {std_f1:.3f}\")\n",
    "        print(f\"Balanced Acc: {mean_balanced_acc:.3f} ± {std_balanced_acc:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            'fold_aucs': fold_aucs,\n",
    "            'fold_f1s': fold_f1s,\n",
    "            'fold_balanced_accs': fold_balanced_accs,\n",
    "            'mean_auc': mean_auc,\n",
    "            'std_auc': std_auc,\n",
    "            'mean_f1': mean_f1,\n",
    "            'std_f1': std_f1,\n",
    "            'mean_balanced_acc': mean_balanced_acc,\n",
    "            'std_balanced_acc': std_balanced_acc,\n",
    "            'n_splits': n_splits,\n",
    "            'n_repeats': n_repeats,\n",
    "            'total_folds': fold_count\n",
    "        }\n",
    "    \n",
    "    def fit(self, patient_mutations, gene_embeddings, pfs_binary, \n",
    "            epochs=100, lr=0.001, **model_kwargs):\n",
    "        \n",
    "        n_genes, embedding_dim = gene_embeddings.shape\n",
    "        self.create_model(n_genes, embedding_dim, **model_kwargs)\n",
    "        \n",
    "        # Calculate class weights\n",
    "        class_counts = np.bincount(pfs_binary.astype(int))\n",
    "        if len(class_counts) == 2 and class_counts[1] > 0:\n",
    "            pos_weight = torch.FloatTensor([class_counts[0] / class_counts[1]]).to(self.device)\n",
    "        else:\n",
    "            pos_weight = torch.FloatTensor([1.0]).to(self.device)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        mut_tensor = torch.FloatTensor(patient_mutations).to(self.device)\n",
    "        pfs_tensor = torch.FloatTensor(pfs_binary).to(self.device)\n",
    "        gene_emb = torch.FloatTensor(gene_embeddings).to(self.device)\n",
    "        \n",
    "        # Setup training with class balancing\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            optimizer.zero_grad()\n",
    "            logits = self.model(mut_tensor, gene_emb).squeeze()\n",
    "            loss = criterion(logits, pfs_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    def predict(self, patient_mutations, gene_embeddings):\n",
    "        self.model.eval()\n",
    "        \n",
    "        mut_tensor = torch.FloatTensor(patient_mutations).to(self.device)\n",
    "        gene_emb = torch.FloatTensor(gene_embeddings).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = self.model(mut_tensor, gene_emb).squeeze()\n",
    "            probabilities = torch.sigmoid(logits).cpu().numpy()\n",
    "        \n",
    "        predictions = (probabilities > 0.5).astype(int)\n",
    "        \n",
    "        return predictions, probabilities\n",
    "    \n",
    "    def evaluate(self, patient_mutations, gene_embeddings, true_labels):\n",
    "        predictions, probabilities = self.predict(patient_mutations, gene_embeddings)\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(true_labels, predictions),\n",
    "            'f1': f1_score(true_labels, predictions, zero_division=0),\n",
    "            'balanced_accuracy': balanced_accuracy_score(true_labels, predictions),\n",
    "            'auc': roc_auc_score(true_labels, probabilities) if len(np.unique(true_labels)) > 1 else 0.5\n",
    "        }\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5253eb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier = PFSClassifier()\n",
    "cv_results = classifier.cross_validate(patient_mutations, gene_embeddings, pfs_binary, 10, 5, 250)\n",
    "# This will run 5-fold CV repeated 3 times = 15 total folds for more robust estimates\n",
    "\n",
    "# classifier.fit(patient_mutations, gene_embeddings, pfs_binary)\n",
    "# metrics = classifier.evaluate(test_mutations, gene_embeddings, test_labels)\n",
    "# print(f\"Test AUC: {metrics['auc']:.3f}, Test F1: {metrics['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d302c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = PFSClassifier(use_kg=False)\n",
    "cv_results = classifier.cross_validate(patient_mutations, gene_embeddings, pfs_binary, 10, 5, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f6e132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation results (20-fold, 5 repeats, usekg, mlp):\n",
    "# AUC:          0.730 ± 0.205\n",
    "# F1:           0.740 ± 0.139\n",
    "# Balanced Acc: 0.775 ± 0.156\n",
    "\n",
    "# Cross-validation results (20-fold, 5 repeats, nokg, mlp):\n",
    "# AUC:          0.639 ± 0.207\n",
    "# F1:           0.683 ± 0.118\n",
    "# Balanced Acc: 0.719 ± 0.132"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
