{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa1ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from ChromaVDB.chroma import ChromaFramework\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "vdb = ChromaFramework(persist_directory=\"./ChromaVDB/chroma_db\")\n",
    "records = vdb.list_records()\n",
    "names = [record['name'] for record in records]\n",
    "embs = [record['embeddings'] for record in records]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc277e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('data/2025_03_29.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba05006",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_set = \"LNF\"\n",
    "gene_measure = \"MUT\"\n",
    "\n",
    "gene_data = data[[col for col in data.columns if gene_set in col and gene_measure in col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cd8d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = list(set([ gene.split('_')[0] for gene in gene_data.columns  ]))\n",
    "\n",
    "final_columns = []\n",
    "embeddings = []\n",
    "\n",
    "for gene in genes:\n",
    "    if gene in names:\n",
    "        final_columns.append(gene+\"_\"+gene_set+\"_\"+gene_measure)\n",
    "        embeddings.append(embs[names.index(gene)])\n",
    "    else:\n",
    "        print(gene)\n",
    "\n",
    "print(len(final_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f2d849",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_data = gene_data[final_columns]\n",
    "gene_data['pfs'] = data['PFS_Months_updated']\n",
    "gene_data = gene_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f4612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# GPU Configuration and Optimization\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"GPU device:\", torch.cuda.get_device_name(0))\n",
    "    \n",
    "    # Set GPU memory management\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "\n",
    "class PFSPredictionMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP model that combines gene embeddings with patient binary mutation features\n",
    "    to predict progression-free survival scores.\n",
    "    \n",
    "    Data structure:\n",
    "    - gene_embeddings: (n_genes, embedding_dim) - GNN embeddings for each gene\n",
    "    - patient_mutations: (n_patients, n_genes) - binary mutation matrix\n",
    "    - pfs_scores: (n_patients,) - PFS score for each patient\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_genes, embedding_dim, hidden_dims=[512, 256, 128], \n",
    "                 dropout_rate=0.3, combination_method='weighted_sum'):\n",
    "        super(PFSPredictionMLP, self).__init__()\n",
    "        \n",
    "        self.n_genes = n_genes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.combination_method = combination_method\n",
    "        \n",
    "        # Gene-level fusion methods\n",
    "        if combination_method == 'weighted_sum':\n",
    "            # Weight each gene embedding by mutation status and sum across genes\n",
    "            self.gene_weight = nn.Linear(1, 1, bias=False)  # Learn importance of mutation\n",
    "            patient_repr_dim = embedding_dim\n",
    "            \n",
    "        elif combination_method == 'attention':\n",
    "            # Use attention mechanism over genes\n",
    "            self.gene_attention = nn.MultiheadAttention(\n",
    "                embed_dim=embedding_dim, \n",
    "                num_heads=4, \n",
    "                batch_first=True\n",
    "            )\n",
    "            # Project mutation status to same dim as embeddings for attention\n",
    "            self.mutation_proj = nn.Linear(1, embedding_dim)\n",
    "            patient_repr_dim = embedding_dim\n",
    "            \n",
    "        elif combination_method == 'transformer':\n",
    "            # More sophisticated gene-level transformer\n",
    "            self.gene_transform = nn.TransformerEncoder(\n",
    "                nn.TransformerEncoderLayer(\n",
    "                    d_model=embedding_dim + 1,  # embedding + mutation status\n",
    "                    nhead=4,\n",
    "                    dim_feedforward=embedding_dim * 2,\n",
    "                    dropout=dropout_rate,\n",
    "                    batch_first=True\n",
    "                ),\n",
    "                num_layers=2\n",
    "            )\n",
    "            patient_repr_dim = embedding_dim + 1\n",
    "            \n",
    "        elif combination_method == 'gene_mlp':\n",
    "            # MLP for each gene, then aggregate\n",
    "            self.gene_mlp = nn.Sequential(\n",
    "                nn.Linear(embedding_dim + 1, embedding_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.Linear(embedding_dim, embedding_dim // 2)\n",
    "            )\n",
    "            patient_repr_dim = embedding_dim // 2\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"combination_method must be 'weighted_sum', 'attention', 'transformer', or 'gene_mlp'\")\n",
    "        \n",
    "        # Patient-level MLP for final PFS prediction\n",
    "        layers = []\n",
    "        prev_dim = patient_repr_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.BatchNorm1d(hidden_dim)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer for PFS prediction\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.patient_mlp = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, patient_mutations, gene_embeddings):\n",
    "        \"\"\"\n",
    "        Forward pass combining gene embeddings with patient mutations.\n",
    "        \n",
    "        Args:\n",
    "            patient_mutations: (batch_size, n_genes) binary mutation matrix\n",
    "            gene_embeddings: (n_genes, embedding_dim) gene embeddings from GNN\n",
    "        \n",
    "        Returns:\n",
    "            pfs_prediction: (batch_size, 1) predicted PFS scores\n",
    "        \"\"\"\n",
    "        batch_size = patient_mutations.shape[0]\n",
    "        \n",
    "        if self.combination_method == 'weighted_sum':\n",
    "            # For each patient, weight gene embeddings by mutation status\n",
    "            # patient_mutations: (batch_size, n_genes, 1)\n",
    "            mutations_expanded = patient_mutations.unsqueeze(-1)  # (batch_size, n_genes, 1)\n",
    "            \n",
    "            # Apply learned weight to mutation status\n",
    "            weighted_mutations = self.gene_weight(mutations_expanded)  # (batch_size, n_genes, 1)\n",
    "            \n",
    "            # Weight gene embeddings by mutation status\n",
    "            # gene_embeddings: (n_genes, embedding_dim) -> (1, n_genes, embedding_dim)\n",
    "            gene_emb_expanded = gene_embeddings.unsqueeze(0)  # (1, n_genes, embedding_dim)\n",
    "            \n",
    "            # Broadcast multiply: (batch_size, n_genes, 1) * (1, n_genes, embedding_dim)\n",
    "            weighted_gene_embs = weighted_mutations * gene_emb_expanded  # (batch_size, n_genes, embedding_dim)\n",
    "            \n",
    "            # Sum across genes for each patient\n",
    "            patient_repr = weighted_gene_embs.sum(dim=1)  # (batch_size, embedding_dim)\n",
    "            \n",
    "        elif self.combination_method == 'attention':\n",
    "            # Use attention mechanism to focus on relevant genes\n",
    "            mutations_expanded = patient_mutations.unsqueeze(-1)  # (batch_size, n_genes, 1)\n",
    "            \n",
    "            # Project mutations to embedding dimension\n",
    "            mutation_features = self.mutation_proj(mutations_expanded)  # (batch_size, n_genes, embedding_dim)\n",
    "            \n",
    "            # Add gene embeddings to mutation features\n",
    "            gene_emb_batch = gene_embeddings.unsqueeze(0).repeat(batch_size, 1, 1)  # (batch_size, n_genes, embedding_dim)\n",
    "            combined_features = gene_emb_batch + mutation_features  # (batch_size, n_genes, embedding_dim)\n",
    "            \n",
    "            # Self-attention over genes\n",
    "            attended_features, _ = self.gene_attention(\n",
    "                combined_features, combined_features, combined_features\n",
    "            )  # (batch_size, n_genes, embedding_dim)\n",
    "            \n",
    "            # Average pool across genes (could also use max pool or learned aggregation)\n",
    "            patient_repr = attended_features.mean(dim=1)  # (batch_size, embedding_dim)\n",
    "            \n",
    "        elif self.combination_method == 'transformer':\n",
    "            # Concatenate gene embeddings with mutation status\n",
    "            mutations_expanded = patient_mutations.unsqueeze(-1)  # (batch_size, n_genes, 1)\n",
    "            gene_emb_batch = gene_embeddings.unsqueeze(0).repeat(batch_size, 1, 1)  # (batch_size, n_genes, embedding_dim)\n",
    "            \n",
    "            # Concatenate: (batch_size, n_genes, embedding_dim + 1)\n",
    "            gene_features = torch.cat([gene_emb_batch, mutations_expanded], dim=-1)\n",
    "            \n",
    "            # Apply transformer encoder\n",
    "            transformed = self.gene_transform(gene_features)  # (batch_size, n_genes, embedding_dim + 1)\n",
    "            \n",
    "            # Global average pooling across genes\n",
    "            patient_repr = transformed.mean(dim=1)  # (batch_size, embedding_dim + 1)\n",
    "            \n",
    "        elif self.combination_method == 'gene_mlp':\n",
    "            # Process each gene with MLP, then aggregate\n",
    "            mutations_expanded = patient_mutations.unsqueeze(-1)  # (batch_size, n_genes, 1)\n",
    "            gene_emb_batch = gene_embeddings.unsqueeze(0).repeat(batch_size, 1, 1)  # (batch_size, n_genes, embedding_dim)\n",
    "            \n",
    "            # Concatenate and reshape for MLP processing\n",
    "            gene_features = torch.cat([gene_emb_batch, mutations_expanded], dim=-1)  # (batch_size, n_genes, embedding_dim + 1)\n",
    "            \n",
    "            # Reshape to process all genes at once: (batch_size * n_genes, embedding_dim + 1)\n",
    "            gene_features_flat = gene_features.view(-1, self.embedding_dim + 1)\n",
    "            \n",
    "            # Apply MLP to each gene\n",
    "            gene_processed = self.gene_mlp(gene_features_flat)  # (batch_size * n_genes, embedding_dim // 2)\n",
    "            \n",
    "            # Reshape back: (batch_size, n_genes, embedding_dim // 2)\n",
    "            gene_processed = gene_processed.view(batch_size, self.n_genes, -1)\n",
    "            \n",
    "            # Sum across genes for each patient\n",
    "            patient_repr = gene_processed.sum(dim=1)  # (batch_size, embedding_dim // 2)\n",
    "        \n",
    "        # Final MLP for PFS prediction\n",
    "        pfs_prediction = self.patient_mlp(patient_repr)\n",
    "        return pfs_prediction\n",
    "\n",
    "class PFSPredictor:\n",
    "    \"\"\"\n",
    "    Main class for training and evaluating PFS prediction models with gene-level fusion.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, combination_method='weighted_sum', device='cuda'):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = device\n",
    "            torch.cuda.empty_cache()\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "            print(f\"Using GPU: {gpu_name} ({gpu_memory:.1f}GB)\")\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "            print(\"CUDA not available, using CPU\")\n",
    "                \n",
    "        self.combination_method = combination_method\n",
    "        self.model = None\n",
    "        self.pfs_scaler = StandardScaler()\n",
    "        \n",
    "    def prepare_data(self, patient_mutations, gene_embeddings, pfs_scores):\n",
    "        \"\"\"\n",
    "        Prepare and normalize data for training.\n",
    "        \n",
    "        Args:\n",
    "            patient_mutations: (n_patients, n_genes) binary mutation matrix\n",
    "            gene_embeddings: (n_genes, embedding_dim) gene embeddings from GNN\n",
    "            pfs_scores: (n_patients,) PFS scores\n",
    "        \n",
    "        Returns:\n",
    "            Normalized tensors ready for training\n",
    "        \"\"\"\n",
    "        # Normalize PFS scores only\n",
    "        pfs_normalized = self.pfs_scaler.fit_transform(pfs_scores.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Convert to tensors (keep mutations binary, normalize embeddings)\n",
    "        mutations_tensor = torch.FloatTensor(patient_mutations)  # Keep binary\n",
    "        embeddings_tensor = torch.FloatTensor(gene_embeddings)   # Already normalized from GNN\n",
    "        pfs_tensor = torch.FloatTensor(pfs_normalized)\n",
    "        \n",
    "        return mutations_tensor, embeddings_tensor, pfs_tensor\n",
    "    \n",
    "    def create_model(self, n_genes, embedding_dim, **model_kwargs):\n",
    "        \"\"\"Create and initialize the model.\"\"\"\n",
    "        self.model = PFSPredictionMLP(\n",
    "            n_genes=n_genes,\n",
    "            embedding_dim=embedding_dim,\n",
    "            combination_method=self.combination_method,\n",
    "            **model_kwargs\n",
    "        ).to(self.device)\n",
    "        \n",
    "    def train(self, patient_mutations, gene_embeddings, pfs_scores, \n",
    "              test_size=0.15, validation_size=0.15, batch_size=64, \n",
    "              epochs=200, lr=0.001, weight_decay=1e-4, \n",
    "              early_stopping_patience=20, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the PFS prediction model.\n",
    "        \n",
    "        Args:\n",
    "            patient_mutations: (n_patients, n_genes) binary mutation matrix\n",
    "            gene_embeddings: (n_genes, embedding_dim) gene embeddings from GNN\n",
    "            pfs_scores: (n_patients,) PFS scores\n",
    "            \n",
    "        Returns:\n",
    "            training_history: dict with loss curves and metrics\n",
    "        \"\"\"\n",
    "        \n",
    "        # Prepare data\n",
    "        mut_tensor, emb_tensor, pfs_tensor = self.prepare_data(patient_mutations, gene_embeddings, pfs_scores)\n",
    "        n_patients = len(patient_mutations)\n",
    "        n_genes, embedding_dim = gene_embeddings.shape\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Training on {self.device.upper()}\")\n",
    "            print(f\"Data: {n_patients} patients, {n_genes} genes, {embedding_dim}D gene embeddings\")\n",
    "        \n",
    "        # Split data: train/val/test\n",
    "        indices = np.arange(n_patients)\n",
    "        train_idx, temp_idx = train_test_split(indices, test_size=test_size+validation_size, random_state=42)\n",
    "        val_idx, test_idx = train_test_split(temp_idx, test_size=test_size/(test_size+validation_size), random_state=42)\n",
    "        \n",
    "        # Create splits and move to GPU\n",
    "        train_mut = mut_tensor[train_idx].to(self.device)\n",
    "        train_pfs = pfs_tensor[train_idx].to(self.device)\n",
    "        \n",
    "        val_mut = mut_tensor[val_idx].to(self.device)\n",
    "        val_pfs = pfs_tensor[val_idx].to(self.device)\n",
    "        \n",
    "        test_mut = mut_tensor[test_idx].to(self.device)\n",
    "        test_pfs = pfs_tensor[test_idx].to(self.device)\n",
    "        \n",
    "        # Gene embeddings stay constant (move to GPU once)\n",
    "        gene_emb_gpu = emb_tensor.to(self.device)\n",
    "        \n",
    "        if verbose and self.device == 'cuda':\n",
    "            print(f\"Data moved to GPU. Memory usage: {torch.cuda.memory_allocated()/1024**2:.1f}MB\")\n",
    "        \n",
    "        # Create model\n",
    "        if self.model is None:\n",
    "            self.create_model(n_genes, embedding_dim)\n",
    "            \n",
    "        if verbose and self.device == 'cuda':\n",
    "            model_params = sum(p.numel() for p in self.model.parameters())\n",
    "            print(f\"Model created with {model_params:,} parameters. GPU memory: {torch.cuda.memory_allocated()/1024**2:.1f}MB\")\n",
    "        \n",
    "        # Create data loader\n",
    "        train_dataset = TensorDataset(train_mut, train_pfs)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=False)\n",
    "        \n",
    "        # Optimizer and loss\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Training history\n",
    "        history = {\n",
    "            'train_losses': [], 'val_losses': [], 'test_losses': [],\n",
    "            'train_r2': [], 'val_r2': [], 'test_r2': []\n",
    "        }\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Starting training for {epochs} epochs...\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            epoch_train_loss = 0\n",
    "            train_predictions = []\n",
    "            train_targets = []\n",
    "            \n",
    "            for batch_mut, batch_pfs in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                predictions = self.model(batch_mut, gene_emb_gpu).squeeze()\n",
    "                loss = criterion(predictions, batch_pfs)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_train_loss += loss.item()\n",
    "                train_predictions.append(predictions.detach().cpu().numpy())\n",
    "                train_targets.append(batch_pfs.detach().cpu().numpy())\n",
    "            \n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_pred = self.model(val_mut, gene_emb_gpu).squeeze()\n",
    "                val_loss = criterion(val_pred, val_pfs)\n",
    "                \n",
    "                # Test evaluation\n",
    "                test_pred = self.model(test_mut, gene_emb_gpu).squeeze()\n",
    "                test_loss = criterion(test_pred, test_pfs)\n",
    "            \n",
    "            # Calculate R² scores\n",
    "            train_pred_all = np.concatenate(train_predictions)\n",
    "            train_true_all = np.concatenate(train_targets)\n",
    "            train_r2 = r2_score(train_true_all, train_pred_all)\n",
    "            \n",
    "            val_r2 = r2_score(val_pfs.cpu().numpy(), val_pred.cpu().numpy())\n",
    "            test_r2 = r2_score(test_pfs.cpu().numpy(), test_pred.cpu().numpy())\n",
    "            \n",
    "            # Update history\n",
    "            history['train_losses'].append(epoch_train_loss / len(train_loader))\n",
    "            history['val_losses'].append(val_loss.item())\n",
    "            history['test_losses'].append(test_loss.item())\n",
    "            history['train_r2'].append(train_r2)\n",
    "            history['val_r2'].append(val_r2)\n",
    "            history['test_r2'].append(test_r2)\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                self.best_model_state = {k: v.clone() for k, v in self.model.state_dict().items()}\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "            \n",
    "            if verbose and epoch % 25 == 0:\n",
    "                print(f'Epoch {epoch}: Train Loss = {history[\"train_losses\"][-1]:.4f}, '\n",
    "                      f'Val Loss = {history[\"val_losses\"][-1]:.4f}, '\n",
    "                      f'Val R² = {val_r2:.4f}')\n",
    "        \n",
    "        # Load best model\n",
    "        if hasattr(self, 'best_model_state'):\n",
    "            self.model.load_state_dict(self.best_model_state)\n",
    "        \n",
    "        # Clear GPU cache\n",
    "        if self.device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict(self, patient_mutations, gene_embeddings, batch_size=256):\n",
    "        \"\"\"Make predictions on new data.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        all_predictions = []\n",
    "        n_patients = len(patient_mutations)\n",
    "        \n",
    "        # Move gene embeddings to GPU once\n",
    "        gene_emb_gpu = torch.FloatTensor(gene_embeddings).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, n_patients, batch_size):\n",
    "                end_idx = min(i + batch_size, n_patients)\n",
    "                \n",
    "                batch_mut = torch.FloatTensor(patient_mutations[i:end_idx]).to(self.device)\n",
    "                batch_pred = self.model(batch_mut, gene_emb_gpu).cpu().numpy()\n",
    "                all_predictions.append(batch_pred)\n",
    "        \n",
    "        # Combine all predictions\n",
    "        predictions = np.concatenate(all_predictions, axis=0)\n",
    "        \n",
    "        # Inverse transform to original scale\n",
    "        predictions_original = self.pfs_scaler.inverse_transform(predictions)\n",
    "        \n",
    "        return predictions_original.flatten()\n",
    "    \n",
    "    def evaluate(self, patient_mutations, gene_embeddings, true_pfs):\n",
    "        \"\"\"Evaluate model performance.\"\"\"\n",
    "        predictions = self.predict(patient_mutations, gene_embeddings)\n",
    "        \n",
    "        # Regression metrics\n",
    "        mse = mean_squared_error(true_pfs, predictions)\n",
    "        mae = mean_absolute_error(true_pfs, predictions)\n",
    "        r2 = r2_score(true_pfs, predictions)\n",
    "        \n",
    "        # Correlation metrics\n",
    "        pearson_r, pearson_p = pearsonr(true_pfs, predictions)\n",
    "        spearman_r, spearman_p = spearmanr(true_pfs, predictions)\n",
    "        \n",
    "        metrics = {\n",
    "            'mse': mse,\n",
    "            'mae': mae,\n",
    "            'rmse': np.sqrt(mse),\n",
    "            'r2': r2,\n",
    "            'pearson_r': pearson_r,\n",
    "            'pearson_p': pearson_p,\n",
    "            'spearman_r': spearman_r,\n",
    "            'spearman_p': spearman_p\n",
    "        }\n",
    "        \n",
    "        return metrics, predictions\n",
    "\n",
    "def plot_training_curves(history):\n",
    "    \"\"\"Plot training curves.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Loss curves\n",
    "    axes[0].plot(history['train_losses'], label='Train Loss', alpha=0.8)\n",
    "    axes[0].plot(history['val_losses'], label='Validation Loss', alpha=0.8)\n",
    "    axes[0].plot(history['test_losses'], label='Test Loss', alpha=0.8)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('MSE Loss')\n",
    "    axes[0].set_title('Training Curves - Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # R² curves\n",
    "    axes[1].plot(history['train_r2'], label='Train R²', alpha=0.8)\n",
    "    axes[1].plot(history['val_r2'], label='Validation R²', alpha=0.8)\n",
    "    axes[1].plot(history['test_r2'], label='Test R²', alpha=0.8)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('R² Score')\n",
    "    axes[1].set_title('Training Curves - R²')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_predictions(true_pfs, predictions, title=\"PFS Predictions vs True Values\"):\n",
    "    \"\"\"Plot predictions vs true values.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Scatter plot\n",
    "    plt.scatter(true_pfs, predictions, alpha=0.6, s=50)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(true_pfs.min(), predictions.min())\n",
    "    max_val = max(true_pfs.max(), predictions.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, label='Perfect Prediction')\n",
    "    \n",
    "    # Calculate and display metrics\n",
    "    r2 = r2_score(true_pfs, predictions)\n",
    "    pearson_r, _ = pearsonr(true_pfs, predictions)\n",
    "    \n",
    "    plt.xlabel('True PFS Scores')\n",
    "    plt.ylabel('Predicted PFS Scores')\n",
    "    plt.title(f'{title}\\nR² = {r2:.3f}, Pearson r = {pearson_r:.3f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f23f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfs_scores = gene_data['pfs'].values\n",
    "mutants = gene_data.drop(columns=['pfs'])\n",
    "patient_mutations = mutants.values\n",
    "gene_embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a7d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Example usage with CORRECT data structure.\"\"\"\n",
    "print(\"=== Gene-Level PFS Prediction ===\")\n",
    "\n",
    "print(\"Generating example data...\")\n",
    "# patient_mutations, gene_embeddings, pfs_scores = generate_example_data()\n",
    "\n",
    "print(f\"Data shapes:\")\n",
    "print(f\"Patient mutations: {patient_mutations.shape} (n_patients, n_genes)\")\n",
    "print(f\"Gene embeddings: {gene_embeddings.shape} (n_genes, embedding_dim)\")\n",
    "print(f\"PFS scores: {pfs_scores.shape} (n_patients,)\")\n",
    "print(f\"PFS score range: {pfs_scores.min():.2f} to {pfs_scores.max():.2f}\")\n",
    "print(f\"Mutation rate: {patient_mutations.mean():.3f}\")\n",
    "\n",
    "# Test different gene-level combination methods\n",
    "methods = ['weighted_sum', 'attention', 'gene_mlp']\n",
    "results = {}\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"\\n--- Training with {method} method ---\")\n",
    "    \n",
    "    predictor = PFSPredictor(combination_method=method)\n",
    "    \n",
    "    # Train model\n",
    "    history = predictor.train(\n",
    "        patient_mutations, gene_embeddings, pfs_scores,\n",
    "        epochs=100, batch_size=32, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics, predictions = predictor.evaluate(patient_mutations, gene_embeddings, pfs_scores)\n",
    "    results[method] = {'metrics': metrics, 'predictions': predictions, 'history': history}\n",
    "    \n",
    "    print(f\"Results:\")\n",
    "    print(f\"  R² Score: {metrics['r2']:.4f}\")\n",
    "    print(f\"  RMSE: {metrics['rmse']:.4f}\")\n",
    "    print(f\"  Pearson r: {metrics['pearson_r']:.4f}\")\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Find best method\n",
    "best_method = max(results.keys(), key=lambda x: results[x]['metrics']['r2'])\n",
    "print(f\"\\nBest method: {best_method} (R² = {results[best_method]['metrics']['r2']:.4f})\")\n",
    "\n",
    "# Plot results for best method\n",
    "plot_training_curves(results[best_method]['history'])\n",
    "plot_predictions(pfs_scores, results[best_method]['predictions'], \n",
    "                f\"Best Method: {best_method}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
