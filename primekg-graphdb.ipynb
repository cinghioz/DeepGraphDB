{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce29420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "class PrimeKGLoader:\n",
    "    \"\"\"\n",
    "    Prepares PrimeKG data for efficient loading into DGL heterogeneous graphs.\n",
    "    Each node type gets sequential IDs starting from 0.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.node_type_mapping = {}  # string -> int\n",
    "        self.relationship_type_mapping = {}  # string -> int\n",
    "        self.reverse_node_type_mapping = {}  # int -> string\n",
    "        self.reverse_relationship_type_mapping = {}  # int -> string\n",
    "        self.global_to_local_mapping = {}  # For reference: global_id -> (node_type, local_id)\n",
    "        \n",
    "    def load_and_prepare_primekg(self, nodes_csv_path: str, edges_csv_path: str):\n",
    "        \"\"\"\n",
    "        Load PrimeKG data and prepare it for bulk_load_heterogeneous_graph.\n",
    "        Each node type gets sequential IDs starting from 0.\n",
    "        \n",
    "        Args:\n",
    "            nodes_csv_path: Path to nodes CSV file\n",
    "            edges_csv_path: Path to edges CSV file\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (node_types_dict, edge_types_dict) ready for DGL loading\n",
    "        \"\"\"\n",
    "        print(\"Loading PrimeKG data...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Load raw data\n",
    "        print(\"  Reading CSV files...\")\n",
    "        nodes_df = pd.read_csv(nodes_csv_path, low_memory=False)\n",
    "        edges_df = pd.read_csv(edges_csv_path, low_memory=False)\n",
    "        \n",
    "        print(f\"  Loaded {len(nodes_df):,} nodes and {len(edges_df):,} edges\")\n",
    "        \n",
    "        # Create type mappings\n",
    "        print(\"  Creating type mappings...\")\n",
    "        self._create_type_mappings(nodes_df, edges_df)\n",
    "        \n",
    "        # Prepare node data (sequential IDs starting from 0 for each type)\n",
    "        print(\"  Preparing node data...\")\n",
    "        node_types_dict = self._prepare_node_data(nodes_df)\n",
    "        \n",
    "        # Prepare edge data (using local IDs)\n",
    "        print(\"  Preparing edge data...\")\n",
    "        edge_types_dict = self._prepare_edge_data(edges_df, nodes_df)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nData preparation completed in {total_time:.2f}s\")\n",
    "        \n",
    "        # Print summary\n",
    "        self._print_summary(node_types_dict, edge_types_dict)\n",
    "        \n",
    "        return node_types_dict, edge_types_dict, self.global_to_local_mapping\n",
    "    \n",
    "    def _create_type_mappings(self, nodes_df: pd.DataFrame, edges_df: pd.DataFrame):\n",
    "        \"\"\"Create mappings between string types and integer representations.\"\"\"\n",
    "        \n",
    "        # Node type mappings\n",
    "        unique_node_types = sorted(nodes_df['node_type'].unique())\n",
    "        self.node_type_mapping = {node_type: i for i, node_type in enumerate(unique_node_types)}\n",
    "        self.reverse_node_type_mapping = {i: node_type for node_type, i in self.node_type_mapping.items()}\n",
    "        \n",
    "        # Relationship type mappings\n",
    "        unique_rel_types = sorted(edges_df['relationship_type'].unique())\n",
    "        self.relationship_type_mapping = {rel_type: i for i, rel_type in enumerate(unique_rel_types)}\n",
    "        self.reverse_relationship_type_mapping = {i: rel_type for rel_type, i in self.relationship_type_mapping.items()}\n",
    "        \n",
    "        print(f\"    Found {len(unique_node_types)} node types: {unique_node_types}\")\n",
    "        print(f\"    Found {len(unique_rel_types)} relationship types: {unique_rel_types}\")\n",
    "    \n",
    "    def _prepare_node_data(self, nodes_df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Group nodes by type and prepare DataFrames with sequential IDs starting from 0.\n",
    "        \n",
    "        Returns:\n",
    "            Dict mapping node_type_string -> DataFrame with columns ['node_id', 'name', 'metadata_source', 'node_type_id', 'original_global_id']\n",
    "        \"\"\"\n",
    "        node_types_dict = {}\n",
    "        \n",
    "        # Add numeric type ID to nodes\n",
    "        nodes_df_copy = nodes_df.copy()\n",
    "        nodes_df_copy['node_type_id'] = nodes_df_copy['node_type'].map(self.node_type_mapping)\n",
    "        \n",
    "        # Group by node type and assign sequential IDs starting from 0\n",
    "        for node_type_str, group_df in nodes_df_copy.groupby('node_type'):\n",
    "            # Sort by original ID for consistency\n",
    "            group_df = group_df.sort_values('id').reset_index(drop=True)\n",
    "            \n",
    "            # Create sequential IDs starting from 0\n",
    "            num_nodes = len(group_df)\n",
    "            \n",
    "            # Build global to local mapping for this node type\n",
    "            global_ids = group_df['id'].values\n",
    "            local_ids = np.arange(num_nodes)  # 0, 1, 2, ..., num_nodes-1\n",
    "            \n",
    "            # Store the mapping for edge processing\n",
    "            for local_id, global_id in zip(local_ids, global_ids):\n",
    "                self.global_to_local_mapping[global_id] = (node_type_str, local_id)\n",
    "            \n",
    "            # Prepare DataFrame for DGL\n",
    "            prepared_df = pd.DataFrame({\n",
    "                'node_id': local_ids,  # Sequential IDs starting from 0\n",
    "                'name': group_df['name'].values,\n",
    "                'metadata_source': group_df['metadata_source'].values,\n",
    "                'node_type_id': group_df['node_type_id'].values,\n",
    "                'original_global_id': global_ids  # Keep original for reference\n",
    "            })\n",
    "            \n",
    "            node_types_dict[node_type_str] = prepared_df\n",
    "            print(f\"    {node_type_str}: {num_nodes:,} nodes (IDs: 0 to {num_nodes-1})\")\n",
    "            \n",
    "        return node_types_dict\n",
    "    \n",
    "    def _prepare_edge_data(self, edges_df: pd.DataFrame, nodes_df: pd.DataFrame) -> Dict[Tuple[str, str, str], pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Prepare edge data grouped by (src_type, edge_type, dst_type) using local IDs.\n",
    "        \n",
    "        Returns:\n",
    "            Dict mapping (src_type, edge_type, dst_type) -> DataFrame with columns ['src', 'dst', 'relationship_type_id']\n",
    "        \"\"\"\n",
    "        # Create node ID to type mapping for fast lookup\n",
    "        node_id_to_type = dict(zip(nodes_df['id'], nodes_df['node_type']))\n",
    "        \n",
    "        # Add relationship type IDs\n",
    "        edges_df_copy = edges_df.copy()\n",
    "        edges_df_copy['relationship_type_id'] = edges_df_copy['relationship_type'].map(self.relationship_type_mapping)\n",
    "        \n",
    "        # Add source and target node types\n",
    "        edges_df_copy['src_type'] = edges_df_copy['source_id'].map(node_id_to_type)\n",
    "        edges_df_copy['dst_type'] = edges_df_copy['target_id'].map(node_id_to_type)\n",
    "        \n",
    "        # Filter out edges with unknown nodes\n",
    "        valid_mask = (edges_df_copy['src_type'].notna()) & (edges_df_copy['dst_type'].notna())\n",
    "        valid_edges = edges_df_copy[valid_mask]\n",
    "        \n",
    "        if len(valid_edges) < len(edges_df_copy):\n",
    "            print(f\"    Warning: Filtered out {len(edges_df_copy) - len(valid_edges)} edges with unknown nodes\")\n",
    "        \n",
    "        # Group by (src_type, relationship_type, dst_type)\n",
    "        edge_types_dict = {}\n",
    "        \n",
    "        for (src_type, rel_type, dst_type), group_df in valid_edges.groupby(['src_type', 'relationship_type', 'dst_type']):\n",
    "            print(f\"    Processing {src_type} --[{rel_type}]--> {dst_type}: {len(group_df):,} edges\")\n",
    "            \n",
    "            # VECTORIZED APPROACH - Much faster than loops\n",
    "            group_df_reset = group_df.reset_index(drop=True)\n",
    "            \n",
    "            # Create mapping functions for this specific edge type\n",
    "            src_type_mapping = {global_id: local_id for global_id, (nt, local_id) in self.global_to_local_mapping.items() if nt == src_type}\n",
    "            dst_type_mapping = {global_id: local_id for global_id, (nt, local_id) in self.global_to_local_mapping.items() if nt == dst_type}\n",
    "            \n",
    "            # Vectorized mapping using pandas map\n",
    "            group_df_reset['src_local'] = group_df_reset['source_id'].map(src_type_mapping)\n",
    "            group_df_reset['dst_local'] = group_df_reset['target_id'].map(dst_type_mapping)\n",
    "            \n",
    "            # Filter valid edges (both src and dst must be mapped)\n",
    "            valid_mask = (group_df_reset['src_local'].notna()) & (group_df_reset['dst_local'].notna())\n",
    "            valid_edges_df = group_df_reset[valid_mask]\n",
    "            \n",
    "            if len(valid_edges_df) == 0:\n",
    "                print(f\"      Warning: No valid edges found for {src_type}-{rel_type}->{dst_type}\")\n",
    "                continue\n",
    "            \n",
    "            # Create edge DataFrame with local node IDs\n",
    "            edge_df = pd.DataFrame({\n",
    "                'src': valid_edges_df['src_local'].astype(int).values,  # Local IDs (0-based for each node type)\n",
    "                'dst': valid_edges_df['dst_local'].astype(int).values,  # Local IDs (0-based for each node type)\n",
    "                'relationship_type_id': valid_edges_df['relationship_type_id'].values,\n",
    "                'original_src_id': valid_edges_df['source_id'].values,  # Keep original for reference\n",
    "                'original_dst_id': valid_edges_df['target_id'].values   # Keep original for reference\n",
    "            })\n",
    "            \n",
    "            edge_types_dict[(src_type, rel_type, dst_type)] = edge_df\n",
    "            print(f\"      Created {len(edge_df):,} valid edges\")\n",
    "            \n",
    "        return edge_types_dict\n",
    "    \n",
    "    def _print_summary(self, node_types_dict: Dict[str, pd.DataFrame], \n",
    "                      edge_types_dict: Dict[Tuple[str, str, str], pd.DataFrame]):\n",
    "        \"\"\"Print summary of prepared data.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PRIMEKG DATA PREPARATION SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(\"\\nNode Type Mappings:\")\n",
    "        for str_type, int_type in self.node_type_mapping.items():\n",
    "            count = len(node_types_dict.get(str_type, []))\n",
    "            print(f\"  {int_type}: {str_type} ({count:,} nodes, IDs: 0 to {count-1})\")\n",
    "        \n",
    "        print(\"\\nRelationship Type Mappings:\")\n",
    "        for str_type, int_type in self.relationship_type_mapping.items():\n",
    "            print(f\"  {int_type}: {str_type}\")\n",
    "        \n",
    "        print(\"\\nPrepared Node Types:\")\n",
    "        total_nodes = 0\n",
    "        for node_type, df in node_types_dict.items():\n",
    "            min_id = df['node_id'].min()\n",
    "            max_id = df['node_id'].max()\n",
    "            print(f\"  {node_type}: {len(df):,} nodes (local IDs: {min_id} to {max_id})\")\n",
    "            total_nodes += len(df)\n",
    "        print(f\"  TOTAL: {total_nodes:,} nodes\")\n",
    "        \n",
    "        print(\"\\nPrepared Edge Types:\")\n",
    "        total_edges = 0\n",
    "        for (src_type, edge_type, dst_type), df in edge_types_dict.items():\n",
    "            print(f\"  {src_type} --[{edge_type}]--> {dst_type}: {len(df):,} edges\")\n",
    "            total_edges += len(df)\n",
    "        print(f\"  TOTAL: {total_edges:,} edges\")\n",
    "        \n",
    "        print(\"\\nData Format Verification:\")\n",
    "        for node_type, df in node_types_dict.items():\n",
    "            assert df['node_id'].min() == 0, f\"Node IDs for {node_type} don't start at 0!\"\n",
    "            assert df['node_id'].max() == len(df) - 1, f\"Node IDs for {node_type} are not sequential!\"\n",
    "            print(f\"  ✅ {node_type}: Sequential IDs 0 to {len(df)-1}\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    def get_type_mappings(self):\n",
    "        \"\"\"Return the type mappings for reference.\"\"\"\n",
    "        return {\n",
    "            'node_types': self.node_type_mapping,\n",
    "            'relationship_types': self.relationship_type_mapping,\n",
    "            'reverse_node_types': self.reverse_node_type_mapping,\n",
    "            'reverse_relationship_types': self.reverse_relationship_type_mapping\n",
    "        }\n",
    "    \n",
    "    def get_global_to_local_mapping(self):\n",
    "        \"\"\"Return the global to local ID mapping for reference.\"\"\"\n",
    "        return self.global_to_local_mapping.copy()\n",
    "    \n",
    "    def global_id_to_local(self, global_id: int) -> Tuple[str, int]:\n",
    "        \"\"\"Convert a global node ID to (node_type, local_id).\"\"\"\n",
    "        if global_id in self.global_to_local_mapping:\n",
    "            return self.global_to_local_mapping[global_id]\n",
    "        else:\n",
    "            raise ValueError(f\"Global ID {global_id} not found in mapping\")\n",
    "    \n",
    "    def local_id_to_global(self, node_type: str, local_id: int) -> int:\n",
    "        \"\"\"Convert (node_type, local_id) to global node ID.\"\"\"\n",
    "        for global_id, (nt, lid) in self.global_to_local_mapping.items():\n",
    "            if nt == node_type and lid == local_id:\n",
    "                return global_id\n",
    "        raise ValueError(f\"Local ID ({node_type}, {local_id}) not found in mapping\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10666ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Dict\n",
    "from dgl.dataloading import DataLoader, NeighborSampler\n",
    "\n",
    "def create_simple_dataloader(graph, \n",
    "                            batch_size: int = 512,\n",
    "                            fanouts: List[int] = [15, 10],\n",
    "                            shuffle: bool = True):\n",
    "    \"\"\"\n",
    "    Create simple DGL DataLoader compatible with different versions.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Number of seed nodes per batch\n",
    "        fanouts: Number of neighbors to sample per layer\n",
    "        shuffle: Whether to shuffle the seed nodes\n",
    "        \n",
    "    Returns:\n",
    "        DGL DataLoader that covers all nodes\n",
    "    \"\"\"\n",
    "    \n",
    "    sampler = NeighborSampler(fanouts)\n",
    "    print(\"Using modern DGL DataLoader\")\n",
    "\n",
    "    all_nodes = {}\n",
    "    total_count = 0\n",
    "    for node_type in graph.ntypes:\n",
    "        num_nodes = graph.num_nodes(node_type)\n",
    "        all_nodes[node_type] = torch.arange(num_nodes)\n",
    "        total_count += num_nodes\n",
    "    print(f\"Created dataloader for {total_count} nodes across {len(all_nodes)} types\")\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataloader = DataLoader(\n",
    "        graph,\n",
    "        all_nodes,\n",
    "        sampler,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=False,\n",
    "        num_workers=0  # Start with 0 for compatibility\n",
    "    )\n",
    "    \n",
    "    print(f\"DataLoader ready: {len(dataloader)} batches\")\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cd2f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepGraphDB import DeepGraphDB\n",
    "\n",
    "db = DeepGraphDB()\n",
    "# Initialize the loader\n",
    "loader = PrimeKGLoader()\n",
    "\n",
    "# Load and prepare data\n",
    "nodes_csv = \"nodes.csv\"  # Replace with your actual path\n",
    "edges_csv = \"edges.csv\"  # Replace with your actual path\n",
    "\n",
    "node_types_dict, edge_types_dict, mapping = loader.load_and_prepare_primekg(nodes_csv, edges_csv)\n",
    "\n",
    "    \n",
    "# Get type mappings for reference\n",
    "mappings = loader.get_type_mappings()\n",
    "print(\"\\nType mappings created:\")\n",
    "print(\"Node types:\", mappings['node_types'])\n",
    "print(\"Relationship types:\", mappings['relationship_types'])\n",
    "\n",
    "# Verify data format\n",
    "print(\"\\nData format verification:\")\n",
    "for node_type, df in node_types_dict.items():\n",
    "    print(f\"  {node_type}: node_id range {df['node_id'].min()}-{df['node_id'].max()}\")\n",
    "\n",
    "# Now you can use this data with your DGL graph analyzer\n",
    "print(\"\\nReady to load into DGL!\")\n",
    "print(\"Use: analyzer.bulk_load_heterogeneous_graph(node_types_dict, edge_types_dict)\")\n",
    "db.bulk_load_heterogeneous_graph(node_types_dict, edge_types_dict)\n",
    "db.set_mappings(loader.node_type_mapping, loader.relationship_type_mapping)\n",
    "db.set_global_to_local_mapping(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ac4767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(max(db.global_to_local_mapping.keys())+1, 128)\n",
    "db.load_node_features_for_gnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ef0517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import create_dataloader_with_negative_sampling\n",
    "\n",
    "dataloader = create_dataloader_with_negative_sampling(\n",
    "    db.graph,\n",
    "    batch_size=256,\n",
    "    fanouts=[10, 5],\n",
    "    negative_sampler='global_uniform',\n",
    "    num_negative_edges=1\n",
    ")\n",
    "\n",
    "# dataloader = create_simple_dataloader(db.graph, batch_size=512, fanouts=[15, 10, 5], shuffle=True)\n",
    "\n",
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1899e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad623c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nGraph statistics:\")\n",
    "# stats = db.get_graph_statistics()\n",
    "# for key, value in stats.items():\n",
    "#     print(f\"  {key}: {value}\")\n",
    "\n",
    "test_node = 26719\n",
    "\n",
    "print(f\"\\nTesting k-hop neighbors around node {test_node}...\")\n",
    "\n",
    "start = time.time()\n",
    "neighbors = db.get_k_hop_neighbors([test_node], k=3, edge_types=[(\"effectphenotype\", \"phenotype_phenotype\" ,\"effectphenotype\")])\n",
    "end = time.time()\n",
    "print(f\"3-hop neighbors of node {test_node}: {len(neighbors.get(1, set()))} at hop 1, {len(neighbors.get(2, set()))} at hop 2, {len(neighbors.get(3, set()))} at hop 3\")\n",
    "print(f\"K-hop query took {end-start:.4f} seconds\")\n",
    "print(f\"Neighbors: {neighbors}\")\n",
    "\n",
    "print(f\"\\nTesting subgraph extraction around node {test_node}...\")\n",
    "start = time.time()\n",
    "subgraph, k_hop_res = db.extract_subgraph([test_node], k=3)\n",
    "end = time.time()\n",
    "total_subgraph_nodes = sum(subgraph.num_nodes(ntype) for ntype in subgraph.ntypes) if hasattr(subgraph, 'ntypes') else subgraph.num_nodes()\n",
    "total_subgraph_edges = sum(subgraph.num_edges(etype) for etype in subgraph.canonical_etypes) if hasattr(subgraph, 'canonical_etypes') else subgraph.num_edges()\n",
    "print(f\"3-hop subgraph: {total_subgraph_nodes} nodes, {total_subgraph_edges} edges\")\n",
    "print(f\"Subgraph extraction took {end-start:.4f} seconds\")\n",
    "\n",
    "# print(f\"\\nTesting meta-path queries from node {test_node}...\")\n",
    "# start = time.time()\n",
    "# meta_paths = db.find_meta_paths([test_node], ['authored', 'about'], max_paths_per_node=5)\n",
    "# end = time.time()\n",
    "# print(f\"Meta-paths from node {test_node}: {len(meta_paths.get(test_node, []))} paths found\")\n",
    "# if meta_paths.get(test_node):\n",
    "#     print(f\"First path example: {meta_paths[test_node][0] if meta_paths[test_node] else 'None'}\")\n",
    "# print(f\"Meta-path query took {end-start:.4f} seconds\")\n",
    "\n",
    "# print(\"\\nTesting node queries...\")\n",
    "# start = time.time()\n",
    "# nodes_df = db.query_nodes_by_feature('person', 'age', 'gt', 50, return_features=['h_index'])\n",
    "# end = time.time()\n",
    "# print(f\"Filtered person nodes (age > 50):\\n{nodes_df['node_ids'].shape}\")\n",
    "# print(f\"Node query took {end-start:.4f} seconds\")\n",
    "\n",
    "# print(\"\\nTesting node queries...\")\n",
    "# start = time.time()\n",
    "# nodes_df = db.query_nodes_by_feature('person', 'age', 'gt', 50, return_features=['h_index'])\n",
    "# nodes_df =  db.get_top_nodes_by_feature('person', 'h_index', top_k=5, ascending=False, return_features=['name'])\n",
    "# end = time.time()\n",
    "# print(f\"Top k node x h-index:\\n{nodes_df['name']}\")\n",
    "# print(f\"Node query took {end-start:.4f} seconds\")\n",
    "\n",
    "# # print(\"\\nTesting edge queries...\")\n",
    "# # start = time.time()\n",
    "# # edges_df = db.query_edges(edge_type=('person', 'authored', 'paper'), limit=5)\n",
    "# # end = time.time()\n",
    "# # print(f\"Authored edges (first 5):\\n{edges_df}\")\n",
    "# # print(f\"Edge query took {end-start:.4f} seconds\")\n",
    "\n",
    "# # Performance summary\n",
    "# print(f\"\\n{'='*50}\")\n",
    "# print(\"PERFORMANCE SUMMARY\")\n",
    "# print(f\"{'='*50}\")\n",
    "# print(f\"Graph size: {stats.get('total_nodes', 0):,} nodes, {stats.get('total_edges', 0):,} edges\")\n",
    "# print(\"All operations completed successfully on large-scale graph!\")\n",
    "# print(\"Framework is ready for production use with millions of nodes/edges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e71457",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [36278, 9, 120987]\n",
    "k = 2\n",
    "\n",
    "neighbors = db.get_k_hop_neighbors(nodes, k=k)\n",
    "print(f\"{k}-hop neighbors of nodes: {len(neighbors.get(1, set()))} at hop 1, {len(neighbors.get(2, set()))} at hop 2, {len(neighbors.get(3, set()))} at hop 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8e1d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph, k_hop_res = db.extract_subgraph(nodes, k=2)\n",
    "total_subgraph_nodes = sum(subgraph.num_nodes(ntype) for ntype in subgraph.ntypes) if hasattr(subgraph, 'ntypes') else subgraph.num_nodes()\n",
    "total_subgraph_edges = sum(subgraph.num_edges(etype) for etype in subgraph.canonical_etypes) if hasattr(subgraph, 'canonical_etypes') else subgraph.num_edges()\n",
    "print(f\"{k}-hop subgraph: {total_subgraph_nodes} nodes, {total_subgraph_edges} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba747158",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_of_nodes = [[36278], [120987]]\n",
    "merged_graphs, stats = db.merge_k_hop_subgraphs(group_of_nodes, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947b7c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.find_meta_paths([6372], ['protein_protein', 'anatomy_protein_present'], 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8685d4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = pd.read_csv(\"edges.csv\", low_memory=False)\n",
    "nodes = pd.read_csv(\"nodes.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78593cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_nodes = set([36278, 120987])  # Replace with your actual node IDs\n",
    "\n",
    "# Step 1: Find all 1-hop neighbors\n",
    "one_hop_edges = edges[edges['source_id'].isin(start_nodes) | edges['target_id'].isin(start_nodes)]\n",
    "one_hop_nodes = set(one_hop_edges['source_id']).union(set(one_hop_edges['target_id']))\n",
    "\n",
    "# Step 2: Find all 2-hop neighbors\n",
    "two_hop_edges = edges[edges['source_id'].isin(one_hop_nodes) | edges['target_id'].isin(one_hop_nodes)]\n",
    "two_hop_nodes = set(two_hop_edges['source_id']).union(set(two_hop_edges['target_id']))\n",
    "\n",
    "# Subgraph nodes (within 2 hops from start_nodes)\n",
    "subgraph_nodes = two_hop_nodes\n",
    "\n",
    "# Step 3: Extract the full 2-hop subgraph (edges between any of those nodes)\n",
    "subgraph = edges[edges['source_id'].isin(subgraph_nodes) & edges['target_id'].isin(subgraph_nodes)]\n",
    "\n",
    "# Display or export the result\n",
    "print(subgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf40b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.groupby(['source_id']).size().reset_index(name='count').sort_values(by='count', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f51dae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nodes.loc[100])\n",
    "\n",
    "# local_type, local_id = db.global_to_local_mapping[100]\n",
    "# print(db.node_data[local_type]['name'][local_id])\n",
    "\n",
    "# db.bulk_modify_nodes({100: {'name': 'SGAHII'}})\n",
    "\n",
    "# local_type, local_id = db.global_to_local_mapping[100]\n",
    "# print(db.node_data[local_type]['name'][local_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c2d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(db.graph.num_nodes())\n",
    "# print(db.graph.num_edges())\n",
    "# print('----------------------')\n",
    "# print(len(db.global_to_local_mapping.keys()))\n",
    "\n",
    "# count = 0\n",
    "# for k, v in db.node_data.items():\n",
    "#     count += v['name'].shape[0]\n",
    "\n",
    "# print(count)\n",
    "\n",
    "# db.bulk_delete_nodes([63423, 63376, 64544])\n",
    "\n",
    "# print(db.graph.num_nodes())\n",
    "# print(db.graph.num_edges())\n",
    "# print('----------------------')\n",
    "# print(len(db.global_to_local_mapping.keys()))\n",
    "\n",
    "# count = 0\n",
    "# for k, v in db.node_data.items():\n",
    "#     count += v['name'].shape[0]\n",
    "\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad61c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# --- Core remapping function for a single type batch ---\n",
    "def remap_type(typ, global_to_local, global_to_vector):\n",
    "    \"\"\"\n",
    "    After deletions, for one type:\n",
    "      1. Rescale local_ids\n",
    "      2. Remap global_ids to consecutive starting at min\n",
    "      3. Resort both dicts ascending\n",
    "    \"\"\"\n",
    "    # Gather remaining entries for this type\n",
    "    entries = [(gid, loc, global_to_vector[gid])\n",
    "               for gid, (loc, t) in global_to_local.items() if t == typ]\n",
    "    entries.sort(key=lambda x: x[1])\n",
    "\n",
    "    # Compute new global IDs\n",
    "    old_ids = [gid for gid, _, _ in entries]\n",
    "    if not old_ids:\n",
    "        return\n",
    "    min_id = min(old_ids)\n",
    "    new_globals = [min_id + i for i in range(len(entries))]\n",
    "\n",
    "    # Clear old entries for this type\n",
    "    for gid, _, _ in entries:\n",
    "        del global_to_local[gid]\n",
    "        del global_to_vector[gid]\n",
    "\n",
    "    # Reassign mappings\n",
    "    for new_local, ((old_gid, _, vec_id), new_gid) in enumerate(zip(entries, new_globals)):\n",
    "        global_to_local[new_gid] = (new_local, typ)\n",
    "        global_to_vector[new_gid] = vec_id\n",
    "\n",
    "# --- Batch delete and remap without elems list ---\n",
    "def delete_many(global_ids, global_to_local, global_to_vector):\n",
    "    \"\"\"\n",
    "    Delete multiple global_ids, then remap affected types in one pass.\n",
    "    Operates only on the two mappings.\n",
    "    \"\"\"\n",
    "    types_to_fix = set()\n",
    "\n",
    "    # 1. Remove specified IDs\n",
    "    for gid in global_ids:\n",
    "        old_local, typ = global_to_local.pop(gid)\n",
    "        global_to_vector.pop(gid)\n",
    "        types_to_fix.add(typ)\n",
    "\n",
    "    # 2. Remap each affected type\n",
    "    for typ in types_to_fix:\n",
    "        remap_type(typ, global_to_local, global_to_vector)\n",
    "\n",
    "    # 3. Resort both dicts by global_id\n",
    "    sorted_keys = sorted(global_to_local)\n",
    "    global_to_local.update({k: global_to_local.pop(k) for k in sorted_keys})\n",
    "    global_to_vector.update({k: global_to_vector.pop(k) for k in sorted_keys})\n",
    "\n",
    "# --- Test harness for 1M IDs, 10 types ---\n",
    "NUM_TYPES = 100\n",
    "TOTAL_IDS = 1_000_000\n",
    "IDS_PER_TYPE = TOTAL_IDS // NUM_TYPES\n",
    "\n",
    "global_to_local = {}\n",
    "global_to_vector = {}\n",
    "\n",
    "gid_start = 0\n",
    "for t in range(NUM_TYPES):\n",
    "    typ = chr(ord('A') + t)\n",
    "    for local in range(IDS_PER_TYPE):\n",
    "        global_to_local[gid_start] = (local, typ)\n",
    "        global_to_vector[gid_start] = random.randint(1_000_000, 9_999_999)\n",
    "        gid_start += 1\n",
    "\n",
    "# Delete a batch of IDs\n",
    "to_delete = random.sample(list(global_to_local.keys()), 1000)\n",
    "print(f\"Deleting {len(to_delete)} IDs...\")\n",
    "\n",
    "# Time the batch operation\n",
    "t0 = time.perf_counter()\n",
    "delete_many(to_delete, global_to_local, global_to_vector)\n",
    "t1 = time.perf_counter()\n",
    "print(f\"Batch deletion and remap took {t1 - t0:.4f} seconds\")\n",
    "\n",
    "# Sanity checks\n",
    "remaining = len(global_to_local)\n",
    "print(f\"Remaining entries: {remaining} (should be {TOTAL_IDS - len(to_delete)})\")\n",
    "# Check one random type\n",
    "typ = random.choice([tp for _, (_, tp) in global_to_local.items()])\n",
    "ids_of_type = [gid for gid, (_, tt) in global_to_local.items() if tt == typ]\n",
    "print(f\"Type {typ} has {len(ids_of_type)} entries spanning {min(ids_of_type)} to {max(ids_of_type)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
